{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本教程提供了一个如何使用tf.data.TextLineDataset示例从文本文件中加载示例的示例。TextLineDataset用于从文本文件创建数据集，其中每个示例都是原始文件中的一行文本。对于主要基于行的任何文本数据（例如，诗歌或错误日志），这可能很有用。\n",
    "\n",
    "在本教程中，我们将使用同一作品的三种不同的英语翻译，即荷马的《伊利亚特》，并训练一个模型以在单行文本的情况下识别翻译者。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.通过`tf.data.TextLineDataset`加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这三种翻译的文本是：\n",
    "\n",
    "* 威廉·考珀（William Cowper） - 文字\n",
    "\n",
    "* 德比伯爵爱德华（Edward） — 文本\n",
    "\n",
    "* 塞缪尔·巴特勒（Samuel Butler） - 文字\n",
    "\n",
    "本教程中使用的文本文件已经执行了一些典型的预处理任务，主要是删除了内容-文档的页眉和页脚，行号，章节标题。从本地下载这些轻描淡写的文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/illiad/cowper.txt\n",
      "819200/815980 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 1s 1us/step\n",
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/illiad/derby.txt\n",
      "811008/809730 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA: 1: - ETA: 32s - ETA: 31 - ETA: 29 - ETA: 12 - ETA: 9 - ETA:  - ETA:  - ETA:  - 31s 38us/step\n",
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/illiad/butler.txt\n",
      "811008/807992 [==============================] - ETA: 16 - ETA: 10 - ETA: 5 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 2s 3us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\sha\\\\.keras\\\\datasets'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DIRECTORY_URL = 'https://storage.googleapis.com/download.tensorflow.org/data/illiad/'\n",
    "FILE_NAMES = ['cowper.txt', 'derby.txt', 'butler.txt']\n",
    "\n",
    "for name in FILE_NAMES:\n",
    "    text_dir = tf.keras.utils.get_file(name, origin=DIRECTORY_URL+name)\n",
    "\n",
    "parent_dir = os.path.dirname(text_dir)\n",
    "\n",
    "parent_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将文本加载到数据集中<br>\n",
    "遍历文件，将每个文件加载到其自己的数据集中。\n",
    "\n",
    "每个示例都需要单独标记，因此可以tf.data.Dataset.map对每个示例应用标记功能。这将遍历数据集中的每个示例，并返回（example, label）对。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeler(example, index):\n",
    "    return example, tf.cast(index, tf.int64)  \n",
    "\n",
    "labeled_data_sets = []\n",
    "\n",
    "for i, file_name in enumerate(FILE_NAMES):\n",
    "    lines_dataset = tf.data.TextLineDataset(os.path.join(parent_dir, file_name))\n",
    "    labeled_dataset = lines_dataset.map(lambda ex: labeler(ex, i))\n",
    "    labeled_data_sets.append(labeled_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labeled_data_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将这些带标签的数据集合并为一个数据集，然后对其进行随机排序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 50000\n",
    "BATCH_SIZE = 64\n",
    "TAKE_SIZE = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labeled_data = labeled_data_sets[0]\n",
    "for labeled_dataset in labeled_data_sets[1:]:\n",
    "    all_labeled_data = all_labeled_data.concatenate(labeled_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labeled_data = all_labeled_data.shuffle(\n",
    "    BUFFER_SIZE, reshuffle_each_iteration=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您可以使用tf.data.Dataset.take和print来查看(example, label)配对的外观。该numpy属性显示每个张量的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: id=202, shape=(), dtype=string, numpy=b'mounted the chariot sick and sorry at heart, while Iris sat beside her'>, <tf.Tensor: id=203, shape=(), dtype=int64, numpy=2>)\n",
      "(<tf.Tensor: id=204, shape=(), dtype=string, numpy=b'Crazed as he is, and by the stroke of Jove'>, <tf.Tensor: id=205, shape=(), dtype=int64, numpy=0>)\n",
      "(<tf.Tensor: id=206, shape=(), dtype=string, numpy=b\"Slew and despoil'd, and through the Grecian host\">, <tf.Tensor: id=207, shape=(), dtype=int64, numpy=0>)\n",
      "(<tf.Tensor: id=208, shape=(), dtype=string, numpy=b'Then he called on his horses and said to them, \"Keep your pace, and'>, <tf.Tensor: id=209, shape=(), dtype=int64, numpy=2>)\n",
      "(<tf.Tensor: id=210, shape=(), dtype=string, numpy=b'Disease portends to miserable man;'>, <tf.Tensor: id=211, shape=(), dtype=int64, numpy=0>)\n"
     ]
    }
   ],
   "source": [
    "for ex in all_labeled_data.take(5):\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、编码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将文本行编码为数字\n",
    "机器学习模型处理数字而不是单词，因此需要将字符串值转换为数字列表。为此，请将每个唯一的单词映射到唯一的整数。\n",
    "\n",
    "建立词汇\n",
    "首先，通过将文本标记为单个独特单词的集合来建立词汇表。TensorFlow和Python中都有几种方法可以做到这一点。对于本教程：\n",
    "\n",
    "1. 遍历每个示例的numpy值。\n",
    "1. 用于tfds.features.text.Tokenizer将其拆分为令牌。\n",
    "1. 将这些令牌收集到Python集中，以删除重复项。\n",
    "1. 获取词汇表的大小以备后用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17178"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = tfds.features.text.Tokenizer()\n",
    "\n",
    "vocabulary_set = set()\n",
    "for text_tensor, _ in all_labeled_data:\n",
    "    some_tokens = tokenizer.tokenize(text_tensor.numpy())\n",
    "    vocabulary_set.update(some_tokens)\n",
    "\n",
    "vocab_size = len(vocabulary_set)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "编码示例<br>\n",
    "将传递给，vocabulary_set以创建编码器tfds.features.text.TokenTextEncoder。编码器的encode方法接收一个文本字符串，并返回一个整数列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tfds.features.text.TokenTextEncoder(vocabulary_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您可以在一行上尝试一下，以查看输出结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'mounted the chariot sick and sorry at heart, while Iris sat beside her'\n"
     ]
    }
   ],
   "source": [
    "example_text = next(iter(all_labeled_data))[0].numpy()\n",
    "print(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9077, 16923, 7733, 10293, 10794, 9463, 8400, 8203, 13312, 3596, 17020, 1323, 17007]\n"
     ]
    }
   ],
   "source": [
    "encoded_example = encoder.encode(example_text)\n",
    "print(encoded_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，通过将编码器包装tf.py_function并传递到数据集的map方法，从而在数据集上运行编码器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text_tensor, label):\n",
    "    encoded_text = encoder.encode(text_tensor.numpy())\n",
    "    return encoded_text, label\n",
    "\n",
    "def encode_map_fn(text, label):\n",
    "    return tf.py_function(encode, inp=[text, label], Tout=(tf.int64, tf.int64))\n",
    "\n",
    "all_encoded_data = all_labeled_data.map(encode_map_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将数据集拆分为测试和训练批次\n",
    "\n",
    "使用tf.data.Dataset.take和tf.data.Dataset.skip创建一个小的测试数据集和一个更大的训练集。\n",
    "\n",
    "在传递到模型之前，需要对数据集进行批处理。通常，批内的示例必须具有相同的大小和形状。但是，这些数据集中的示例的大小并不完全相同-每一行文本的单词数量不同。因此，请使用tf.data.Dataset.padded_batch（而不是batch）将示例填充为相同大小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3、构造训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = all_encoded_data.skip(TAKE_SIZE).shuffle(BUFFER_SIZE)\n",
    "train_data = train_data.padded_batch(BATCH_SIZE, padded_shapes=([-1],[]))\n",
    "\n",
    "test_data = all_encoded_data.take(TAKE_SIZE)\n",
    "test_data = test_data.padded_batch(BATCH_SIZE, padded_shapes=([-1],[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，test_data和train_data不是（example, label）对的集合，而是批次的集合。每一批是一对（很多示例，很多标签），以数组表示。\n",
    "\n",
    "为了显示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=99675, shape=(16,), dtype=int64, numpy=\n",
       " array([ 9077, 16923,  7733, 10293, 10794,  9463,  8400,  8203, 13312,\n",
       "         3596, 17020,  1323, 17007,     0,     0,     0], dtype=int64)>,\n",
       " <tf.Tensor: id=99679, shape=(), dtype=int64, numpy=2>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text, sample_labels = next(iter(test_data))\n",
    "\n",
    "sample_text[0], sample_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 16])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自从我们引入了新的令牌编码（用于填充的零）以来，词汇量就增加了一个。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4、构建模型，并训练测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一层将整数表示转换为密集的矢量嵌入。有关更多详细信息，请参见Word Embeddings教程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Embedding(vocab_size, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下一层是“ 长期短期记忆”层，它使模型可以将上下文中的单词与其他单词一起理解。LSTM上的双向包装器可帮助它了解与之前和之后的数据点相关的数据点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，我们将具有一系列由一个或多个紧密连接的层组成的层，最后一层是输出层。输出层为所有标签产生概率。可能性最高的是示例标签的模型预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One or more dense layers.\n",
    "# Edit the list in the `for` line to experiment with layer sizes.\n",
    "for units in [64, 64]:\n",
    "    model.add(tf.keras.layers.Dense(units, activation='relu'))\n",
    "\n",
    "# Output layer. The first argument is the number of labels.\n",
    "model.add(tf.keras.layers.Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，编译模型。对于softmax分类模型，sparse_categorical_crossentropy用作损失函数。您可以尝试其他优化器，但这adam很常见。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练模型\n",
    "\n",
    "在此数据上运行的该模型产生了不错的结果（约83％）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    168/Unknown - 16s 16s/step - loss: 1.0982 - accuracy: 0.37 - 16s 8s/step - loss: 1.0961 - accuracy: 0.4062 - 16s 5s/step - loss: 1.0943 - accuracy: 0.390 - 16s 4s/step - loss: 1.0942 - accuracy: 0.359 - 16s 3s/step - loss: 1.0916 - accuracy: 0.368 - 16s 3s/step - loss: 1.0892 - accuracy: 0.372 - 16s 2s/step - loss: 1.0877 - accuracy: 0.361 - 16s 2s/step - loss: 1.0849 - accuracy: 0.361 - 16s 2s/step - loss: 1.0834 - accuracy: 0.357 - 16s 2s/step - loss: 1.0787 - accuracy: 0.373 - 16s 1s/step - loss: 1.0758 - accuracy: 0.379 - 16s 1s/step - loss: 1.0744 - accuracy: 0.381 - 16s 1s/step - loss: 1.0713 - accuracy: 0.384 - 16s 1s/step - loss: 1.0680 - accuracy: 0.383 - 16s 1s/step - loss: 1.0644 - accuracy: 0.389 - 16s 1s/step - loss: 1.0644 - accuracy: 0.389 - 16s 942ms/step - loss: 1.0613 - accuracy: 0.39 - 16s 891ms/step - loss: 1.0566 - accuracy: 0.39 - 16s 845ms/step - loss: 1.0551 - accuracy: 0.38 - 16s 804ms/step - loss: 1.0505 - accuracy: 0.39 - 16s 767ms/step - loss: 1.0463 - accuracy: 0.39 - 16s 733ms/step - loss: 1.0398 - accuracy: 0.39 - 16s 702ms/step - loss: 1.0357 - accuracy: 0.40 - 16s 673ms/step - loss: 1.0313 - accuracy: 0.40 - 16s 647ms/step - loss: 1.0279 - accuracy: 0.41 - 16s 623ms/step - loss: 1.0259 - accuracy: 0.41 - 16s 601ms/step - loss: 1.0213 - accuracy: 0.42 - 16s 580ms/step - loss: 1.0143 - accuracy: 0.42 - 16s 561ms/step - loss: 1.0073 - accuracy: 0.43 - 16s 543ms/step - loss: 1.0020 - accuracy: 0.44 - 16s 526ms/step - loss: 0.9972 - accuracy: 0.45 - 16s 510ms/step - loss: 0.9901 - accuracy: 0.45 - 16s 495ms/step - loss: 0.9853 - accuracy: 0.45 - 16s 481ms/step - loss: 0.9779 - accuracy: 0.46 - 16s 468ms/step - loss: 0.9737 - accuracy: 0.46 - 16s 455ms/step - loss: 0.9690 - accuracy: 0.47 - 16s 444ms/step - loss: 0.9647 - accuracy: 0.47 - 16s 432ms/step - loss: 0.9603 - accuracy: 0.47 - 16s 422ms/step - loss: 0.9518 - accuracy: 0.48 - 16s 412ms/step - loss: 0.9474 - accuracy: 0.48 - 16s 402ms/step - loss: 0.9408 - accuracy: 0.49 - 17s 393ms/step - loss: 0.9331 - accuracy: 0.49 - 17s 385ms/step - loss: 0.9274 - accuracy: 0.49 - 17s 376ms/step - loss: 0.9203 - accuracy: 0.49 - 17s 368ms/step - loss: 0.9133 - accuracy: 0.50 - 17s 361ms/step - loss: 0.9079 - accuracy: 0.50 - 17s 354ms/step - loss: 0.9015 - accuracy: 0.51 - 17s 347ms/step - loss: 0.8946 - accuracy: 0.51 - 17s 340ms/step - loss: 0.8889 - accuracy: 0.51 - 17s 334ms/step - loss: 0.8854 - accuracy: 0.51 - 17s 327ms/step - loss: 0.8791 - accuracy: 0.51 - 17s 321ms/step - loss: 0.8736 - accuracy: 0.51 - 17s 316ms/step - loss: 0.8685 - accuracy: 0.52 - 17s 310ms/step - loss: 0.8631 - accuracy: 0.52 - 17s 305ms/step - loss: 0.8573 - accuracy: 0.52 - 17s 300ms/step - loss: 0.8582 - accuracy: 0.52 - 17s 295ms/step - loss: 0.8547 - accuracy: 0.52 - 17s 290ms/step - loss: 0.8519 - accuracy: 0.52 - 17s 286ms/step - loss: 0.8478 - accuracy: 0.53 - 17s 281ms/step - loss: 0.8440 - accuracy: 0.53 - 17s 277ms/step - loss: 0.8431 - accuracy: 0.53 - 17s 273ms/step - loss: 0.8387 - accuracy: 0.53 - 17s 269ms/step - loss: 0.8359 - accuracy: 0.53 - 17s 265ms/step - loss: 0.8378 - accuracy: 0.53 - 17s 261ms/step - loss: 0.8363 - accuracy: 0.53 - 17s 258ms/step - loss: 0.8324 - accuracy: 0.54 - 17s 254ms/step - loss: 0.8280 - accuracy: 0.54 - 17s 251ms/step - loss: 0.8266 - accuracy: 0.54 - 17s 247ms/step - loss: 0.8242 - accuracy: 0.54 - 17s 244ms/step - loss: 0.8225 - accuracy: 0.54 - 17s 241ms/step - loss: 0.8200 - accuracy: 0.54 - 17s 238ms/step - loss: 0.8169 - accuracy: 0.54 - 17s 235ms/step - loss: 0.8146 - accuracy: 0.54 - 17s 232ms/step - loss: 0.8142 - accuracy: 0.54 - 17s 229ms/step - loss: 0.8120 - accuracy: 0.55 - 17s 226ms/step - loss: 0.8099 - accuracy: 0.55 - 17s 224ms/step - loss: 0.8085 - accuracy: 0.55 - 17s 221ms/step - loss: 0.8072 - accuracy: 0.55 - 17s 218ms/step - loss: 0.8057 - accuracy: 0.55 - 17s 216ms/step - loss: 0.8037 - accuracy: 0.55 - 17s 214ms/step - loss: 0.8015 - accuracy: 0.55 - 17s 211ms/step - loss: 0.7998 - accuracy: 0.55 - 17s 209ms/step - loss: 0.7969 - accuracy: 0.55 - 17s 207ms/step - loss: 0.7946 - accuracy: 0.56 - 17s 204ms/step - loss: 0.7926 - accuracy: 0.56 - 17s 202ms/step - loss: 0.7921 - accuracy: 0.56 - 17s 200ms/step - loss: 0.7905 - accuracy: 0.56 - 17s 198ms/step - loss: 0.7889 - accuracy: 0.56 - 17s 196ms/step - loss: 0.7862 - accuracy: 0.56 - 17s 194ms/step - loss: 0.7858 - accuracy: 0.56 - 18s 192ms/step - loss: 0.7837 - accuracy: 0.56 - 18s 190ms/step - loss: 0.7818 - accuracy: 0.56 - 18s 189ms/step - loss: 0.7808 - accuracy: 0.56 - 18s 187ms/step - loss: 0.7791 - accuracy: 0.56 - 18s 185ms/step - loss: 0.7775 - accuracy: 0.56 - 18s 183ms/step - loss: 0.7769 - accuracy: 0.56 - 18s 182ms/step - loss: 0.7766 - accuracy: 0.56 - 18s 180ms/step - loss: 0.7749 - accuracy: 0.56 - 18s 178ms/step - loss: 0.7744 - accuracy: 0.57 - 18s 177ms/step - loss: 0.7746 - accuracy: 0.56 - 18s 175ms/step - loss: 0.7736 - accuracy: 0.57 - 18s 174ms/step - loss: 0.7718 - accuracy: 0.57 - 18s 172ms/step - loss: 0.7718 - accuracy: 0.57 - 18s 171ms/step - loss: 0.7705 - accuracy: 0.57 - 18s 169ms/step - loss: 0.7701 - accuracy: 0.57 - 18s 168ms/step - loss: 0.7689 - accuracy: 0.57 - 18s 166ms/step - loss: 0.7679 - accuracy: 0.57 - 18s 165ms/step - loss: 0.7668 - accuracy: 0.57 - 18s 164ms/step - loss: 0.7650 - accuracy: 0.57 - 18s 162ms/step - loss: 0.7629 - accuracy: 0.57 - 18s 161ms/step - loss: 0.7611 - accuracy: 0.57 - 18s 160ms/step - loss: 0.7594 - accuracy: 0.57 - 18s 159ms/step - loss: 0.7578 - accuracy: 0.57 - 18s 157ms/step - loss: 0.7567 - accuracy: 0.57 - 18s 156ms/step - loss: 0.7562 - accuracy: 0.57 - 18s 155ms/step - loss: 0.7553 - accuracy: 0.57 - 18s 154ms/step - loss: 0.7549 - accuracy: 0.57 - 18s 153ms/step - loss: 0.7537 - accuracy: 0.57 - 18s 152ms/step - loss: 0.7535 - accuracy: 0.57 - 18s 151ms/step - loss: 0.7526 - accuracy: 0.57 - 18s 150ms/step - loss: 0.7512 - accuracy: 0.57 - 18s 148ms/step - loss: 0.7493 - accuracy: 0.58 - 18s 147ms/step - loss: 0.7480 - accuracy: 0.58 - 18s 146ms/step - loss: 0.7465 - accuracy: 0.58 - 18s 145ms/step - loss: 0.7452 - accuracy: 0.58 - 18s 144ms/step - loss: 0.7448 - accuracy: 0.58 - 18s 143ms/step - loss: 0.7448 - accuracy: 0.58 - 18s 142ms/step - loss: 0.7443 - accuracy: 0.58 - 18s 141ms/step - loss: 0.7431 - accuracy: 0.58 - 18s 141ms/step - loss: 0.7429 - accuracy: 0.58 - 18s 140ms/step - loss: 0.7429 - accuracy: 0.58 - 18s 139ms/step - loss: 0.7428 - accuracy: 0.58 - 18s 138ms/step - loss: 0.7426 - accuracy: 0.58 - 18s 137ms/step - loss: 0.7419 - accuracy: 0.58 - 18s 136ms/step - loss: 0.7407 - accuracy: 0.58 - 18s 135ms/step - loss: 0.7394 - accuracy: 0.58 - 18s 134ms/step - loss: 0.7386 - accuracy: 0.58 - 18s 134ms/step - loss: 0.7375 - accuracy: 0.58 - 18s 133ms/step - loss: 0.7370 - accuracy: 0.59 - 18s 132ms/step - loss: 0.7360 - accuracy: 0.59 - 18s 131ms/step - loss: 0.7350 - accuracy: 0.59 - 19s 130ms/step - loss: 0.7333 - accuracy: 0.59 - 19s 130ms/step - loss: 0.7327 - accuracy: 0.59 - 19s 129ms/step - loss: 0.7312 - accuracy: 0.59 - 19s 128ms/step - loss: 0.7304 - accuracy: 0.59 - 19s 127ms/step - loss: 0.7289 - accuracy: 0.59 - 19s 127ms/step - loss: 0.7283 - accuracy: 0.59 - 19s 126ms/step - loss: 0.7279 - accuracy: 0.59 - 19s 125ms/step - loss: 0.7267 - accuracy: 0.59 - 19s 124ms/step - loss: 0.7263 - accuracy: 0.59 - 19s 124ms/step - loss: 0.7258 - accuracy: 0.59 - 19s 123ms/step - loss: 0.7256 - accuracy: 0.59 - 19s 122ms/step - loss: 0.7251 - accuracy: 0.60 - 19s 122ms/step - loss: 0.7245 - accuracy: 0.60 - 19s 121ms/step - loss: 0.7239 - accuracy: 0.60 - 19s 120ms/step - loss: 0.7226 - accuracy: 0.60 - 19s 120ms/step - loss: 0.7221 - accuracy: 0.60 - 19s 119ms/step - loss: 0.7209 - accuracy: 0.60 - 19s 119ms/step - loss: 0.7202 - accuracy: 0.60 - 19s 118ms/step - loss: 0.7202 - accuracy: 0.60 - 19s 117ms/step - loss: 0.7191 - accuracy: 0.60 - 19s 117ms/step - loss: 0.7180 - accuracy: 0.60 - 19s 116ms/step - loss: 0.7181 - accuracy: 0.60 - 19s 116ms/step - loss: 0.7170 - accuracy: 0.60 - 19s 115ms/step - loss: 0.7168 - accuracy: 0.60 - 19s 114ms/step - loss: 0.7161 - accuracy: 0.60 - 19s 114ms/step - loss: 0.7151 - accuracy: 0.60 - 19s 113ms/step - loss: 0.7144 - accuracy: 0.60    335/Unknown - 19s 113ms/step - loss: 0.7134 - accuracy: 0.60 - 19s 112ms/step - loss: 0.7125 - accuracy: 0.60 - 19s 112ms/step - loss: 0.7118 - accuracy: 0.60 - 19s 111ms/step - loss: 0.7117 - accuracy: 0.60 - 19s 111ms/step - loss: 0.7109 - accuracy: 0.61 - 19s 110ms/step - loss: 0.7103 - accuracy: 0.61 - 19s 110ms/step - loss: 0.7098 - accuracy: 0.61 - 19s 109ms/step - loss: 0.7088 - accuracy: 0.61 - 19s 109ms/step - loss: 0.7084 - accuracy: 0.61 - 19s 108ms/step - loss: 0.7076 - accuracy: 0.61 - 19s 108ms/step - loss: 0.7072 - accuracy: 0.61 - 19s 107ms/step - loss: 0.7067 - accuracy: 0.61 - 19s 107ms/step - loss: 0.7062 - accuracy: 0.61 - 19s 106ms/step - loss: 0.7056 - accuracy: 0.61 - 19s 106ms/step - loss: 0.7052 - accuracy: 0.61 - 19s 105ms/step - loss: 0.7052 - accuracy: 0.61 - 19s 105ms/step - loss: 0.7039 - accuracy: 0.61 - 19s 104ms/step - loss: 0.7031 - accuracy: 0.61 - 19s 104ms/step - loss: 0.7028 - accuracy: 0.61 - 19s 103ms/step - loss: 0.7019 - accuracy: 0.61 - 19s 103ms/step - loss: 0.7014 - accuracy: 0.61 - 19s 102ms/step - loss: 0.7010 - accuracy: 0.61 - 19s 102ms/step - loss: 0.7011 - accuracy: 0.61 - 20s 102ms/step - loss: 0.7006 - accuracy: 0.61 - 20s 101ms/step - loss: 0.6994 - accuracy: 0.61 - 20s 101ms/step - loss: 0.6993 - accuracy: 0.61 - 20s 100ms/step - loss: 0.6986 - accuracy: 0.61 - 20s 100ms/step - loss: 0.6977 - accuracy: 0.61 - 20s 100ms/step - loss: 0.6973 - accuracy: 0.61 - 20s 99ms/step - loss: 0.6969 - accuracy: 0.6202 - 20s 99ms/step - loss: 0.6959 - accuracy: 0.621 - 20s 98ms/step - loss: 0.6955 - accuracy: 0.621 - 20s 98ms/step - loss: 0.6956 - accuracy: 0.621 - 20s 98ms/step - loss: 0.6949 - accuracy: 0.621 - 20s 97ms/step - loss: 0.6940 - accuracy: 0.622 - 20s 97ms/step - loss: 0.6929 - accuracy: 0.623 - 20s 96ms/step - loss: 0.6931 - accuracy: 0.623 - 20s 96ms/step - loss: 0.6921 - accuracy: 0.623 - 20s 96ms/step - loss: 0.6913 - accuracy: 0.624 - 20s 95ms/step - loss: 0.6908 - accuracy: 0.624 - 20s 95ms/step - loss: 0.6906 - accuracy: 0.625 - 20s 95ms/step - loss: 0.6899 - accuracy: 0.626 - 20s 94ms/step - loss: 0.6895 - accuracy: 0.626 - 20s 94ms/step - loss: 0.6890 - accuracy: 0.627 - 20s 94ms/step - loss: 0.6886 - accuracy: 0.627 - 20s 93ms/step - loss: 0.6889 - accuracy: 0.627 - 20s 93ms/step - loss: 0.6882 - accuracy: 0.628 - 20s 93ms/step - loss: 0.6872 - accuracy: 0.628 - 20s 92ms/step - loss: 0.6865 - accuracy: 0.629 - 20s 92ms/step - loss: 0.6858 - accuracy: 0.629 - 20s 92ms/step - loss: 0.6850 - accuracy: 0.630 - 20s 91ms/step - loss: 0.6847 - accuracy: 0.630 - 20s 91ms/step - loss: 0.6842 - accuracy: 0.630 - 20s 91ms/step - loss: 0.6838 - accuracy: 0.631 - 20s 90ms/step - loss: 0.6825 - accuracy: 0.632 - 20s 90ms/step - loss: 0.6819 - accuracy: 0.632 - 20s 90ms/step - loss: 0.6823 - accuracy: 0.632 - 20s 89ms/step - loss: 0.6819 - accuracy: 0.632 - 20s 89ms/step - loss: 0.6810 - accuracy: 0.633 - 20s 89ms/step - loss: 0.6801 - accuracy: 0.634 - 20s 88ms/step - loss: 0.6794 - accuracy: 0.634 - 20s 88ms/step - loss: 0.6785 - accuracy: 0.634 - 20s 88ms/step - loss: 0.6773 - accuracy: 0.635 - 20s 88ms/step - loss: 0.6766 - accuracy: 0.636 - 20s 87ms/step - loss: 0.6767 - accuracy: 0.636 - 20s 87ms/step - loss: 0.6769 - accuracy: 0.636 - 20s 87ms/step - loss: 0.6769 - accuracy: 0.636 - 20s 86ms/step - loss: 0.6770 - accuracy: 0.636 - 20s 86ms/step - loss: 0.6769 - accuracy: 0.637 - 20s 86ms/step - loss: 0.6765 - accuracy: 0.637 - 20s 86ms/step - loss: 0.6758 - accuracy: 0.637 - 20s 85ms/step - loss: 0.6755 - accuracy: 0.638 - 20s 85ms/step - loss: 0.6749 - accuracy: 0.638 - 21s 85ms/step - loss: 0.6736 - accuracy: 0.639 - 21s 85ms/step - loss: 0.6729 - accuracy: 0.639 - 21s 84ms/step - loss: 0.6727 - accuracy: 0.640 - 21s 84ms/step - loss: 0.6726 - accuracy: 0.640 - 21s 84ms/step - loss: 0.6717 - accuracy: 0.641 - 21s 83ms/step - loss: 0.6713 - accuracy: 0.641 - 21s 83ms/step - loss: 0.6706 - accuracy: 0.641 - 21s 83ms/step - loss: 0.6702 - accuracy: 0.641 - 21s 83ms/step - loss: 0.6701 - accuracy: 0.642 - 21s 82ms/step - loss: 0.6695 - accuracy: 0.642 - 21s 82ms/step - loss: 0.6690 - accuracy: 0.643 - 21s 82ms/step - loss: 0.6680 - accuracy: 0.643 - 21s 82ms/step - loss: 0.6675 - accuracy: 0.644 - 21s 81ms/step - loss: 0.6673 - accuracy: 0.644 - 21s 81ms/step - loss: 0.6670 - accuracy: 0.645 - 21s 81ms/step - loss: 0.6666 - accuracy: 0.645 - 21s 81ms/step - loss: 0.6660 - accuracy: 0.645 - 21s 81ms/step - loss: 0.6654 - accuracy: 0.646 - 21s 80ms/step - loss: 0.6650 - accuracy: 0.646 - 21s 80ms/step - loss: 0.6647 - accuracy: 0.646 - 21s 80ms/step - loss: 0.6643 - accuracy: 0.647 - 21s 80ms/step - loss: 0.6644 - accuracy: 0.646 - 21s 79ms/step - loss: 0.6641 - accuracy: 0.647 - 21s 79ms/step - loss: 0.6634 - accuracy: 0.647 - 21s 79ms/step - loss: 0.6631 - accuracy: 0.648 - 21s 79ms/step - loss: 0.6627 - accuracy: 0.648 - 21s 78ms/step - loss: 0.6619 - accuracy: 0.648 - 21s 78ms/step - loss: 0.6616 - accuracy: 0.649 - 21s 78ms/step - loss: 0.6609 - accuracy: 0.649 - 21s 78ms/step - loss: 0.6604 - accuracy: 0.649 - 21s 78ms/step - loss: 0.6596 - accuracy: 0.650 - 21s 77ms/step - loss: 0.6594 - accuracy: 0.650 - 21s 77ms/step - loss: 0.6589 - accuracy: 0.651 - 21s 77ms/step - loss: 0.6588 - accuracy: 0.651 - 21s 77ms/step - loss: 0.6583 - accuracy: 0.651 - 21s 77ms/step - loss: 0.6580 - accuracy: 0.651 - 21s 76ms/step - loss: 0.6575 - accuracy: 0.652 - 21s 76ms/step - loss: 0.6575 - accuracy: 0.652 - 21s 76ms/step - loss: 0.6568 - accuracy: 0.652 - 21s 76ms/step - loss: 0.6562 - accuracy: 0.652 - 21s 76ms/step - loss: 0.6557 - accuracy: 0.653 - 21s 75ms/step - loss: 0.6551 - accuracy: 0.653 - 21s 75ms/step - loss: 0.6548 - accuracy: 0.653 - 21s 75ms/step - loss: 0.6541 - accuracy: 0.654 - 21s 75ms/step - loss: 0.6536 - accuracy: 0.654 - 21s 75ms/step - loss: 0.6534 - accuracy: 0.655 - 21s 74ms/step - loss: 0.6526 - accuracy: 0.655 - 21s 74ms/step - loss: 0.6521 - accuracy: 0.656 - 21s 74ms/step - loss: 0.6523 - accuracy: 0.656 - 21s 74ms/step - loss: 0.6519 - accuracy: 0.656 - 22s 74ms/step - loss: 0.6513 - accuracy: 0.656 - 22s 74ms/step - loss: 0.6513 - accuracy: 0.656 - 22s 73ms/step - loss: 0.6507 - accuracy: 0.657 - 22s 73ms/step - loss: 0.6505 - accuracy: 0.657 - 22s 73ms/step - loss: 0.6500 - accuracy: 0.657 - 22s 73ms/step - loss: 0.6496 - accuracy: 0.657 - 22s 73ms/step - loss: 0.6492 - accuracy: 0.658 - 22s 72ms/step - loss: 0.6486 - accuracy: 0.658 - 22s 72ms/step - loss: 0.6476 - accuracy: 0.659 - 22s 72ms/step - loss: 0.6470 - accuracy: 0.659 - 22s 72ms/step - loss: 0.6465 - accuracy: 0.660 - 22s 72ms/step - loss: 0.6459 - accuracy: 0.661 - 22s 72ms/step - loss: 0.6454 - accuracy: 0.661 - 22s 71ms/step - loss: 0.6449 - accuracy: 0.661 - 22s 71ms/step - loss: 0.6442 - accuracy: 0.661 - 22s 71ms/step - loss: 0.6434 - accuracy: 0.662 - 22s 71ms/step - loss: 0.6429 - accuracy: 0.662 - 22s 71ms/step - loss: 0.6423 - accuracy: 0.663 - 22s 71ms/step - loss: 0.6416 - accuracy: 0.664 - 22s 70ms/step - loss: 0.6406 - accuracy: 0.664 - 22s 70ms/step - loss: 0.6402 - accuracy: 0.665 - 22s 70ms/step - loss: 0.6396 - accuracy: 0.665 - 22s 70ms/step - loss: 0.6390 - accuracy: 0.665 - 22s 70ms/step - loss: 0.6377 - accuracy: 0.666 - 22s 70ms/step - loss: 0.6378 - accuracy: 0.666 - 22s 69ms/step - loss: 0.6370 - accuracy: 0.667 - 22s 69ms/step - loss: 0.6359 - accuracy: 0.667 - 22s 69ms/step - loss: 0.6353 - accuracy: 0.668 - 22s 69ms/step - loss: 0.6348 - accuracy: 0.668 - 22s 69ms/step - loss: 0.6340 - accuracy: 0.668 - 22s 69ms/step - loss: 0.6340 - accuracy: 0.669 - 22s 69ms/step - loss: 0.6335 - accuracy: 0.669 - 22s 68ms/step - loss: 0.6330 - accuracy: 0.669 - 22s 68ms/step - loss: 0.6325 - accuracy: 0.669 - 22s 68ms/step - loss: 0.6323 - accuracy: 0.669 - 22s 68ms/step - loss: 0.6314 - accuracy: 0.670 - 22s 68ms/step - loss: 0.6305 - accuracy: 0.671 - 22s 68ms/step - loss: 0.6299 - accuracy: 0.671 - 22s 68ms/step - loss: 0.6293 - accuracy: 0.671 - 22s 67ms/step - loss: 0.6289 - accuracy: 0.672 - 22s 67ms/step - loss: 0.6285 - accuracy: 0.672 - 22s 67ms/step - loss: 0.6281 - accuracy: 0.672 - 22s 67ms/step - loss: 0.6273 - accuracy: 0.673 - 22s 67ms/step - loss: 0.6272 - accuracy: 0.6733"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    502/Unknown - 22s 67ms/step - loss: 0.6268 - accuracy: 0.673 - 22s 67ms/step - loss: 0.6264 - accuracy: 0.673 - 22s 66ms/step - loss: 0.6255 - accuracy: 0.674 - 22s 66ms/step - loss: 0.6254 - accuracy: 0.674 - 22s 66ms/step - loss: 0.6248 - accuracy: 0.674 - 23s 66ms/step - loss: 0.6242 - accuracy: 0.675 - 23s 66ms/step - loss: 0.6234 - accuracy: 0.675 - 23s 66ms/step - loss: 0.6235 - accuracy: 0.675 - 23s 66ms/step - loss: 0.6231 - accuracy: 0.676 - 23s 65ms/step - loss: 0.6225 - accuracy: 0.676 - 23s 65ms/step - loss: 0.6219 - accuracy: 0.677 - 23s 65ms/step - loss: 0.6218 - accuracy: 0.677 - 23s 65ms/step - loss: 0.6215 - accuracy: 0.677 - 23s 65ms/step - loss: 0.6211 - accuracy: 0.677 - 23s 65ms/step - loss: 0.6206 - accuracy: 0.677 - 23s 65ms/step - loss: 0.6202 - accuracy: 0.678 - 23s 65ms/step - loss: 0.6197 - accuracy: 0.678 - 23s 64ms/step - loss: 0.6190 - accuracy: 0.679 - 23s 64ms/step - loss: 0.6184 - accuracy: 0.679 - 23s 64ms/step - loss: 0.6177 - accuracy: 0.679 - 23s 64ms/step - loss: 0.6173 - accuracy: 0.680 - 23s 64ms/step - loss: 0.6168 - accuracy: 0.680 - 23s 64ms/step - loss: 0.6162 - accuracy: 0.680 - 23s 64ms/step - loss: 0.6156 - accuracy: 0.681 - 23s 64ms/step - loss: 0.6147 - accuracy: 0.681 - 23s 63ms/step - loss: 0.6145 - accuracy: 0.681 - 23s 63ms/step - loss: 0.6144 - accuracy: 0.682 - 23s 63ms/step - loss: 0.6143 - accuracy: 0.682 - 23s 63ms/step - loss: 0.6141 - accuracy: 0.682 - 23s 63ms/step - loss: 0.6136 - accuracy: 0.682 - 23s 63ms/step - loss: 0.6132 - accuracy: 0.682 - 23s 63ms/step - loss: 0.6126 - accuracy: 0.683 - 23s 63ms/step - loss: 0.6121 - accuracy: 0.683 - 23s 63ms/step - loss: 0.6120 - accuracy: 0.683 - 23s 62ms/step - loss: 0.6119 - accuracy: 0.683 - 23s 62ms/step - loss: 0.6112 - accuracy: 0.684 - 23s 62ms/step - loss: 0.6111 - accuracy: 0.684 - 23s 62ms/step - loss: 0.6107 - accuracy: 0.684 - 23s 62ms/step - loss: 0.6103 - accuracy: 0.685 - 23s 62ms/step - loss: 0.6097 - accuracy: 0.685 - 23s 62ms/step - loss: 0.6093 - accuracy: 0.685 - 23s 62ms/step - loss: 0.6092 - accuracy: 0.686 - 23s 62ms/step - loss: 0.6087 - accuracy: 0.686 - 23s 61ms/step - loss: 0.6085 - accuracy: 0.686 - 23s 61ms/step - loss: 0.6082 - accuracy: 0.686 - 23s 61ms/step - loss: 0.6078 - accuracy: 0.687 - 23s 61ms/step - loss: 0.6075 - accuracy: 0.687 - 23s 61ms/step - loss: 0.6072 - accuracy: 0.687 - 23s 61ms/step - loss: 0.6066 - accuracy: 0.687 - 23s 61ms/step - loss: 0.6065 - accuracy: 0.687 - 23s 61ms/step - loss: 0.6061 - accuracy: 0.688 - 23s 61ms/step - loss: 0.6057 - accuracy: 0.688 - 23s 60ms/step - loss: 0.6051 - accuracy: 0.688 - 23s 60ms/step - loss: 0.6044 - accuracy: 0.689 - 24s 60ms/step - loss: 0.6041 - accuracy: 0.689 - 24s 60ms/step - loss: 0.6037 - accuracy: 0.689 - 24s 60ms/step - loss: 0.6032 - accuracy: 0.690 - 24s 60ms/step - loss: 0.6028 - accuracy: 0.690 - 24s 60ms/step - loss: 0.6026 - accuracy: 0.690 - 24s 60ms/step - loss: 0.6021 - accuracy: 0.690 - 24s 60ms/step - loss: 0.6015 - accuracy: 0.691 - 24s 60ms/step - loss: 0.6008 - accuracy: 0.691 - 24s 59ms/step - loss: 0.6005 - accuracy: 0.691 - 24s 59ms/step - loss: 0.6000 - accuracy: 0.692 - 24s 59ms/step - loss: 0.5996 - accuracy: 0.692 - 24s 59ms/step - loss: 0.5990 - accuracy: 0.692 - 24s 59ms/step - loss: 0.5985 - accuracy: 0.693 - 24s 59ms/step - loss: 0.5980 - accuracy: 0.693 - 24s 59ms/step - loss: 0.5975 - accuracy: 0.693 - 24s 59ms/step - loss: 0.5973 - accuracy: 0.693 - 24s 59ms/step - loss: 0.5969 - accuracy: 0.693 - 24s 59ms/step - loss: 0.5965 - accuracy: 0.694 - 24s 59ms/step - loss: 0.5965 - accuracy: 0.694 - 24s 58ms/step - loss: 0.5958 - accuracy: 0.694 - 24s 58ms/step - loss: 0.5953 - accuracy: 0.694 - 24s 58ms/step - loss: 0.5951 - accuracy: 0.695 - 24s 58ms/step - loss: 0.5947 - accuracy: 0.695 - 24s 58ms/step - loss: 0.5942 - accuracy: 0.695 - 24s 58ms/step - loss: 0.5938 - accuracy: 0.696 - 24s 58ms/step - loss: 0.5933 - accuracy: 0.696 - 24s 58ms/step - loss: 0.5928 - accuracy: 0.696 - 24s 58ms/step - loss: 0.5926 - accuracy: 0.696 - 24s 58ms/step - loss: 0.5922 - accuracy: 0.697 - 24s 57ms/step - loss: 0.5915 - accuracy: 0.697 - 24s 57ms/step - loss: 0.5914 - accuracy: 0.697 - 24s 57ms/step - loss: 0.5909 - accuracy: 0.698 - 24s 57ms/step - loss: 0.5905 - accuracy: 0.698 - 24s 57ms/step - loss: 0.5900 - accuracy: 0.698 - 24s 57ms/step - loss: 0.5899 - accuracy: 0.698 - 24s 57ms/step - loss: 0.5896 - accuracy: 0.699 - 24s 57ms/step - loss: 0.5890 - accuracy: 0.699 - 24s 57ms/step - loss: 0.5883 - accuracy: 0.699 - 24s 57ms/step - loss: 0.5883 - accuracy: 0.699 - 24s 57ms/step - loss: 0.5878 - accuracy: 0.700 - 24s 57ms/step - loss: 0.5876 - accuracy: 0.700 - 24s 56ms/step - loss: 0.5871 - accuracy: 0.700 - 24s 56ms/step - loss: 0.5868 - accuracy: 0.700 - 24s 56ms/step - loss: 0.5862 - accuracy: 0.701 - 24s 56ms/step - loss: 0.5856 - accuracy: 0.701 - 24s 56ms/step - loss: 0.5853 - accuracy: 0.701 - 24s 56ms/step - loss: 0.5848 - accuracy: 0.702 - 24s 56ms/step - loss: 0.5849 - accuracy: 0.702 - 24s 56ms/step - loss: 0.5847 - accuracy: 0.702 - 24s 56ms/step - loss: 0.5842 - accuracy: 0.702 - 25s 56ms/step - loss: 0.5839 - accuracy: 0.702 - 25s 56ms/step - loss: 0.5835 - accuracy: 0.703 - 25s 56ms/step - loss: 0.5830 - accuracy: 0.703 - 25s 55ms/step - loss: 0.5827 - accuracy: 0.703 - 25s 55ms/step - loss: 0.5824 - accuracy: 0.703 - 25s 55ms/step - loss: 0.5820 - accuracy: 0.703 - 25s 55ms/step - loss: 0.5820 - accuracy: 0.704 - 25s 55ms/step - loss: 0.5815 - accuracy: 0.704 - 25s 55ms/step - loss: 0.5810 - accuracy: 0.704 - 25s 55ms/step - loss: 0.5806 - accuracy: 0.704 - 25s 55ms/step - loss: 0.5803 - accuracy: 0.705 - 25s 55ms/step - loss: 0.5799 - accuracy: 0.705 - 25s 55ms/step - loss: 0.5794 - accuracy: 0.705 - 25s 55ms/step - loss: 0.5792 - accuracy: 0.705 - 25s 55ms/step - loss: 0.5786 - accuracy: 0.706 - 25s 55ms/step - loss: 0.5783 - accuracy: 0.706 - 25s 54ms/step - loss: 0.5778 - accuracy: 0.706 - 25s 54ms/step - loss: 0.5777 - accuracy: 0.706 - 25s 54ms/step - loss: 0.5774 - accuracy: 0.706 - 25s 54ms/step - loss: 0.5768 - accuracy: 0.707 - 25s 54ms/step - loss: 0.5763 - accuracy: 0.707 - 25s 54ms/step - loss: 0.5760 - accuracy: 0.707 - 25s 54ms/step - loss: 0.5753 - accuracy: 0.708 - 25s 54ms/step - loss: 0.5750 - accuracy: 0.708 - 25s 54ms/step - loss: 0.5744 - accuracy: 0.708 - 25s 54ms/step - loss: 0.5742 - accuracy: 0.708 - 25s 54ms/step - loss: 0.5740 - accuracy: 0.709 - 25s 54ms/step - loss: 0.5737 - accuracy: 0.709 - 25s 54ms/step - loss: 0.5735 - accuracy: 0.709 - 25s 54ms/step - loss: 0.5735 - accuracy: 0.709 - 25s 53ms/step - loss: 0.5733 - accuracy: 0.709 - 25s 53ms/step - loss: 0.5730 - accuracy: 0.709 - 25s 53ms/step - loss: 0.5725 - accuracy: 0.710 - 25s 53ms/step - loss: 0.5722 - accuracy: 0.710 - 25s 53ms/step - loss: 0.5720 - accuracy: 0.710 - 25s 53ms/step - loss: 0.5717 - accuracy: 0.710 - 25s 53ms/step - loss: 0.5715 - accuracy: 0.710 - 25s 53ms/step - loss: 0.5711 - accuracy: 0.710 - 25s 53ms/step - loss: 0.5713 - accuracy: 0.710 - 25s 53ms/step - loss: 0.5711 - accuracy: 0.710 - 25s 53ms/step - loss: 0.5707 - accuracy: 0.711 - 25s 53ms/step - loss: 0.5706 - accuracy: 0.711 - 25s 53ms/step - loss: 0.5704 - accuracy: 0.711 - 25s 53ms/step - loss: 0.5700 - accuracy: 0.711 - 25s 52ms/step - loss: 0.5696 - accuracy: 0.711 - 25s 52ms/step - loss: 0.5692 - accuracy: 0.711 - 25s 52ms/step - loss: 0.5688 - accuracy: 0.712 - 25s 52ms/step - loss: 0.5686 - accuracy: 0.712 - 25s 52ms/step - loss: 0.5684 - accuracy: 0.712 - 25s 52ms/step - loss: 0.5682 - accuracy: 0.712 - 26s 52ms/step - loss: 0.5681 - accuracy: 0.712 - 26s 52ms/step - loss: 0.5679 - accuracy: 0.712 - 26s 52ms/step - loss: 0.5674 - accuracy: 0.713 - 26s 52ms/step - loss: 0.5672 - accuracy: 0.713 - 26s 52ms/step - loss: 0.5670 - accuracy: 0.713 - 26s 52ms/step - loss: 0.5668 - accuracy: 0.713 - 26s 52ms/step - loss: 0.5668 - accuracy: 0.713 - 26s 52ms/step - loss: 0.5668 - accuracy: 0.713 - 26s 52ms/step - loss: 0.5666 - accuracy: 0.713 - 26s 51ms/step - loss: 0.5663 - accuracy: 0.713 - 26s 51ms/step - loss: 0.5663 - accuracy: 0.713 - 26s 51ms/step - loss: 0.5660 - accuracy: 0.713 - 26s 51ms/step - loss: 0.5658 - accuracy: 0.7137"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    669/Unknown - 26s 51ms/step - loss: 0.5654 - accuracy: 0.714 - 26s 51ms/step - loss: 0.5651 - accuracy: 0.714 - 26s 51ms/step - loss: 0.5649 - accuracy: 0.714 - 26s 51ms/step - loss: 0.5647 - accuracy: 0.714 - 26s 51ms/step - loss: 0.5645 - accuracy: 0.714 - 26s 51ms/step - loss: 0.5643 - accuracy: 0.714 - 26s 51ms/step - loss: 0.5640 - accuracy: 0.715 - 26s 51ms/step - loss: 0.5638 - accuracy: 0.715 - 26s 51ms/step - loss: 0.5635 - accuracy: 0.715 - 26s 51ms/step - loss: 0.5632 - accuracy: 0.715 - 26s 51ms/step - loss: 0.5629 - accuracy: 0.715 - 26s 51ms/step - loss: 0.5630 - accuracy: 0.715 - 26s 51ms/step - loss: 0.5628 - accuracy: 0.715 - 26s 50ms/step - loss: 0.5624 - accuracy: 0.715 - 26s 50ms/step - loss: 0.5619 - accuracy: 0.716 - 26s 50ms/step - loss: 0.5617 - accuracy: 0.716 - 26s 50ms/step - loss: 0.5615 - accuracy: 0.716 - 26s 50ms/step - loss: 0.5612 - accuracy: 0.716 - 26s 50ms/step - loss: 0.5609 - accuracy: 0.716 - 26s 50ms/step - loss: 0.5605 - accuracy: 0.716 - 26s 50ms/step - loss: 0.5603 - accuracy: 0.716 - 26s 50ms/step - loss: 0.5600 - accuracy: 0.717 - 26s 50ms/step - loss: 0.5596 - accuracy: 0.717 - 26s 50ms/step - loss: 0.5592 - accuracy: 0.717 - 26s 50ms/step - loss: 0.5589 - accuracy: 0.717 - 26s 50ms/step - loss: 0.5584 - accuracy: 0.717 - 26s 50ms/step - loss: 0.5581 - accuracy: 0.718 - 26s 50ms/step - loss: 0.5578 - accuracy: 0.718 - 26s 50ms/step - loss: 0.5575 - accuracy: 0.718 - 26s 50ms/step - loss: 0.5573 - accuracy: 0.718 - 26s 49ms/step - loss: 0.5573 - accuracy: 0.718 - 26s 49ms/step - loss: 0.5568 - accuracy: 0.718 - 26s 49ms/step - loss: 0.5565 - accuracy: 0.719 - 26s 49ms/step - loss: 0.5562 - accuracy: 0.719 - 26s 49ms/step - loss: 0.5561 - accuracy: 0.719 - 26s 49ms/step - loss: 0.5556 - accuracy: 0.719 - 26s 49ms/step - loss: 0.5552 - accuracy: 0.719 - 27s 49ms/step - loss: 0.5549 - accuracy: 0.720 - 27s 49ms/step - loss: 0.5547 - accuracy: 0.720 - 27s 49ms/step - loss: 0.5544 - accuracy: 0.720 - 27s 49ms/step - loss: 0.5540 - accuracy: 0.720 - 27s 49ms/step - loss: 0.5536 - accuracy: 0.720 - 27s 49ms/step - loss: 0.5534 - accuracy: 0.720 - 27s 49ms/step - loss: 0.5530 - accuracy: 0.721 - 27s 49ms/step - loss: 0.5526 - accuracy: 0.721 - 27s 49ms/step - loss: 0.5522 - accuracy: 0.721 - 27s 49ms/step - loss: 0.5522 - accuracy: 0.721 - 27s 49ms/step - loss: 0.5518 - accuracy: 0.722 - 27s 49ms/step - loss: 0.5514 - accuracy: 0.722 - 27s 48ms/step - loss: 0.5511 - accuracy: 0.722 - 27s 48ms/step - loss: 0.5508 - accuracy: 0.722 - 27s 48ms/step - loss: 0.5511 - accuracy: 0.722 - 27s 48ms/step - loss: 0.5510 - accuracy: 0.722 - 27s 48ms/step - loss: 0.5508 - accuracy: 0.723 - 27s 48ms/step - loss: 0.5505 - accuracy: 0.723 - 27s 48ms/step - loss: 0.5501 - accuracy: 0.723 - 27s 48ms/step - loss: 0.5501 - accuracy: 0.723 - 27s 48ms/step - loss: 0.5499 - accuracy: 0.723 - 27s 48ms/step - loss: 0.5498 - accuracy: 0.723 - 27s 48ms/step - loss: 0.5495 - accuracy: 0.723 - 27s 48ms/step - loss: 0.5492 - accuracy: 0.723 - 27s 48ms/step - loss: 0.5489 - accuracy: 0.723 - 27s 48ms/step - loss: 0.5486 - accuracy: 0.724 - 27s 48ms/step - loss: 0.5483 - accuracy: 0.724 - 27s 48ms/step - loss: 0.5479 - accuracy: 0.724 - 27s 48ms/step - loss: 0.5476 - accuracy: 0.724 - 27s 48ms/step - loss: 0.5473 - accuracy: 0.725 - 27s 48ms/step - loss: 0.5469 - accuracy: 0.725 - 27s 48ms/step - loss: 0.5468 - accuracy: 0.725 - 27s 47ms/step - loss: 0.5467 - accuracy: 0.725 - 27s 47ms/step - loss: 0.5464 - accuracy: 0.725 - 27s 47ms/step - loss: 0.5461 - accuracy: 0.725 - 27s 47ms/step - loss: 0.5458 - accuracy: 0.726 - 27s 47ms/step - loss: 0.5456 - accuracy: 0.726 - 27s 47ms/step - loss: 0.5453 - accuracy: 0.726 - 27s 47ms/step - loss: 0.5451 - accuracy: 0.726 - 27s 47ms/step - loss: 0.5447 - accuracy: 0.726 - 27s 47ms/step - loss: 0.5448 - accuracy: 0.726 - 27s 47ms/step - loss: 0.5444 - accuracy: 0.726 - 27s 47ms/step - loss: 0.5439 - accuracy: 0.727 - 27s 47ms/step - loss: 0.5438 - accuracy: 0.727 - 27s 47ms/step - loss: 0.5434 - accuracy: 0.727 - 27s 47ms/step - loss: 0.5429 - accuracy: 0.727 - 27s 47ms/step - loss: 0.5427 - accuracy: 0.727 - 27s 47ms/step - loss: 0.5427 - accuracy: 0.727 - 27s 47ms/step - loss: 0.5423 - accuracy: 0.728 - 28s 47ms/step - loss: 0.5422 - accuracy: 0.728 - 28s 47ms/step - loss: 0.5419 - accuracy: 0.728 - 28s 47ms/step - loss: 0.5415 - accuracy: 0.728 - 28s 47ms/step - loss: 0.5413 - accuracy: 0.728 - 28s 47ms/step - loss: 0.5409 - accuracy: 0.729 - 28s 46ms/step - loss: 0.5405 - accuracy: 0.729 - 28s 46ms/step - loss: 0.5401 - accuracy: 0.729 - 28s 46ms/step - loss: 0.5400 - accuracy: 0.729 - 28s 46ms/step - loss: 0.5398 - accuracy: 0.729 - 28s 46ms/step - loss: 0.5397 - accuracy: 0.729 - 28s 46ms/step - loss: 0.5395 - accuracy: 0.729 - 28s 46ms/step - loss: 0.5391 - accuracy: 0.730 - 28s 46ms/step - loss: 0.5388 - accuracy: 0.730 - 28s 46ms/step - loss: 0.5387 - accuracy: 0.730 - 28s 46ms/step - loss: 0.5386 - accuracy: 0.730 - 28s 46ms/step - loss: 0.5385 - accuracy: 0.730 - 28s 46ms/step - loss: 0.5383 - accuracy: 0.730 - 28s 46ms/step - loss: 0.5379 - accuracy: 0.730 - 28s 46ms/step - loss: 0.5378 - accuracy: 0.730 - 28s 46ms/step - loss: 0.5376 - accuracy: 0.731 - 28s 46ms/step - loss: 0.5372 - accuracy: 0.731 - 28s 46ms/step - loss: 0.5367 - accuracy: 0.731 - 28s 46ms/step - loss: 0.5365 - accuracy: 0.731 - 28s 46ms/step - loss: 0.5363 - accuracy: 0.731 - 28s 46ms/step - loss: 0.5362 - accuracy: 0.731 - 28s 46ms/step - loss: 0.5361 - accuracy: 0.731 - 28s 46ms/step - loss: 0.5361 - accuracy: 0.731 - 28s 46ms/step - loss: 0.5359 - accuracy: 0.731 - 28s 45ms/step - loss: 0.5356 - accuracy: 0.732 - 28s 45ms/step - loss: 0.5354 - accuracy: 0.732 - 28s 45ms/step - loss: 0.5351 - accuracy: 0.732 - 28s 45ms/step - loss: 0.5348 - accuracy: 0.732 - 28s 45ms/step - loss: 0.5345 - accuracy: 0.732 - 28s 45ms/step - loss: 0.5343 - accuracy: 0.733 - 28s 45ms/step - loss: 0.5342 - accuracy: 0.733 - 28s 45ms/step - loss: 0.5339 - accuracy: 0.733 - 28s 45ms/step - loss: 0.5337 - accuracy: 0.733 - 28s 45ms/step - loss: 0.5334 - accuracy: 0.733 - 28s 45ms/step - loss: 0.5331 - accuracy: 0.733 - 28s 45ms/step - loss: 0.5330 - accuracy: 0.733 - 28s 45ms/step - loss: 0.5329 - accuracy: 0.734 - 28s 45ms/step - loss: 0.5326 - accuracy: 0.734 - 28s 45ms/step - loss: 0.5324 - accuracy: 0.734 - 28s 45ms/step - loss: 0.5320 - accuracy: 0.734 - 28s 45ms/step - loss: 0.5318 - accuracy: 0.734 - 28s 45ms/step - loss: 0.5317 - accuracy: 0.734 - 28s 45ms/step - loss: 0.5316 - accuracy: 0.734 - 28s 45ms/step - loss: 0.5313 - accuracy: 0.735 - 28s 45ms/step - loss: 0.5310 - accuracy: 0.735 - 28s 45ms/step - loss: 0.5307 - accuracy: 0.735 - 29s 45ms/step - loss: 0.5304 - accuracy: 0.735 - 29s 45ms/step - loss: 0.5305 - accuracy: 0.735 - 29s 45ms/step - loss: 0.5302 - accuracy: 0.735 - 29s 44ms/step - loss: 0.5301 - accuracy: 0.735 - 29s 44ms/step - loss: 0.5300 - accuracy: 0.736 - 29s 44ms/step - loss: 0.5298 - accuracy: 0.736 - 29s 44ms/step - loss: 0.5295 - accuracy: 0.736 - 29s 44ms/step - loss: 0.5292 - accuracy: 0.736 - 29s 44ms/step - loss: 0.5291 - accuracy: 0.736 - 29s 44ms/step - loss: 0.5288 - accuracy: 0.736 - 29s 44ms/step - loss: 0.5285 - accuracy: 0.736 - 29s 44ms/step - loss: 0.5283 - accuracy: 0.737 - 29s 44ms/step - loss: 0.5283 - accuracy: 0.736 - 29s 44ms/step - loss: 0.5279 - accuracy: 0.737 - 29s 44ms/step - loss: 0.5276 - accuracy: 0.737 - 29s 44ms/step - loss: 0.5273 - accuracy: 0.737 - 29s 44ms/step - loss: 0.5271 - accuracy: 0.737 - 29s 44ms/step - loss: 0.5269 - accuracy: 0.737 - 29s 44ms/step - loss: 0.5269 - accuracy: 0.737 - 29s 44ms/step - loss: 0.5267 - accuracy: 0.738 - 29s 44ms/step - loss: 0.5265 - accuracy: 0.738 - 29s 44ms/step - loss: 0.5264 - accuracy: 0.738 - 29s 44ms/step - loss: 0.5261 - accuracy: 0.738 - 29s 44ms/step - loss: 0.5261 - accuracy: 0.738 - 29s 44ms/step - loss: 0.5261 - accuracy: 0.738 - 29s 44ms/step - loss: 0.5259 - accuracy: 0.738 - 29s 44ms/step - loss: 0.5257 - accuracy: 0.738 - 29s 44ms/step - loss: 0.5254 - accuracy: 0.738 - 29s 44ms/step - loss: 0.5253 - accuracy: 0.738 - 29s 44ms/step - loss: 0.5251 - accuracy: 0.738 - 29s 43ms/step - loss: 0.5249 - accuracy: 0.7390"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "697/697 [==============================]0.5247 - accuracy: 0.739 - 29s 43ms/step - loss: 0.5245 - accuracy: 0.739 - 29s 43ms/step - loss: 0.5243 - accuracy: 0.739 - 29s 43ms/step - loss: 0.5240 - accuracy: 0.739 - 29s 43ms/step - loss: 0.5236 - accuracy: 0.739 - 29s 43ms/step - loss: 0.5234 - accuracy: 0.739 - 29s 43ms/step - loss: 0.5232 - accuracy: 0.739 - 29s 43ms/step - loss: 0.5229 - accuracy: 0.740 - 29s 43ms/step - loss: 0.5227 - accuracy: 0.740 - 29s 43ms/step - loss: 0.5225 - accuracy: 0.740 - 29s 43ms/step - loss: 0.5221 - accuracy: 0.740 - 29s 43ms/step - loss: 0.5217 - accuracy: 0.740 - 29s 43ms/step - loss: 0.5216 - accuracy: 0.741 - 29s 43ms/step - loss: 0.5215 - accuracy: 0.741 - 29s 43ms/step - loss: 0.5211 - accuracy: 0.741 - 29s 43ms/step - loss: 0.5209 - accuracy: 0.741 - 29s 43ms/step - loss: 0.5206 - accuracy: 0.741 - 29s 43ms/step - loss: 0.5204 - accuracy: 0.741 - 29s 43ms/step - loss: 0.5201 - accuracy: 0.741 - 29s 43ms/step - loss: 0.5198 - accuracy: 0.742 - 30s 43ms/step - loss: 0.5196 - accuracy: 0.742 - 30s 43ms/step - loss: 0.5195 - accuracy: 0.742 - 30s 43ms/step - loss: 0.5191 - accuracy: 0.742 - 30s 43ms/step - loss: 0.5187 - accuracy: 0.742 - 30s 43ms/step - loss: 0.5186 - accuracy: 0.742 - 30s 43ms/step - loss: 0.5184 - accuracy: 0.742 - 30s 43ms/step - loss: 0.5181 - accuracy: 0.743 - 30s 43ms/step - loss: 0.5178 - accuracy: 0.743 - 34s 48ms/step - loss: 0.5178 - accuracy: 0.7432 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/3\n",
      "697/697 [==============================] ETA: 14:29 - loss: 0.3767 - accuracy: 0.812 - ETA: 3:30 - loss: 0.3449 - accuracy: 0.851 - ETA: 1:55 - loss: 0.3067 - accuracy: 0.85 - ETA: 1:18 - loss: 0.3011 - accuracy: 0.85 - ETA: 57s - loss: 0.2839 - accuracy: 0.8690 - ETA: 44s - loss: 0.2798 - accuracy: 0.874 - ETA: 36s - loss: 0.2858 - accuracy: 0.875 - ETA: 29s - loss: 0.2865 - accuracy: 0.875 - ETA: 25s - loss: 0.2872 - accuracy: 0.873 - ETA: 21s - loss: 0.2855 - accuracy: 0.875 - ETA: 18s - loss: 0.2922 - accuracy: 0.875 - ETA: 15s - loss: 0.2863 - accuracy: 0.878 - ETA: 13s - loss: 0.2867 - accuracy: 0.878 - ETA: 11s - loss: 0.2841 - accuracy: 0.879 - ETA: 9s - loss: 0.2859 - accuracy: 0.878 - ETA: 8s - loss: 0.2885 - accuracy: 0.87 - ETA: 7s - loss: 0.2848 - accuracy: 0.87 - ETA: 6s - loss: 0.2863 - accuracy: 0.87 - ETA: 5s - loss: 0.2862 - accuracy: 0.87 - ETA: 4s - loss: 0.2835 - accuracy: 0.87 - ETA: 3s - loss: 0.2826 - accuracy: 0.87 - ETA: 2s - loss: 0.2824 - accuracy: 0.87 - ETA: 2s - loss: 0.2823 - accuracy: 0.87 - ETA: 1s - loss: 0.2831 - accuracy: 0.88 - ETA: 1s - loss: 0.2827 - accuracy: 0.88 - ETA: 0s - loss: 0.2827 - accuracy: 0.88 - 26s 37ms/step - loss: 0.2943 - accuracy: 0.8712 - val_loss: 0.3755 - val_accuracy: 0.8226\n",
      "Epoch 3/3\n",
      "697/697 [==============================] ETA: 14:32 - loss: 0.2186 - accuracy: 0.906 - ETA: 3:30 - loss: 0.1909 - accuracy: 0.937 - ETA: 1:56 - loss: 0.1832 - accuracy: 0.92 - ETA: 1:18 - loss: 0.1664 - accuracy: 0.93 - ETA: 57s - loss: 0.1744 - accuracy: 0.9327 - ETA: 45s - loss: 0.1853 - accuracy: 0.926 - ETA: 36s - loss: 0.1753 - accuracy: 0.930 - ETA: 29s - loss: 0.1743 - accuracy: 0.931 - ETA: 25s - loss: 0.1801 - accuracy: 0.929 - ETA: 21s - loss: 0.1757 - accuracy: 0.930 - ETA: 18s - loss: 0.1772 - accuracy: 0.928 - ETA: 15s - loss: 0.1810 - accuracy: 0.925 - ETA: 13s - loss: 0.1836 - accuracy: 0.924 - ETA: 11s - loss: 0.1877 - accuracy: 0.921 - ETA: 9s - loss: 0.1864 - accuracy: 0.923 - ETA: 8s - loss: 0.1886 - accuracy: 0.92 - ETA: 7s - loss: 0.1886 - accuracy: 0.92 - ETA: 6s - loss: 0.1925 - accuracy: 0.92 - ETA: 5s - loss: 0.1912 - accuracy: 0.91 - ETA: 4s - loss: 0.1914 - accuracy: 0.91 - ETA: 3s - loss: 0.1909 - accuracy: 0.91 - ETA: 2s - loss: 0.1912 - accuracy: 0.91 - ETA: 2s - loss: 0.1947 - accuracy: 0.91 - ETA: 1s - loss: 0.1935 - accuracy: 0.91 - ETA: 1s - loss: 0.1942 - accuracy: 0.91 - ETA: 0s - loss: 0.1945 - accuracy: 0.91 - 26s 38ms/step - loss: 0.2234 - accuracy: 0.9018 - val_loss: 0.3975 - val_accuracy: 0.8322\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1bd70b0e160>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, epochs=3, validation_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 64)          1099456   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 128)               66048     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 1,178,115\n",
      "Trainable params: 1,178,115\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================].4966 - accuracy: 0.76 - 1s 749ms/step - loss: 0.4594 - accuracy: 0.789 - 2s 506ms/step - loss: 0.4356 - accuracy: 0.812 - 2s 384ms/step - loss: 0.4718 - accuracy: 0.816 - 2s 312ms/step - loss: 0.4490 - accuracy: 0.821 - 2s 264ms/step - loss: 0.4558 - accuracy: 0.817 - 2s 229ms/step - loss: 0.4413 - accuracy: 0.814 - 2s 203ms/step - loss: 0.4283 - accuracy: 0.820 - 2s 183ms/step - loss: 0.4185 - accuracy: 0.822 - 2s 166ms/step - loss: 0.4205 - accuracy: 0.821 - 2s 153ms/step - loss: 0.4176 - accuracy: 0.821 - 2s 142ms/step - loss: 0.4121 - accuracy: 0.822 - 2s 133ms/step - loss: 0.4131 - accuracy: 0.824 - 2s 125ms/step - loss: 0.4132 - accuracy: 0.824 - 2s 118ms/step - loss: 0.4295 - accuracy: 0.817 - 2s 112ms/step - loss: 0.4393 - accuracy: 0.816 - 2s 106ms/step - loss: 0.4250 - accuracy: 0.822 - 2s 102ms/step - loss: 0.4203 - accuracy: 0.822 - 2s 97ms/step - loss: 0.4130 - accuracy: 0.825 - 2s 94ms/step - loss: 0.4328 - accuracy: 0.82 - 2s 90ms/step - loss: 0.4271 - accuracy: 0.82 - 2s 87ms/step - loss: 0.4255 - accuracy: 0.82 - 2s 84ms/step - loss: 0.4219 - accuracy: 0.82 - 2s 81ms/step - loss: 0.4194 - accuracy: 0.82 - 2s 79ms/step - loss: 0.4198 - accuracy: 0.82 - 2s 77ms/step - loss: 0.4128 - accuracy: 0.82 - 2s 75ms/step - loss: 0.4124 - accuracy: 0.82 - 2s 73ms/step - loss: 0.4137 - accuracy: 0.82 - 2s 71ms/step - loss: 0.4101 - accuracy: 0.82 - 2s 69ms/step - loss: 0.4093 - accuracy: 0.82 - 2s 68ms/step - loss: 0.4181 - accuracy: 0.82 - 2s 66ms/step - loss: 0.4206 - accuracy: 0.82 - 2s 65ms/step - loss: 0.4167 - accuracy: 0.82 - 2s 63ms/step - loss: 0.4155 - accuracy: 0.82 - 2s 62ms/step - loss: 0.4148 - accuracy: 0.82 - 2s 61ms/step - loss: 0.4106 - accuracy: 0.82 - 2s 60ms/step - loss: 0.4092 - accuracy: 0.82 - 2s 59ms/step - loss: 0.4070 - accuracy: 0.82 - 2s 58ms/step - loss: 0.4051 - accuracy: 0.82 - 2s 57ms/step - loss: 0.4061 - accuracy: 0.82 - 2s 56ms/step - loss: 0.4022 - accuracy: 0.82 - 2s 55ms/step - loss: 0.4015 - accuracy: 0.82 - 2s 55ms/step - loss: 0.3996 - accuracy: 0.82 - 2s 54ms/step - loss: 0.3972 - accuracy: 0.83 - 2s 53ms/step - loss: 0.3954 - accuracy: 0.83 - 2s 52ms/step - loss: 0.3924 - accuracy: 0.83 - 2s 52ms/step - loss: 0.3888 - accuracy: 0.83 - 2s 51ms/step - loss: 0.3904 - accuracy: 0.83 - 2s 50ms/step - loss: 0.3912 - accuracy: 0.83 - 2s 50ms/step - loss: 0.3890 - accuracy: 0.83 - 3s 49ms/step - loss: 0.3941 - accuracy: 0.83 - 3s 49ms/step - loss: 0.3952 - accuracy: 0.83 - 3s 48ms/step - loss: 0.3943 - accuracy: 0.83 - 3s 48ms/step - loss: 0.3937 - accuracy: 0.83 - 3s 47ms/step - loss: 0.3949 - accuracy: 0.83 - 3s 47ms/step - loss: 0.3963 - accuracy: 0.83 - 3s 46ms/step - loss: 0.3991 - accuracy: 0.83 - 3s 46ms/step - loss: 0.3994 - accuracy: 0.83 - 3s 45ms/step - loss: 0.4013 - accuracy: 0.83 - 3s 45ms/step - loss: 0.3985 - accuracy: 0.83 - 3s 45ms/step - loss: 0.3966 - accuracy: 0.83 - 3s 44ms/step - loss: 0.3980 - accuracy: 0.83 - 3s 44ms/step - loss: 0.3973 - accuracy: 0.83 - 3s 43ms/step - loss: 0.3966 - accuracy: 0.82 - 3s 43ms/step - loss: 0.4016 - accuracy: 0.82 - 3s 43ms/step - loss: 0.3999 - accuracy: 0.82 - 3s 42ms/step - loss: 0.3987 - accuracy: 0.82 - 3s 42ms/step - loss: 0.3988 - accuracy: 0.83 - 3s 42ms/step - loss: 0.3993 - accuracy: 0.83 - 3s 41ms/step - loss: 0.3992 - accuracy: 0.82 - 3s 41ms/step - loss: 0.3995 - accuracy: 0.82 - 3s 41ms/step - loss: 0.3983 - accuracy: 0.82 - 3s 41ms/step - loss: 0.3973 - accuracy: 0.83 - 3s 40ms/step - loss: 0.3964 - accuracy: 0.83 - 3s 40ms/step - loss: 0.3986 - accuracy: 0.83 - 3s 40ms/step - loss: 0.3970 - accuracy: 0.83 - 3s 40ms/step - loss: 0.3966 - accuracy: 0.83 - 3s 39ms/step - loss: 0.3949 - accuracy: 0.83 - 3s 39ms/step - loss: 0.3975 - accuracy: 0.83 - 3s 40ms/step - loss: 0.3975 - accuracy: 0.8322\n"
     ]
    }
   ],
   "source": [
    "eval_loss, eval_acc = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 0.398, Eval accuracy: 0.832\n"
     ]
    }
   ],
   "source": [
    "print('Eval loss: {:.3f}, Eval accuracy: {:.3f}'.format(eval_loss, eval_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
