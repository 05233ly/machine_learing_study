{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0],True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
     ]
    }
   ],
   "source": [
    "# tfds.disable_progress_bar()\n",
    "(train_data, test_data), info = tfds.load(\n",
    "    'imdb_reviews/subwords8k', \n",
    "    split = (tfds.Split.TRAIN, tfds.Split.TEST), \n",
    "    with_info=True, as_supervised=True)\n",
    "encoder = info.features['text'].encoder\n",
    "padded_shapes = ([None],())\n",
    "train_batches = train_data.shuffle(10000).padded_batch(D, padded_shapes = padded_shapes)\n",
    "test_batches = test_data.shuffle(10000).padded_batch(100, padded_shapes = padded_shapes)\n",
    "\n",
    "embedding_dim=16\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Embedding(encoder.vocab_size, embedding_dim,mask_zero=True),\n",
    "    layers.Bidirectional(tf.keras.laQyers.LSTM(32)),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# TF_FORCE_GPU_ALLOW_GROWTH=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "    168/Unknown - 8s 8s/step - loss: 0.6932 - accuracy: 0.53 - 8s 4s/step - loss: 0.6927 - accuracy: 0.55 - 8s 3s/step - loss: 0.6924 - accuracy: 0.56 - 8s 2s/step - loss: 0.6923 - accuracy: 0.55 - 8s 2s/step - loss: 0.6922 - accuracy: 0.54 - 8s 1s/step - loss: 0.6922 - accuracy: 0.54 - 9s 1s/step - loss: 0.6926 - accuracy: 0.53 - 9s 1s/step - loss: 0.6927 - accuracy: 0.52 - 9s 978ms/step - loss: 0.6936 - accuracy: 0.513 - 9s 890ms/step - loss: 0.6935 - accuracy: 0.511 - 9s 819ms/step - loss: 0.6933 - accuracy: 0.513 - 9s 760ms/step - loss: 0.6932 - accuracy: 0.513 - 9s 710ms/step - loss: 0.6933 - accuracy: 0.511 - 9s 668ms/step - loss: 0.6933 - accuracy: 0.510 - 9s 630ms/step - loss: 0.6933 - accuracy: 0.509 - 10s 597ms/step - loss: 0.6933 - accuracy: 0.50 - 10s 568ms/step - loss: 0.6932 - accuracy: 0.50 - 10s 543ms/step - loss: 0.6930 - accuracy: 0.51 - 10s 519ms/step - loss: 0.6930 - accuracy: 0.50 - 10s 509ms/step - loss: 0.6930 - accuracy: 0.51 - 10s 489ms/step - loss: 0.6927 - accuracy: 0.51 - 10s 471ms/step - loss: 0.6926 - accuracy: 0.51 - 10s 456ms/step - loss: 0.6925 - accuracy: 0.51 - 11s 442ms/step - loss: 0.6926 - accuracy: 0.51 - 11s 429ms/step - loss: 0.6927 - accuracy: 0.51 - 11s 416ms/step - loss: 0.6926 - accuracy: 0.51 - 11s 404ms/step - loss: 0.6926 - accuracy: 0.51 - 11s 394ms/step - loss: 0.6926 - accuracy: 0.51 - 11s 384ms/step - loss: 0.6926 - accuracy: 0.51 - 11s 374ms/step - loss: 0.6926 - accuracy: 0.51 - 11s 366ms/step - loss: 0.6926 - accuracy: 0.51 - 11s 357ms/step - loss: 0.6926 - accuracy: 0.51 - 12s 349ms/step - loss: 0.6926 - accuracy: 0.50 - 12s 343ms/step - loss: 0.6925 - accuracy: 0.51 - 12s 336ms/step - loss: 0.6925 - accuracy: 0.51 - 12s 331ms/step - loss: 0.6924 - accuracy: 0.51 - 12s 325ms/step - loss: 0.6924 - accuracy: 0.51 - 12s 319ms/step - loss: 0.6924 - accuracy: 0.51 - 12s 314ms/step - loss: 0.6924 - accuracy: 0.51 - 12s 309ms/step - loss: 0.6924 - accuracy: 0.51 - 12s 303ms/step - loss: 0.6923 - accuracy: 0.51 - 13s 299ms/step - loss: 0.6922 - accuracy: 0.51 - 13s 294ms/step - loss: 0.6921 - accuracy: 0.52 - 13s 289ms/step - loss: 0.6921 - accuracy: 0.52 - 13s 285ms/step - loss: 0.6921 - accuracy: 0.52 - 13s 281ms/step - loss: 0.6921 - accuracy: 0.52 - 13s 277ms/step - loss: 0.6920 - accuracy: 0.52 - 13s 273ms/step - loss: 0.6919 - accuracy: 0.52 - 13s 270ms/step - loss: 0.6919 - accuracy: 0.52 - 13s 268ms/step - loss: 0.6918 - accuracy: 0.53 - 13s 265ms/step - loss: 0.6917 - accuracy: 0.53 - 14s 262ms/step - loss: 0.6916 - accuracy: 0.53 - 14s 258ms/step - loss: 0.6914 - accuracy: 0.53 - 14s 255ms/step - loss: 0.6914 - accuracy: 0.53 - 14s 253ms/step - loss: 0.6913 - accuracy: 0.53 - 14s 250ms/step - loss: 0.6911 - accuracy: 0.53 - 14s 247ms/step - loss: 0.6910 - accuracy: 0.53 - 14s 245ms/step - loss: 0.6908 - accuracy: 0.53 - 14s 242ms/step - loss: 0.6906 - accuracy: 0.53 - 14s 239ms/step - loss: 0.6903 - accuracy: 0.53 - 14s 237ms/step - loss: 0.6895 - accuracy: 0.54 - 15s 235ms/step - loss: 0.6881 - accuracy: 0.54 - 15s 233ms/step - loss: 0.6872 - accuracy: 0.54 - 15s 231ms/step - loss: 0.6857 - accuracy: 0.55 - 15s 229ms/step - loss: 0.6853 - accuracy: 0.55 - 15s 227ms/step - loss: 0.6851 - accuracy: 0.55 - 15s 225ms/step - loss: 0.6842 - accuracy: 0.55 - 15s 224ms/step - loss: 0.6840 - accuracy: 0.55 - 15s 222ms/step - loss: 0.6831 - accuracy: 0.55 - 15s 220ms/step - loss: 0.6818 - accuracy: 0.55 - 16s 219ms/step - loss: 0.6805 - accuracy: 0.55 - 16s 217ms/step - loss: 0.6820 - accuracy: 0.55 - 16s 216ms/step - loss: 0.6817 - accuracy: 0.56 - 16s 214ms/step - loss: 0.6807 - accuracy: 0.56 - 16s 213ms/step - loss: 0.6808 - accuracy: 0.56 - 16s 211ms/step - loss: 0.6802 - accuracy: 0.56 - 16s 210ms/step - loss: 0.6789 - accuracy: 0.56 - 16s 209ms/step - loss: 0.6790 - accuracy: 0.56 - 16s 208ms/step - loss: 0.6787 - accuracy: 0.56 - 16s 206ms/step - loss: 0.6772 - accuracy: 0.56 - 17s 205ms/step - loss: 0.6761 - accuracy: 0.56 - 17s 204ms/step - loss: 0.6747 - accuracy: 0.56 - 17s 202ms/step - loss: 0.6731 - accuracy: 0.57 - 17s 201ms/step - loss: 0.6722 - accuracy: 0.57 - 17s 200ms/step - loss: 0.6714 - accuracy: 0.57 - 17s 199ms/step - loss: 0.6709 - accuracy: 0.57 - 17s 198ms/step - loss: 0.6699 - accuracy: 0.57 - 17s 197ms/step - loss: 0.6687 - accuracy: 0.57 - 17s 196ms/step - loss: 0.6680 - accuracy: 0.57 - 18s 195ms/step - loss: 0.6670 - accuracy: 0.57 - 18s 195ms/step - loss: 0.6665 - accuracy: 0.58 - 18s 194ms/step - loss: 0.6656 - accuracy: 0.58 - 18s 193ms/step - loss: 0.6636 - accuracy: 0.58 - 18s 192ms/step - loss: 0.6622 - accuracy: 0.58 - 18s 191ms/step - loss: 0.6616 - accuracy: 0.58 - 18s 190ms/step - loss: 0.6612 - accuracy: 0.58 - 18s 189ms/step - loss: 0.6604 - accuracy: 0.58 - 18s 189ms/step - loss: 0.6595 - accuracy: 0.59 - 19s 188ms/step - loss: 0.6585 - accuracy: 0.59 - 19s 187ms/step - loss: 0.6583 - accuracy: 0.59 - 19s 186ms/step - loss: 0.6573 - accuracy: 0.59 - 19s 185ms/step - loss: 0.6570 - accuracy: 0.59 - 19s 185ms/step - loss: 0.6572 - accuracy: 0.59 - 19s 184ms/step - loss: 0.6569 - accuracy: 0.59 - 19s 183ms/step - loss: 0.6565 - accuracy: 0.59 - 19s 183ms/step - loss: 0.6559 - accuracy: 0.59 - 19s 182ms/step - loss: 0.6557 - accuracy: 0.59 - 20s 181ms/step - loss: 0.6554 - accuracy: 0.59 - 20s 180ms/step - loss: 0.6551 - accuracy: 0.59 - 20s 180ms/step - loss: 0.6547 - accuracy: 0.60 - 20s 179ms/step - loss: 0.6543 - accuracy: 0.60 - 20s 179ms/step - loss: 0.6541 - accuracy: 0.60 - 20s 178ms/step - loss: 0.6540 - accuracy: 0.60 - 20s 178ms/step - loss: 0.6539 - accuracy: 0.60 - 20s 177ms/step - loss: 0.6536 - accuracy: 0.60 - 20s 176ms/step - loss: 0.6534 - accuracy: 0.60 - 21s 176ms/step - loss: 0.6531 - accuracy: 0.60 - 21s 175ms/step - loss: 0.6528 - accuracy: 0.61 - 21s 174ms/step - loss: 0.6522 - accuracy: 0.61 - 21s 174ms/step - loss: 0.6517 - accuracy: 0.61 - 21s 174ms/step - loss: 0.6513 - accuracy: 0.61 - 21s 173ms/step - loss: 0.6508 - accuracy: 0.61 - 21s 173ms/step - loss: 0.6504 - accuracy: 0.61 - 21s 172ms/step - loss: 0.6498 - accuracy: 0.61 - 21s 172ms/step - loss: 0.6490 - accuracy: 0.62 - 22s 171ms/step - loss: 0.6481 - accuracy: 0.62 - 22s 170ms/step - loss: 0.6473 - accuracy: 0.62 - 22s 170ms/step - loss: 0.6463 - accuracy: 0.62 - 22s 169ms/step - loss: 0.6451 - accuracy: 0.62 - 22s 169ms/step - loss: 0.6440 - accuracy: 0.62 - 22s 168ms/step - loss: 0.6431 - accuracy: 0.62 - 22s 168ms/step - loss: 0.6420 - accuracy: 0.62 - 22s 167ms/step - loss: 0.6413 - accuracy: 0.62 - 22s 167ms/step - loss: 0.6408 - accuracy: 0.62 - 22s 166ms/step - loss: 0.6401 - accuracy: 0.62 - 23s 166ms/step - loss: 0.6394 - accuracy: 0.62 - 23s 165ms/step - loss: 0.6389 - accuracy: 0.62 - 23s 165ms/step - loss: 0.6386 - accuracy: 0.62 - 23s 165ms/step - loss: 0.6381 - accuracy: 0.63 - 23s 164ms/step - loss: 0.6375 - accuracy: 0.63 - 23s 164ms/step - loss: 0.6366 - accuracy: 0.63 - 23s 163ms/step - loss: 0.6356 - accuracy: 0.63 - 23s 163ms/step - loss: 0.6345 - accuracy: 0.63 - 23s 162ms/step - loss: 0.6338 - accuracy: 0.63 - 23s 162ms/step - loss: 0.6338 - accuracy: 0.63 - 24s 162ms/step - loss: 0.6339 - accuracy: 0.63 - 24s 161ms/step - loss: 0.6337 - accuracy: 0.63 - 24s 161ms/step - loss: 0.6339 - accuracy: 0.63 - 24s 161ms/step - loss: 0.6328 - accuracy: 0.63 - 24s 161ms/step - loss: 0.6323 - accuracy: 0.63 - 24s 160ms/step - loss: 0.6312 - accuracy: 0.64 - 24s 160ms/step - loss: 0.6298 - accuracy: 0.64 - 24s 160ms/step - loss: 0.6294 - accuracy: 0.64 - 25s 159ms/step - loss: 0.6287 - accuracy: 0.64 - 25s 159ms/step - loss: 0.6279 - accuracy: 0.64 - 25s 159ms/step - loss: 0.6268 - accuracy: 0.64 - 25s 159ms/step - loss: 0.6262 - accuracy: 0.64 - 25s 159ms/step - loss: 0.6256 - accuracy: 0.64 - 25s 158ms/step - loss: 0.6252 - accuracy: 0.64 - 25s 158ms/step - loss: 0.6247 - accuracy: 0.64 - 25s 158ms/step - loss: 0.6240 - accuracy: 0.64 - 26s 158ms/step - loss: 0.6235 - accuracy: 0.65 - 26s 157ms/step - loss: 0.6231 - accuracy: 0.65 - 26s 157ms/step - loss: 0.6227 - accuracy: 0.65 - 26s 157ms/step - loss: 0.6223 - accuracy: 0.65 - 26s 156ms/step - loss: 0.6219 - accuracy: 0.65 - 26s 156ms/step - loss: 0.6212 - accuracy: 0.65 - 26s 156ms/step - loss: 0.6208 - accuracy: 0.65250/250 [==============================] 0.6202 - accuracy: 0.65 - 26s 156ms/step - loss: 0.6193 - accuracy: 0.65 - 27s 155ms/step - loss: 0.6189 - accuracy: 0.65 - 27s 155ms/step - loss: 0.6186 - accuracy: 0.65 - 27s 155ms/step - loss: 0.6178 - accuracy: 0.65 - 27s 155ms/step - loss: 0.6171 - accuracy: 0.65 - 27s 154ms/step - loss: 0.6165 - accuracy: 0.65 - 27s 154ms/step - loss: 0.6161 - accuracy: 0.66 - 27s 154ms/step - loss: 0.6152 - accuracy: 0.66 - 27s 153ms/step - loss: 0.6143 - accuracy: 0.66 - 27s 153ms/step - loss: 0.6137 - accuracy: 0.66 - 28s 153ms/step - loss: 0.6126 - accuracy: 0.66 - 28s 153ms/step - loss: 0.6118 - accuracy: 0.66 - 28s 152ms/step - loss: 0.6109 - accuracy: 0.66 - 28s 152ms/step - loss: 0.6103 - accuracy: 0.66 - 28s 152ms/step - loss: 0.6096 - accuracy: 0.66 - 28s 152ms/step - loss: 0.6087 - accuracy: 0.66 - 28s 151ms/step - loss: 0.6079 - accuracy: 0.66 - 28s 151ms/step - loss: 0.6069 - accuracy: 0.66 - 28s 151ms/step - loss: 0.6061 - accuracy: 0.67 - 28s 151ms/step - loss: 0.6054 - accuracy: 0.67 - 29s 150ms/step - loss: 0.6047 - accuracy: 0.67 - 29s 150ms/step - loss: 0.6037 - accuracy: 0.67 - 29s 150ms/step - loss: 0.6028 - accuracy: 0.67 - 29s 150ms/step - loss: 0.6020 - accuracy: 0.67 - 29s 150ms/step - loss: 0.6016 - accuracy: 0.67 - 29s 149ms/step - loss: 0.6006 - accuracy: 0.67 - 29s 149ms/step - loss: 0.6000 - accuracy: 0.67 - 29s 149ms/step - loss: 0.5988 - accuracy: 0.67 - 29s 149ms/step - loss: 0.5992 - accuracy: 0.67 - 30s 148ms/step - loss: 0.5983 - accuracy: 0.67 - 30s 148ms/step - loss: 0.5977 - accuracy: 0.67 - 30s 148ms/step - loss: 0.5970 - accuracy: 0.67 - 30s 147ms/step - loss: 0.5962 - accuracy: 0.67 - 30s 147ms/step - loss: 0.5952 - accuracy: 0.68 - 30s 147ms/step - loss: 0.5942 - accuracy: 0.68 - 30s 147ms/step - loss: 0.5933 - accuracy: 0.68 - 30s 147ms/step - loss: 0.5925 - accuracy: 0.68 - 30s 147ms/step - loss: 0.5917 - accuracy: 0.68 - 30s 146ms/step - loss: 0.5910 - accuracy: 0.68 - 31s 146ms/step - loss: 0.5903 - accuracy: 0.68 - 31s 146ms/step - loss: 0.5895 - accuracy: 0.68 - 31s 146ms/step - loss: 0.5890 - accuracy: 0.68 - 31s 145ms/step - loss: 0.5885 - accuracy: 0.68 - 31s 145ms/step - loss: 0.5879 - accuracy: 0.68 - 31s 145ms/step - loss: 0.5869 - accuracy: 0.68 - 31s 145ms/step - loss: 0.5861 - accuracy: 0.68 - 31s 145ms/step - loss: 0.5854 - accuracy: 0.68 - 31s 145ms/step - loss: 0.5852 - accuracy: 0.68 - 31s 144ms/step - loss: 0.5849 - accuracy: 0.68 - 32s 144ms/step - loss: 0.5843 - accuracy: 0.68 - 32s 144ms/step - loss: 0.5837 - accuracy: 0.68 - 32s 144ms/step - loss: 0.5839 - accuracy: 0.68 - 32s 144ms/step - loss: 0.5836 - accuracy: 0.68 - 32s 144ms/step - loss: 0.5830 - accuracy: 0.69 - 32s 143ms/step - loss: 0.5823 - accuracy: 0.69 - 32s 143ms/step - loss: 0.5815 - accuracy: 0.69 - 32s 143ms/step - loss: 0.5809 - accuracy: 0.69 - 32s 143ms/step - loss: 0.5803 - accuracy: 0.69 - 33s 143ms/step - loss: 0.5798 - accuracy: 0.69 - 33s 143ms/step - loss: 0.5792 - accuracy: 0.69 - 33s 142ms/step - loss: 0.5788 - accuracy: 0.69 - 33s 142ms/step - loss: 0.5777 - accuracy: 0.69 - 33s 142ms/step - loss: 0.5779 - accuracy: 0.69 - 33s 142ms/step - loss: 0.5771 - accuracy: 0.69 - 33s 142ms/step - loss: 0.5761 - accuracy: 0.69 - 33s 142ms/step - loss: 0.5756 - accuracy: 0.69 - 33s 142ms/step - loss: 0.5747 - accuracy: 0.69 - 33s 141ms/step - loss: 0.5740 - accuracy: 0.69 - 34s 141ms/step - loss: 0.5733 - accuracy: 0.69 - 34s 141ms/step - loss: 0.5725 - accuracy: 0.69 - 34s 141ms/step - loss: 0.5715 - accuracy: 0.70 - 34s 141ms/step - loss: 0.5711 - accuracy: 0.70 - 34s 141ms/step - loss: 0.5703 - accuracy: 0.70 - 34s 141ms/step - loss: 0.5696 - accuracy: 0.70 - 34s 141ms/step - loss: 0.5690 - accuracy: 0.70 - 34s 140ms/step - loss: 0.5683 - accuracy: 0.70 - 35s 140ms/step - loss: 0.5675 - accuracy: 0.70 - 35s 140ms/step - loss: 0.5667 - accuracy: 0.70 - 35s 140ms/step - loss: 0.5659 - accuracy: 0.70 - 35s 140ms/step - loss: 0.5652 - accuracy: 0.70 - 35s 140ms/step - loss: 0.5646 - accuracy: 0.70 - 39s 154ms/step - loss: 0.5646 - accuracy: 0.7052 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/250 [=====================>........] - ETA: 2:45 - loss: 0.3774 - accuracy: 0.83 - ETA: 1:36 - loss: 0.3905 - accuracy: 0.82 - ETA: 1:15 - loss: 0.3880 - accuracy: 0.82 - ETA: 1:03 - loss: 0.3709 - accuracy: 0.84 - ETA: 55s - loss: 0.3636 - accuracy: 0.8500 - ETA: 49s - loss: 0.3592 - accuracy: 0.848 - ETA: 45s - loss: 0.3578 - accuracy: 0.848 - ETA: 42s - loss: 0.3551 - accuracy: 0.850 - ETA: 40s - loss: 0.3583 - accuracy: 0.844 - ETA: 39s - loss: 0.3525 - accuracy: 0.848 - ETA: 37s - loss: 0.3448 - accuracy: 0.855 - ETA: 36s - loss: 0.3424 - accuracy: 0.856 - ETA: 35s - loss: 0.3396 - accuracy: 0.859 - ETA: 34s - loss: 0.3392 - accuracy: 0.860 - ETA: 33s - loss: 0.3394 - accuracy: 0.862 - ETA: 33s - loss: 0.3419 - accuracy: 0.860 - ETA: 32s - loss: 0.3394 - accuracy: 0.862 - ETA: 32s - loss: 0.3401 - accuracy: 0.861 - ETA: 31s - loss: 0.3372 - accuracy: 0.863 - ETA: 31s - loss: 0.3382 - accuracy: 0.862 - ETA: 31s - loss: 0.3400 - accuracy: 0.861 - ETA: 30s - loss: 0.3366 - accuracy: 0.862 - ETA: 30s - loss: 0.3384 - accuracy: 0.863 - ETA: 29s - loss: 0.3384 - accuracy: 0.862 - ETA: 29s - loss: 0.3403 - accuracy: 0.860 - ETA: 28s - loss: 0.3406 - accuracy: 0.860 - ETA: 28s - loss: 0.3442 - accuracy: 0.860 - ETA: 28s - loss: 0.3500 - accuracy: 0.858 - ETA: 27s - loss: 0.3554 - accuracy: 0.854 - ETA: 27s - loss: 0.3558 - accuracy: 0.855 - ETA: 27s - loss: 0.3558 - accuracy: 0.853 - ETA: 27s - loss: 0.3603 - accuracy: 0.850 - ETA: 26s - loss: 0.3588 - accuracy: 0.850 - ETA: 26s - loss: 0.3581 - accuracy: 0.850 - ETA: 26s - loss: 0.3560 - accuracy: 0.852 - ETA: 26s - loss: 0.3572 - accuracy: 0.851 - ETA: 26s - loss: 0.3575 - accuracy: 0.852 - ETA: 25s - loss: 0.3595 - accuracy: 0.851 - ETA: 25s - loss: 0.3597 - accuracy: 0.851 - ETA: 25s - loss: 0.3606 - accuracy: 0.852 - ETA: 25s - loss: 0.3591 - accuracy: 0.853 - ETA: 25s - loss: 0.3608 - accuracy: 0.851 - ETA: 25s - loss: 0.3622 - accuracy: 0.850 - ETA: 25s - loss: 0.3621 - accuracy: 0.851 - ETA: 24s - loss: 0.3628 - accuracy: 0.850 - ETA: 24s - loss: 0.3631 - accuracy: 0.850 - ETA: 24s - loss: 0.3629 - accuracy: 0.849 - ETA: 24s - loss: 0.3611 - accuracy: 0.850 - ETA: 24s - loss: 0.3606 - accuracy: 0.850 - ETA: 24s - loss: 0.3598 - accuracy: 0.850 - ETA: 24s - loss: 0.3600 - accuracy: 0.850 - ETA: 23s - loss: 0.3598 - accuracy: 0.849 - ETA: 23s - loss: 0.3601 - accuracy: 0.849 - ETA: 23s - loss: 0.3591 - accuracy: 0.850 - ETA: 23s - loss: 0.3591 - accuracy: 0.850 - ETA: 23s - loss: 0.3584 - accuracy: 0.850 - ETA: 23s - loss: 0.3576 - accuracy: 0.850 - ETA: 22s - loss: 0.3562 - accuracy: 0.852 - ETA: 22s - loss: 0.3573 - accuracy: 0.852 - ETA: 22s - loss: 0.3573 - accuracy: 0.852 - ETA: 22s - loss: 0.3565 - accuracy: 0.853 - ETA: 22s - loss: 0.3553 - accuracy: 0.853 - ETA: 22s - loss: 0.3555 - accuracy: 0.852 - ETA: 22s - loss: 0.3563 - accuracy: 0.851 - ETA: 21s - loss: 0.3563 - accuracy: 0.851 - ETA: 21s - loss: 0.3568 - accuracy: 0.851 - ETA: 21s - loss: 0.3552 - accuracy: 0.852 - ETA: 21s - loss: 0.3548 - accuracy: 0.851 - ETA: 21s - loss: 0.3540 - accuracy: 0.852 - ETA: 21s - loss: 0.3534 - accuracy: 0.852 - ETA: 21s - loss: 0.3537 - accuracy: 0.851 - ETA: 20s - loss: 0.3536 - accuracy: 0.852 - ETA: 20s - loss: 0.3523 - accuracy: 0.852 - ETA: 20s - loss: 0.3522 - accuracy: 0.853 - ETA: 20s - loss: 0.3519 - accuracy: 0.853 - ETA: 20s - loss: 0.3520 - accuracy: 0.853 - ETA: 20s - loss: 0.3508 - accuracy: 0.853 - ETA: 20s - loss: 0.3506 - accuracy: 0.853 - ETA: 20s - loss: 0.3509 - accuracy: 0.852 - ETA: 19s - loss: 0.3509 - accuracy: 0.852 - ETA: 19s - loss: 0.3506 - accuracy: 0.853 - ETA: 19s - loss: 0.3513 - accuracy: 0.852 - ETA: 19s - loss: 0.3503 - accuracy: 0.852 - ETA: 19s - loss: 0.3494 - accuracy: 0.853 - ETA: 19s - loss: 0.3496 - accuracy: 0.853 - ETA: 19s - loss: 0.3499 - accuracy: 0.852 - ETA: 19s - loss: 0.3504 - accuracy: 0.852 - ETA: 18s - loss: 0.3506 - accuracy: 0.852 - ETA: 18s - loss: 0.3499 - accuracy: 0.852 - ETA: 18s - loss: 0.3496 - accuracy: 0.853 - ETA: 18s - loss: 0.3497 - accuracy: 0.853 - ETA: 18s - loss: 0.3501 - accuracy: 0.852 - ETA: 18s - loss: 0.3492 - accuracy: 0.853 - ETA: 18s - loss: 0.3499 - accuracy: 0.852 - ETA: 18s - loss: 0.3500 - accuracy: 0.852 - ETA: 17s - loss: 0.3498 - accuracy: 0.852 - ETA: 17s - loss: 0.3496 - accuracy: 0.852 - ETA: 17s - loss: 0.3494 - accuracy: 0.852 - ETA: 17s - loss: 0.3494 - accuracy: 0.852 - ETA: 17s - loss: 0.3496 - accuracy: 0.852 - ETA: 17s - loss: 0.3490 - accuracy: 0.852 - ETA: 17s - loss: 0.3483 - accuracy: 0.852 - ETA: 17s - loss: 0.3478 - accuracy: 0.853 - ETA: 16s - loss: 0.3477 - accuracy: 0.853 - ETA: 16s - loss: 0.3471 - accuracy: 0.853 - ETA: 16s - loss: 0.3466 - accuracy: 0.853 - ETA: 16s - loss: 0.3464 - accuracy: 0.854 - ETA: 16s - loss: 0.3453 - accuracy: 0.854 - ETA: 16s - loss: 0.3449 - accuracy: 0.855 - ETA: 16s - loss: 0.3450 - accuracy: 0.854 - ETA: 15s - loss: 0.3444 - accuracy: 0.855 - ETA: 15s - loss: 0.3432 - accuracy: 0.855 - ETA: 15s - loss: 0.3426 - accuracy: 0.856 - ETA: 15s - loss: 0.3418 - accuracy: 0.856 - ETA: 15s - loss: 0.3411 - accuracy: 0.857 - ETA: 15s - loss: 0.3404 - accuracy: 0.857 - ETA: 15s - loss: 0.3395 - accuracy: 0.858 - ETA: 15s - loss: 0.3388 - accuracy: 0.858 - ETA: 14s - loss: 0.3390 - accuracy: 0.858 - ETA: 14s - loss: 0.3387 - accuracy: 0.858 - ETA: 14s - loss: 0.3377 - accuracy: 0.858 - ETA: 14s - loss: 0.3374 - accuracy: 0.858 - ETA: 14s - loss: 0.3361 - accuracy: 0.859 - ETA: 14s - loss: 0.3361 - accuracy: 0.859 - ETA: 14s - loss: 0.3351 - accuracy: 0.860 - ETA: 14s - loss: 0.3349 - accuracy: 0.860 - ETA: 14s - loss: 0.3348 - accuracy: 0.860 - ETA: 14s - loss: 0.3351 - accuracy: 0.860 - ETA: 13s - loss: 0.3347 - accuracy: 0.860 - ETA: 13s - loss: 0.3339 - accuracy: 0.860 - ETA: 13s - loss: 0.3331 - accuracy: 0.861 - ETA: 13s - loss: 0.3323 - accuracy: 0.861 - ETA: 13s - loss: 0.3318 - accuracy: 0.862 - ETA: 13s - loss: 0.3307 - accuracy: 0.862 - ETA: 13s - loss: 0.3305 - accuracy: 0.862 - ETA: 13s - loss: 0.3299 - accuracy: 0.862 - ETA: 12s - loss: 0.3303 - accuracy: 0.862 - ETA: 12s - loss: 0.3301 - accuracy: 0.862 - ETA: 12s - loss: 0.3301 - accuracy: 0.862 - ETA: 12s - loss: 0.3292 - accuracy: 0.863 - ETA: 12s - loss: 0.3286 - accuracy: 0.863 - ETA: 12s - loss: 0.3279 - accuracy: 0.863 - ETA: 12s - loss: 0.3278 - accuracy: 0.863 - ETA: 12s - loss: 0.3280 - accuracy: 0.863 - ETA: 12s - loss: 0.3277 - accuracy: 0.864 - ETA: 11s - loss: 0.3273 - accuracy: 0.864 - ETA: 11s - loss: 0.3270 - accuracy: 0.864 - ETA: 11s - loss: 0.3266 - accuracy: 0.864 - ETA: 11s - loss: 0.3265 - accuracy: 0.865 - ETA: 11s - loss: 0.3267 - accuracy: 0.865 - ETA: 11s - loss: 0.3264 - accuracy: 0.865 - ETA: 11s - loss: 0.3269 - accuracy: 0.864 - ETA: 11s - loss: 0.3271 - accuracy: 0.864 - ETA: 10s - loss: 0.3280 - accuracy: 0.864 - ETA: 10s - loss: 0.3275 - accuracy: 0.864 - ETA: 10s - loss: 0.3278 - accuracy: 0.864 - ETA: 10s - loss: 0.3280 - accuracy: 0.864 - ETA: 10s - loss: 0.3280 - accuracy: 0.864 - ETA: 10s - loss: 0.3278 - accuracy: 0.864 - ETA: 10s - loss: 0.3274 - accuracy: 0.864 - ETA: 10s - loss: 0.3276 - accuracy: 0.864 - ETA: 10s - loss: 0.3278 - accuracy: 0.864 - ETA: 9s - loss: 0.3287 - accuracy: 0.864 - ETA: 9s - loss: 0.3279 - accuracy: 0.86 - ETA: 9s - loss: 0.3273 - accuracy: 0.86 - ETA: 9s - loss: 0.3266 - accuracy: 0.86 - ETA: 9s - loss: 0.3264 - accuracy: 0.86 - ETA: 9s - loss: 0.3258 - accuracy: 0.86 - ETA: 9s - loss: 0.3257 - accuracy: 0.86 - ETA: 9s - loss: 0.3252 - accuracy: 0.86 - ETA: 8s - loss: 0.3247 - accuracy: 0.86 - ETA: 8s - loss: 0.3246 - accuracy: 0.86 - ETA: 8s - loss: 0.3239 - accuracy: 0.86 - ETA: 8s - loss: 0.3234 - accuracy: 0.86 - ETA: 8s - loss: 0.3235 - accuracy: 0.86 - ETA: 8s - loss: 0.3238 - accuracy: 0.86 - ETA: 8s - loss: 0.3233 - accuracy: 0.86 - ETA: 8s - loss: 0.3230 - accuracy: 0.86 - ETA: 8s - loss: 0.3228 - accuracy: 0.86 - ETA: 7s - loss: 0.3228 - accuracy: 0.86 - ETA: 7s - loss: 0.3224 - accuracy: 0.86 - ETA: 7s - loss: 0.3226 - accuracy: 0.86 - ETA: 7s - loss: 0.3222 - accuracy: 0.86 - ETA: 7s - loss: 0.3218 - accuracy: 0.86 - ETA: 7s - loss: 0.3214 - accuracy: 0.86 - ETA: 7s - loss: 0.3213 - accuracy: 0.86 - ETA: 7s - loss: 0.3211 - accuracy: 0.8684"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207/250 [=======================>......] - ETA: 7s - loss: 0.3210 - accuracy: 0.86 - ETA: 6s - loss: 0.3208 - accuracy: 0.86 - ETA: 6s - loss: 0.3209 - accuracy: 0.86 - ETA: 6s - loss: 0.3207 - accuracy: 0.86 - ETA: 6s - loss: 0.3208 - accuracy: 0.86 - ETA: 6s - loss: 0.3204 - accuracy: 0.86 - ETA: 6s - loss: 0.3200 - accuracy: 0.86 - ETA: 6s - loss: 0.3198 - accuracy: 0.86 - ETA: 6s - loss: 0.3199 - accuracy: 0.86 - ETA: 6s - loss: 0.3197 - accuracy: 0.86 - ETA: 5s - loss: 0.3195 - accuracy: 0.86 - ETA: 5s - loss: 0.3191 - accuracy: 0.86 - ETA: 5s - loss: 0.3190 - accuracy: 0.86 - ETA: 5s - loss: 0.3190 - accuracy: 0.86 - ETA: 5s - loss: 0.3186 - accuracy: 0.86 - ETA: 5s - loss: 0.3180 - accuracy: 0.86 - ETA: 5s - loss: 0.3175 - accuracy: 0.86 - ETA: 5s - loss: 0.3169 - accuracy: 0.87 - ETA: 4s - loss: 0.3168 - accuracy: 0.87 - ETA: 4s - loss: 0.3168 - accuracy: 0.8701"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": " [_Derived_]RecvAsync is cancelled.\n\t [[{{node Adam/Adam/update/AssignSubVariableOp/_41}}]]\n\t [[Reshape_11/_38]] [Op:__inference_distributed_function_15947]\n\nFunction call stack:\ndistributed_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-b49add8ca8c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mtrain_batches\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     validation_data=test_batches, validation_steps=20)\n\u001b[0m",
      "\u001b[1;32mc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    122\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 86\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    485\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 487\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    488\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1821\u001b[0m     \u001b[1;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1823\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1825\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1141\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1143\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[1;32m-> 1224\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1225\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    512\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[1;32mc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mCancelledError\u001b[0m:  [_Derived_]RecvAsync is cancelled.\n\t [[{{node Adam/Adam/update/AssignSubVariableOp/_41}}]]\n\t [[Reshape_11/_38]] [Op:__inference_distributed_function_15947]\n\nFunction call stack:\ndistributed_function\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_batches,\n",
    "    epochs=10,\n",
    "    validation_data=test_batches, validation_steps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
