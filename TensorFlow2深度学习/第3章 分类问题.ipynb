{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数为MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  tensorflow as tf\n",
    "from    tensorflow.keras import datasets, layers, optimizers, Sequential, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置GPU使用方式\n",
    "# 获取GPU列表\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "    # 设置GPU为增长式占用\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True) \n",
    "    except RuntimeError as e:\n",
    "        # 打印异常\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets: (60000, 28, 28) (60000,) 0 255\n"
     ]
    }
   ],
   "source": [
    "(xs, ys),_ = datasets.mnist.load_data()\n",
    "print('datasets:', xs.shape, ys.shape, xs.min(), xs.max())\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "xs = tf.convert_to_tensor(xs, dtype=tf.float32) / 255.\n",
    "db = tf.data.Dataset.from_tensor_slices((xs,ys))\n",
    "db = db.batch(batch_size).repeat(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_51 (Dense)             multiple                  200960    \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             multiple                  32896     \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             multiple                  1290      \n",
      "=================================================================\n",
      "Total params: 235,146\n",
      "Trainable params: 235,146\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([layers.Dense(256, activation='relu'), \n",
    "                     layers.Dense(128, activation='relu'),\n",
    "                     layers.Dense(10)])\n",
    "model.build(input_shape=(4, 28*28))\n",
    "model.summary()\n",
    "\n",
    "optimizer = optimizers.SGD(lr=0.01)\n",
    "acc_meter = metrics.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 1.7534599304199219 acc: 0.03125\n",
      "200 loss: 0.44551974534988403 acc: 0.68671876\n",
      "400 loss: 0.4234689176082611 acc: 0.84515625\n",
      "600 loss: 0.33971071243286133 acc: 0.8634375\n",
      "800 loss: 0.2810252010822296 acc: 0.8896875\n",
      "1000 loss: 0.3286469876766205 acc: 0.8846875\n",
      "1200 loss: 0.31151083111763 acc: 0.9060938\n",
      "1400 loss: 0.2206282913684845 acc: 0.91625\n",
      "1600 loss: 0.21076782047748566 acc: 0.9096875\n",
      "1800 loss: 0.22187116742134094 acc: 0.9259375\n",
      "2000 loss: 0.20213583111763 acc: 0.93828124\n",
      "2200 loss: 0.1590643972158432 acc: 0.92515624\n",
      "2400 loss: 0.25978991389274597 acc: 0.92625\n",
      "2600 loss: 0.24126344919204712 acc: 0.9351562\n",
      "2800 loss: 0.14984960854053497 acc: 0.93421876\n",
      "3000 loss: 0.30067193508148193 acc: 0.9271875\n",
      "3200 loss: 0.20468740165233612 acc: 0.93859375\n",
      "3400 loss: 0.1560928225517273 acc: 0.93296874\n",
      "3600 loss: 0.13243088126182556 acc: 0.9365625\n",
      "3800 loss: 0.1819324791431427 acc: 0.9525\n",
      "4000 loss: 0.18269990384578705 acc: 0.94953126\n",
      "4200 loss: 0.1399674266576767 acc: 0.9375\n",
      "4400 loss: 0.1649312824010849 acc: 0.9465625\n",
      "4600 loss: 0.20032700896263123 acc: 0.9454687\n",
      "4800 loss: 0.18345633149147034 acc: 0.943125\n",
      "5000 loss: 0.14246784150600433 acc: 0.9478125\n",
      "5200 loss: 0.2235526144504547 acc: 0.9454687\n",
      "5400 loss: 0.2412944883108139 acc: 0.9454687\n",
      "5600 loss: 0.10880542546510696 acc: 0.95921874\n",
      "5800 loss: 0.18490877747535706 acc: 0.95640624\n",
      "6000 loss: 0.1275872141122818 acc: 0.94890624\n",
      "6200 loss: 0.18277980387210846 acc: 0.9478125\n",
      "6400 loss: 0.12914127111434937 acc: 0.95421875\n",
      "6600 loss: 0.1530056595802307 acc: 0.950625\n",
      "6800 loss: 0.10141438245773315 acc: 0.9529688\n",
      "7000 loss: 0.10216310620307922 acc: 0.9525\n",
      "7200 loss: 0.3091268539428711 acc: 0.94703126\n",
      "7400 loss: 0.15147751569747925 acc: 0.9575\n",
      "7600 loss: 0.19865193963050842 acc: 0.9640625\n",
      "7800 loss: 0.08410143107175827 acc: 0.9553125\n",
      "8000 loss: 0.16907313466072083 acc: 0.9546875\n",
      "8200 loss: 0.09750679135322571 acc: 0.960625\n",
      "8400 loss: 0.07776659727096558 acc: 0.95703125\n",
      "8600 loss: 0.11787543445825577 acc: 0.9532812\n",
      "8800 loss: 0.1570860892534256 acc: 0.9571875\n",
      "9000 loss: 0.14987309277057648 acc: 0.95375\n",
      "9200 loss: 0.09714799374341965 acc: 0.956875\n",
      "9400 loss: 0.09034693986177444 acc: 0.9689062\n",
      "9600 loss: 0.17636899650096893 acc: 0.9632813\n",
      "9800 loss: 0.06793585419654846 acc: 0.9584375\n",
      "10000 loss: 0.16377834975719452 acc: 0.95921874\n",
      "10200 loss: 0.11278069019317627 acc: 0.9610937\n",
      "10400 loss: 0.13720805943012238 acc: 0.95375\n",
      "10600 loss: 0.08051367849111557 acc: 0.963125\n",
      "10800 loss: 0.18812233209609985 acc: 0.9575\n",
      "11000 loss: 0.10402588546276093 acc: 0.9584375\n",
      "11200 loss: 0.12804050743579865 acc: 0.9659375\n",
      "11400 loss: 0.10410410165786743 acc: 0.96921873\n",
      "11600 loss: 0.16653434932231903 acc: 0.96203125\n",
      "11800 loss: 0.12008887529373169 acc: 0.9609375\n",
      "12000 loss: 0.09504532068967819 acc: 0.9639062\n",
      "12200 loss: 0.07147624343633652 acc: 0.96\n",
      "12400 loss: 0.12336316704750061 acc: 0.96375\n",
      "12600 loss: 0.19615055620670319 acc: 0.95921874\n",
      "12800 loss: 0.12266470491886139 acc: 0.96171874\n",
      "13000 loss: 0.10882750898599625 acc: 0.9628125\n",
      "13200 loss: 0.15959687530994415 acc: 0.9717187\n",
      "13400 loss: 0.10251251608133316 acc: 0.9690625\n",
      "13600 loss: 0.1181660071015358 acc: 0.96421874\n",
      "13800 loss: 0.09546701610088348 acc: 0.9653125\n",
      "14000 loss: 0.06484469771385193 acc: 0.9657813\n",
      "14200 loss: 0.18774059414863586 acc: 0.96296877\n",
      "14400 loss: 0.10592037439346313 acc: 0.96671873\n",
      "14600 loss: 0.15521758794784546 acc: 0.961875\n",
      "14800 loss: 0.09088777005672455 acc: 0.9634375\n",
      "15000 loss: 0.11377114057540894 acc: 0.97515625\n",
      "15200 loss: 0.12068203836679459 acc: 0.97203124\n",
      "15400 loss: 0.10918954014778137 acc: 0.9671875\n",
      "15600 loss: 0.09694670885801315 acc: 0.9665625\n",
      "15800 loss: 0.09465395659208298 acc: 0.968125\n",
      "16000 loss: 0.12570485472679138 acc: 0.96234375\n",
      "16200 loss: 0.11285234987735748 acc: 0.96984375\n",
      "16400 loss: 0.08436785638332367 acc: 0.968125\n",
      "16600 loss: 0.09136641770601273 acc: 0.9632813\n",
      "16800 loss: 0.11678474396467209 acc: 0.96984375\n",
      "17000 loss: 0.10424234718084335 acc: 0.976875\n",
      "17200 loss: 0.04335877671837807 acc: 0.96921873\n",
      "17400 loss: 0.11027549207210541 acc: 0.969375\n",
      "17600 loss: 0.11537255346775055 acc: 0.9685938\n",
      "17800 loss: 0.06374642252922058 acc: 0.9689062\n",
      "18000 loss: 0.10723678767681122 acc: 0.970625\n",
      "18200 loss: 0.10442941635847092 acc: 0.96921873\n",
      "18400 loss: 0.08142027258872986 acc: 0.9665625\n",
      "18600 loss: 0.06013837456703186 acc: 0.96671873\n",
      "18800 loss: 0.11392863839864731 acc: 0.9753125\n",
      "19000 loss: 0.09547477215528488 acc: 0.9764063\n",
      "19200 loss: 0.06502095609903336 acc: 0.96984375\n",
      "19400 loss: 0.08304837346076965 acc: 0.97015625\n",
      "19600 loss: 0.13476426899433136 acc: 0.9710938\n",
      "19800 loss: 0.09536436200141907 acc: 0.9690625\n",
      "20000 loss: 0.0686410665512085 acc: 0.9734375\n",
      "20200 loss: 0.15055282413959503 acc: 0.9675\n",
      "20400 loss: 0.1609131097793579 acc: 0.97\n",
      "20600 loss: 0.05000120401382446 acc: 0.9767187\n",
      "20800 loss: 0.10555807501077652 acc: 0.97578126\n",
      "21000 loss: 0.09015783667564392 acc: 0.9709375\n",
      "21200 loss: 0.0965590700507164 acc: 0.971875\n",
      "21400 loss: 0.06609680503606796 acc: 0.974375\n",
      "21600 loss: 0.06728120148181915 acc: 0.96984375\n",
      "21800 loss: 0.04278125613927841 acc: 0.97375\n",
      "22000 loss: 0.05367763713002205 acc: 0.9717187\n",
      "22200 loss: 0.23556645214557648 acc: 0.96765625\n",
      "22400 loss: 0.10814521461725235 acc: 0.9739063\n",
      "22600 loss: 0.124018095433712 acc: 0.9784375\n",
      "22800 loss: 0.04858824238181114 acc: 0.9740625\n",
      "23000 loss: 0.11819498240947723 acc: 0.9735938\n",
      "23200 loss: 0.04966713488101959 acc: 0.9753125\n",
      "23400 loss: 0.04809551686048508 acc: 0.9742187\n",
      "23600 loss: 0.07943110167980194 acc: 0.9739063\n",
      "23800 loss: 0.09924282878637314 acc: 0.97453123\n",
      "24000 loss: 0.10306904464960098 acc: 0.96984375\n",
      "24200 loss: 0.05387512594461441 acc: 0.9725\n",
      "24400 loss: 0.04506679251790047 acc: 0.98015624\n",
      "24600 loss: 0.11463125795125961 acc: 0.9767187\n",
      "24800 loss: 0.038825031369924545 acc: 0.9739063\n",
      "25000 loss: 0.11783921718597412 acc: 0.975625\n",
      "25200 loss: 0.07757831364870071 acc: 0.97625\n",
      "25400 loss: 0.10327021032571793 acc: 0.97125\n",
      "25600 loss: 0.050212398171424866 acc: 0.978125\n",
      "25800 loss: 0.1355067938566208 acc: 0.97203124\n",
      "26000 loss: 0.0662117600440979 acc: 0.9710938\n",
      "26200 loss: 0.08939102292060852 acc: 0.97796875\n",
      "26400 loss: 0.0648375153541565 acc: 0.9790625\n",
      "26600 loss: 0.13603365421295166 acc: 0.975\n",
      "26800 loss: 0.07968592643737793 acc: 0.97609377\n",
      "27000 loss: 0.053562138229608536 acc: 0.9775\n",
      "27200 loss: 0.04184536635875702 acc: 0.9742187\n",
      "27400 loss: 0.08838169276714325 acc: 0.97828126\n",
      "27600 loss: 0.13569338619709015 acc: 0.9725\n",
      "27800 loss: 0.08147194236516953 acc: 0.97375\n",
      "28000 loss: 0.07693774253129959 acc: 0.97609377\n",
      "28200 loss: 0.12175148725509644 acc: 0.9803125\n",
      "28400 loss: 0.07270597666501999 acc: 0.9792187\n",
      "28600 loss: 0.07849811762571335 acc: 0.976875\n",
      "28800 loss: 0.0840013399720192 acc: 0.97765625\n",
      "29000 loss: 0.042530786246061325 acc: 0.97609377\n",
      "29200 loss: 0.1621420681476593 acc: 0.97734374\n",
      "29400 loss: 0.07295803725719452 acc: 0.978125\n",
      "29600 loss: 0.10971292108297348 acc: 0.9735938\n",
      "29800 loss: 0.06704318523406982 acc: 0.975\n",
      "30000 loss: 0.07398013770580292 acc: 0.98234373\n",
      "30200 loss: 0.09079935401678085 acc: 0.9792187\n",
      "30400 loss: 0.08464648574590683 acc: 0.9767187\n",
      "30600 loss: 0.06277937442064285 acc: 0.9789063\n",
      "30800 loss: 0.07064033299684525 acc: 0.9790625\n",
      "31000 loss: 0.09904370456933975 acc: 0.9746875\n",
      "31200 loss: 0.08618798106908798 acc: 0.9792187\n",
      "31400 loss: 0.06336821615695953 acc: 0.97703123\n",
      "31600 loss: 0.06004118546843529 acc: 0.9739063\n",
      "31800 loss: 0.0917401909828186 acc: 0.9790625\n",
      "32000 loss: 0.06868816912174225 acc: 0.9820312\n",
      "32200 loss: 0.026171255856752396 acc: 0.97703123\n",
      "32400 loss: 0.07632339000701904 acc: 0.9789063\n",
      "32600 loss: 0.0830349400639534 acc: 0.9790625\n",
      "32800 loss: 0.05373635143041611 acc: 0.978125\n",
      "33000 loss: 0.06517620384693146 acc: 0.98046875\n",
      "33200 loss: 0.08622699975967407 acc: 0.97765625\n",
      "33400 loss: 0.05888858065009117 acc: 0.9759375\n",
      "33600 loss: 0.039227575063705444 acc: 0.97765625\n",
      "33800 loss: 0.08630839735269547 acc: 0.98125\n",
      "34000 loss: 0.06947828829288483 acc: 0.981875\n",
      "34200 loss: 0.04509499669075012 acc: 0.97828126\n",
      "34400 loss: 0.06285395473241806 acc: 0.9795312\n",
      "34600 loss: 0.10673993825912476 acc: 0.9795312\n",
      "34800 loss: 0.06939414143562317 acc: 0.9790625\n",
      "35000 loss: 0.044108860194683075 acc: 0.980625\n",
      "35200 loss: 0.10922549664974213 acc: 0.97546875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35400 loss: 0.1202152743935585 acc: 0.97859377\n",
      "35600 loss: 0.03339654579758644 acc: 0.983125\n",
      "35800 loss: 0.07228201627731323 acc: 0.9814063\n",
      "36000 loss: 0.07519306987524033 acc: 0.9784375\n",
      "36200 loss: 0.07043199241161346 acc: 0.9815625\n",
      "36400 loss: 0.04598293453454971 acc: 0.9815625\n",
      "36600 loss: 0.0376126691699028 acc: 0.97875\n",
      "36800 loss: 0.024701204150915146 acc: 0.9795312\n",
      "37000 loss: 0.03465190529823303 acc: 0.9792187\n",
      "37200 loss: 0.20427921414375305 acc: 0.9764063\n",
      "37400 loss: 0.09065641462802887 acc: 0.98078126\n",
      "37600 loss: 0.08447228372097015 acc: 0.98375\n",
      "37800 loss: 0.04050110653042793 acc: 0.9809375\n",
      "38000 loss: 0.09670419991016388 acc: 0.9821875\n",
      "38200 loss: 0.03540008142590523 acc: 0.9820312\n",
      "38400 loss: 0.038014017045497894 acc: 0.979375\n",
      "38600 loss: 0.06635181605815887 acc: 0.9817188\n",
      "38800 loss: 0.08189349621534348 acc: 0.98078126\n",
      "39000 loss: 0.08569719642400742 acc: 0.976875\n",
      "39200 loss: 0.037264466285705566 acc: 0.9792187\n",
      "39400 loss: 0.02761935070157051 acc: 0.9845312\n",
      "39600 loss: 0.07806360721588135 acc: 0.9828125\n",
      "39800 loss: 0.03037923015654087 acc: 0.9820312\n",
      "40000 loss: 0.09678859263658524 acc: 0.9821875\n",
      "40200 loss: 0.06439101696014404 acc: 0.9820312\n",
      "40400 loss: 0.08400470018386841 acc: 0.9792187\n",
      "40600 loss: 0.03943703696131706 acc: 0.98390627\n",
      "40800 loss: 0.10895179957151413 acc: 0.9789063\n",
      "41000 loss: 0.05118792876601219 acc: 0.9789063\n",
      "41200 loss: 0.0727650448679924 acc: 0.9825\n",
      "41400 loss: 0.05332111939787865 acc: 0.98484373\n",
      "41600 loss: 0.10700156539678574 acc: 0.98078126\n",
      "41800 loss: 0.05803748965263367 acc: 0.98328125\n",
      "42000 loss: 0.037966735661029816 acc: 0.98328125\n",
      "42200 loss: 0.03457450866699219 acc: 0.9814063\n",
      "42400 loss: 0.07065064460039139 acc: 0.9834375\n",
      "42600 loss: 0.10411860048770905 acc: 0.97875\n",
      "42800 loss: 0.058338288217782974 acc: 0.98046875\n",
      "43000 loss: 0.063315749168396 acc: 0.98109376\n",
      "43200 loss: 0.09835756570100784 acc: 0.9846875\n",
      "43400 loss: 0.056378647685050964 acc: 0.98359376\n",
      "43600 loss: 0.06067042425274849 acc: 0.98328125\n",
      "43800 loss: 0.08020608127117157 acc: 0.98328125\n",
      "44000 loss: 0.030684281140565872 acc: 0.9825\n",
      "44200 loss: 0.14650896191596985 acc: 0.98375\n",
      "44400 loss: 0.04985562339425087 acc: 0.9820312\n",
      "44600 loss: 0.08609969168901443 acc: 0.98\n",
      "44800 loss: 0.05622676759958267 acc: 0.9803125\n",
      "45000 loss: 0.061060138046741486 acc: 0.985\n",
      "45200 loss: 0.07688170671463013 acc: 0.98359376\n",
      "45400 loss: 0.07283923774957657 acc: 0.98234373\n",
      "45600 loss: 0.04553430154919624 acc: 0.9845312\n",
      "45800 loss: 0.05583224818110466 acc: 0.9840625\n",
      "46000 loss: 0.08452271670103073 acc: 0.9814063\n",
      "46200 loss: 0.07574823498725891 acc: 0.9842188\n",
      "46400 loss: 0.050237834453582764 acc: 0.981875\n",
      "46600 loss: 0.0458034910261631 acc: 0.98078126\n",
      "46800 loss: 0.07247455418109894 acc: 0.9828125\n",
      "47000 loss: 0.04883428290486336 acc: 0.9859375\n",
      "47200 loss: 0.020912090316414833 acc: 0.9820312\n",
      "47400 loss: 0.05639052391052246 acc: 0.98515624\n",
      "47600 loss: 0.06764013320207596 acc: 0.98390627\n",
      "47800 loss: 0.045805443078279495 acc: 0.98296875\n",
      "48000 loss: 0.05027199909090996 acc: 0.98515624\n",
      "48200 loss: 0.07316895574331284 acc: 0.9820312\n",
      "48400 loss: 0.04793725535273552 acc: 0.981875\n",
      "48600 loss: 0.02956046164035797 acc: 0.981875\n",
      "48800 loss: 0.06916645169258118 acc: 0.98484373\n",
      "49000 loss: 0.057716626673936844 acc: 0.9859375\n",
      "49200 loss: 0.03628632426261902 acc: 0.98390627\n",
      "49400 loss: 0.05912052467465401 acc: 0.98375\n",
      "49600 loss: 0.08358454704284668 acc: 0.9845312\n",
      "49800 loss: 0.05594490095973015 acc: 0.9842188\n",
      "50000 loss: 0.03711088374257088 acc: 0.9834375\n",
      "50200 loss: 0.08054077625274658 acc: 0.98265624\n",
      "50400 loss: 0.09755997359752655 acc: 0.98265624\n",
      "50600 loss: 0.024397846311330795 acc: 0.9859375\n",
      "50800 loss: 0.05320538207888603 acc: 0.984375\n",
      "51000 loss: 0.06367559731006622 acc: 0.98375\n",
      "51200 loss: 0.055904731154441833 acc: 0.98578125\n",
      "51400 loss: 0.03538845479488373 acc: 0.9859375\n",
      "51600 loss: 0.02598547749221325 acc: 0.9834375\n",
      "51800 loss: 0.018183398991823196 acc: 0.9840625\n",
      "52000 loss: 0.025647230446338654 acc: 0.98390627\n",
      "52200 loss: 0.18564622104167938 acc: 0.9814063\n",
      "52400 loss: 0.07746155560016632 acc: 0.9845312\n",
      "52600 loss: 0.06218817085027695 acc: 0.9871875\n",
      "52800 loss: 0.03649487346410751 acc: 0.9840625\n",
      "53000 loss: 0.08209922164678574 acc: 0.98640627\n",
      "53200 loss: 0.028508417308330536 acc: 0.985625\n",
      "53400 loss: 0.02993667498230934 acc: 0.9842188\n",
      "53600 loss: 0.05544015020132065 acc: 0.98546875\n",
      "53800 loss: 0.07126697897911072 acc: 0.984375\n",
      "54000 loss: 0.07256491482257843 acc: 0.98234373\n",
      "54200 loss: 0.03034975193440914 acc: 0.98328125\n",
      "54400 loss: 0.020113252103328705 acc: 0.98765624\n",
      "54600 loss: 0.059200335294008255 acc: 0.9865625\n",
      "54800 loss: 0.026796050369739532 acc: 0.98546875\n",
      "55000 loss: 0.08414000272750854 acc: 0.98640627\n",
      "55200 loss: 0.05824088305234909 acc: 0.98625\n",
      "55400 loss: 0.07135055959224701 acc: 0.98359376\n",
      "55600 loss: 0.032805971801280975 acc: 0.98578125\n",
      "55800 loss: 0.09014102071523666 acc: 0.9840625\n",
      "56000 loss: 0.044422805309295654 acc: 0.9825\n",
      "56200 loss: 0.054468460381031036 acc: 0.98625\n"
     ]
    }
   ],
   "source": [
    "# 耗时巨大\n",
    "for step, (x,y) in enumerate(db):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # 打平操作，[b, 28, 28] => [b, 784]\n",
    "        x = tf.reshape(x, (-1, 28*28))\n",
    "        # Step1. 得到模型输出output [b, 784] => [b, 10]\n",
    "        out = model(x)\n",
    "        # [b] => [b, 10]\n",
    "        y_onehot = tf.one_hot(y, depth=10)\n",
    "        # 计算差的平方和，[b, 10]\n",
    "        loss = tf.square(out-y_onehot)\n",
    "        # 计算每个样本的平均误差，[b]\n",
    "        loss = tf.reduce_sum(loss) / x.shape[0]\n",
    "\n",
    "\n",
    "    acc_meter.update_state(tf.argmax(out, axis=1), y)\n",
    "\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "\n",
    "    if step % 200==0:\n",
    "\n",
    "        print(step, 'loss:', float(loss), 'acc:', acc_meter.result().numpy())\n",
    "        acc_meter.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 改损失函数为交叉熵 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "import IPython.display as display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  tensorflow as tf\n",
    "from    tensorflow.keras import datasets, layers, optimizers, Sequential, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置GPU使用方式\n",
    "# 获取GPU列表\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "    # 设置GPU为增长式占用\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True) \n",
    "    except RuntimeError as e:\n",
    "        # 打印异常\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets: (60000, 28, 28) (60000,) 0 255\n"
     ]
    }
   ],
   "source": [
    "(xs, ys),_ = datasets.mnist.load_data()\n",
    "print('datasets:', xs.shape, ys.shape, xs.min(), xs.max())\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "xs = tf.convert_to_tensor(xs, dtype=tf.float32) / 255.\n",
    "db = tf.data.Dataset.from_tensor_slices((xs,ys))\n",
    "db = db.batch(batch_size).repeat(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             multiple                  200960    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             multiple                  32896     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             multiple                  1290      \n",
      "=================================================================\n",
      "Total params: 235,146\n",
      "Trainable params: 235,146\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([layers.Dense(256, activation='relu'), \n",
    "                     layers.Dense(128, activation='relu'),\n",
    "                     layers.Dense(10,activation='softmax')])\n",
    "model.build(input_shape=(4, 28*28))\n",
    "model.summary()\n",
    "\n",
    "optimizer = optimizers.Adam()\n",
    "acc_meter = metrics.SparseCategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4da8f0b7fb2045cca0dd10a78331c3ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', layout=Layout(flex='2'), max=1), HTML(value='')), layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 耗时巨大\n",
    "iters=-1\n",
    "bar=tqdm_notebook(db,ncols=600)\n",
    "for (x,y) in bar:\n",
    "    iters=iters+1\n",
    "    with tf.GradientTape() as tape:\n",
    "        # 打平操作，[b, 28, 28] => [b, 784]\n",
    "        x = tf.reshape(x, (-1, 28*28))\n",
    "        # Step1. 得到模型输出output [b, 784] => [b, 10]\n",
    "        out = model(x)\n",
    "        # [b] => [b, 10]\n",
    "#         y_onehot = tf.one_hot(y, depth=10)\n",
    "        # 计算差的平方和，[b, 10]\n",
    "        loss = loss_object(y,out)\n",
    "#         # 计算每个样本的平均误差，[b]\n",
    "#         loss = tf.reduce_sum(loss) / x.shape[0]\n",
    "\n",
    "\n",
    "    acc_meter.update_state(y,out)\n",
    "\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "\n",
    "    if iters % 200==0:\n",
    "        bar.set_description(\"loss{0:.3f},acc{1:.3f}\".format(float(loss),acc_meter.result().numpy()))\n",
    "#         print(iters, 'loss:', float(loss), 'acc:', acc_meter.result().numpy())\n",
    "        acc_meter.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 手写训练参数，损失函数为MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "# Default parameters for plots\n",
    "matplotlib.rcParams['font.size'] = 20\n",
    "matplotlib.rcParams['figure.titlesize'] = 20\n",
    "matplotlib.rcParams['figure.figsize'] = [9, 7]\n",
    "matplotlib.rcParams['font.family'] = ['STKaiTi']\n",
    "matplotlib.rcParams['axes.unicode_minus']=False \n",
    "\n",
    "import  tensorflow as tf\n",
    "from    tensorflow import keras\n",
    "from    tensorflow.keras import datasets\n",
    "import  os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,) <dtype: 'float32'> <dtype: 'int32'>\n",
      "tf.Tensor(0.0, shape=(), dtype=float32) tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "tf.Tensor(0, shape=(), dtype=int32) tf.Tensor(9, shape=(), dtype=int32)\n",
      "batch: (128, 28, 28) (128,)\n"
     ]
    }
   ],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# x: [60k, 28, 28],\n",
    "# y: [60k]\n",
    "(x, y), _ = datasets.mnist.load_data()\n",
    "# x: [0~255] => [0~1.]\n",
    "x = tf.convert_to_tensor(x, dtype=tf.float32) / 255.\n",
    "y = tf.convert_to_tensor(y, dtype=tf.int32)\n",
    "\n",
    "print(x.shape, y.shape, x.dtype, y.dtype)\n",
    "print(tf.reduce_min(x), tf.reduce_max(x))\n",
    "print(tf.reduce_min(y), tf.reduce_max(y))\n",
    "\n",
    "\n",
    "train_db = tf.data.Dataset.from_tensor_slices((x,y)).batch(128)\n",
    "train_iter = iter(train_db)\n",
    "sample = next(train_iter)\n",
    "print('batch:', sample[0].shape, sample[1].shape)\n",
    "\n",
    "\n",
    "# [b, 784] => [b, 256] => [b, 128] => [b, 10]\n",
    "# [dim_in, dim_out], [dim_out]\n",
    "w1 = tf.Variable(tf.random.truncated_normal([784, 256], stddev=0.1))\n",
    "b1 = tf.Variable(tf.zeros([256]))\n",
    "w2 = tf.Variable(tf.random.truncated_normal([256, 128], stddev=0.1))\n",
    "b2 = tf.Variable(tf.zeros([128]))\n",
    "w3 = tf.Variable(tf.random.truncated_normal([128, 10], stddev=0.1))\n",
    "b3 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "lr = 1e-3\n",
    "\n",
    "losses = []\n",
    "acc_meter = tf.keras.metrics.SparseCategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 loss: 0.3248775005340576  acc: 0.1796875\n",
      "0 100 loss: 0.28446534276008606  acc: 0.16335937\n",
      "0 200 loss: 0.2835283577442169  acc: 0.16648437\n",
      "0 300 loss: 0.24933362007141113  acc: 0.17023438\n",
      "0 400 loss: 0.24124212563037872  acc: 0.16664062\n",
      "1 0 loss: 0.236648827791214  acc: 0.16738637\n",
      "1 100 loss: 0.2251092493534088  acc: 0.17734376\n",
      "1 200 loss: 0.2283942699432373  acc: 0.17429687\n",
      "1 300 loss: 0.20752520859241486  acc: 0.17742187\n",
      "1 400 loss: 0.20413661003112793  acc: 0.17695312\n",
      "2 0 loss: 0.2086954116821289  acc: 0.1790909\n",
      "2 100 loss: 0.2058030664920807  acc: 0.18671875\n",
      "2 200 loss: 0.20816676318645477  acc: 0.18703125\n",
      "2 300 loss: 0.19088482856750488  acc: 0.1825\n",
      "2 400 loss: 0.18938495218753815  acc: 0.1834375\n",
      "3 0 loss: 0.19668853282928467  acc: 0.18647727\n",
      "3 100 loss: 0.19711463153362274  acc: 0.19234376\n",
      "3 200 loss: 0.19781233370304108  acc: 0.19171876\n",
      "3 300 loss: 0.18211330473423004  acc: 0.18695313\n",
      "3 400 loss: 0.18142621219158173  acc: 0.18804687\n",
      "4 0 loss: 0.18957862257957458  acc: 0.18920454\n",
      "4 100 loss: 0.19160714745521545  acc: 0.19625\n",
      "4 200 loss: 0.1907677948474884  acc: 0.19679688\n",
      "4 300 loss: 0.17619545757770538  acc: 0.19296876\n",
      "4 400 loss: 0.1758430302143097  acc: 0.19320312\n",
      "5 0 loss: 0.18422181904315948  acc: 0.19522727\n",
      "5 100 loss: 0.18719586730003357  acc: 0.19921875\n",
      "5 200 loss: 0.18505987524986267  acc: 0.20078126\n",
      "5 300 loss: 0.17149290442466736  acc: 0.19671875\n",
      "5 400 loss: 0.17125637829303741  acc: 0.19820313\n",
      "6 0 loss: 0.17962975800037384  acc: 0.1996591\n",
      "6 100 loss: 0.1832369863986969  acc: 0.20492187\n",
      "6 200 loss: 0.1800379455089569  acc: 0.2059375\n",
      "6 300 loss: 0.16743770241737366  acc: 0.20148438\n",
      "6 400 loss: 0.1671913117170334  acc: 0.20398438\n",
      "7 0 loss: 0.175467848777771  acc: 0.20568182\n",
      "7 100 loss: 0.179549902677536  acc: 0.20960937\n",
      "7 200 loss: 0.17547295987606049  acc: 0.21\n",
      "7 300 loss: 0.1637682467699051  acc: 0.20585938\n",
      "7 400 loss: 0.1634601205587387  acc: 0.21007812\n",
      "8 0 loss: 0.1716236174106598  acc: 0.21102273\n",
      "8 100 loss: 0.17607350647449493  acc: 0.2146875\n",
      "8 200 loss: 0.17125023901462555  acc: 0.21539062\n",
      "8 300 loss: 0.16039177775382996  acc: 0.21171875\n",
      "8 400 loss: 0.1599852740764618  acc: 0.2165625\n",
      "9 0 loss: 0.16802789270877838  acc: 0.21613637\n",
      "9 100 loss: 0.17279674112796783  acc: 0.22015625\n",
      "9 200 loss: 0.16733382642269135  acc: 0.219375\n",
      "9 300 loss: 0.15726061165332794  acc: 0.2171875\n",
      "9 400 loss: 0.15672554075717926  acc: 0.22148438\n",
      "10 0 loss: 0.1646578311920166  acc: 0.22227272\n",
      "10 100 loss: 0.16969993710517883  acc: 0.22515625\n",
      "10 200 loss: 0.16367892920970917  acc: 0.22484376\n",
      "10 300 loss: 0.15432175993919373  acc: 0.22359376\n",
      "10 400 loss: 0.15365386009216309  acc: 0.22726563\n",
      "11 0 loss: 0.1614820957183838  acc: 0.22886364\n",
      "11 100 loss: 0.1667633205652237  acc: 0.23085937\n",
      "11 200 loss: 0.1602483093738556  acc: 0.23046875\n",
      "11 300 loss: 0.15155386924743652  acc: 0.22867188\n",
      "11 400 loss: 0.15075090527534485  acc: 0.23117188\n",
      "12 0 loss: 0.15848103165626526  acc: 0.23431818\n",
      "12 100 loss: 0.16397684812545776  acc: 0.23742187\n",
      "12 200 loss: 0.15702423453330994  acc: 0.2365625\n",
      "12 300 loss: 0.1489432156085968  acc: 0.23484375\n",
      "12 400 loss: 0.1480116993188858  acc: 0.23757812\n",
      "13 0 loss: 0.15564671158790588  acc: 0.24011363\n",
      "13 100 loss: 0.16133466362953186  acc: 0.24296875\n",
      "13 200 loss: 0.15397781133651733  acc: 0.24179688\n",
      "13 300 loss: 0.14646756649017334  acc: 0.24226563\n",
      "13 400 loss: 0.145421102643013  acc: 0.24226563\n",
      "14 0 loss: 0.1529606282711029  acc: 0.24784091\n",
      "14 100 loss: 0.15881921350955963  acc: 0.24929687\n",
      "14 200 loss: 0.1510920524597168  acc: 0.24773437\n",
      "14 300 loss: 0.14412075281143188  acc: 0.24703126\n",
      "14 400 loss: 0.14296679198741913  acc: 0.24859375\n",
      "15 0 loss: 0.15040698647499084  acc: 0.2559091\n",
      "15 100 loss: 0.15642496943473816  acc: 0.254375\n",
      "15 200 loss: 0.14836883544921875  acc: 0.25320312\n",
      "15 300 loss: 0.1418982744216919  acc: 0.25320312\n",
      "15 400 loss: 0.14064040780067444  acc: 0.25359374\n",
      "16 0 loss: 0.14798295497894287  acc: 0.26238635\n",
      "16 100 loss: 0.15414777398109436  acc: 0.25976562\n",
      "16 200 loss: 0.14579275250434875  acc: 0.2589844\n",
      "16 300 loss: 0.1397894024848938  acc: 0.258125\n",
      "16 400 loss: 0.13843175768852234  acc: 0.2602344\n",
      "17 0 loss: 0.14568179845809937  acc: 0.26920456\n",
      "17 100 loss: 0.15198491513729095  acc: 0.26632813\n",
      "17 200 loss: 0.1433505117893219  acc: 0.26421875\n",
      "17 300 loss: 0.13778206706047058  acc: 0.26390624\n",
      "17 400 loss: 0.13634082674980164  acc: 0.2657031\n",
      "18 0 loss: 0.14349260926246643  acc: 0.27511364\n",
      "18 100 loss: 0.1499232053756714  acc: 0.27257812\n",
      "18 200 loss: 0.14102789759635925  acc: 0.27007812\n",
      "18 300 loss: 0.13587726652622223  acc: 0.27125\n",
      "18 400 loss: 0.13435795903205872  acc: 0.2722656\n",
      "19 0 loss: 0.14140406250953674  acc: 0.28102273\n",
      "19 100 loss: 0.1479548066854477  acc: 0.27804688\n",
      "19 200 loss: 0.13881716132164001  acc: 0.27609375\n",
      "19 300 loss: 0.13405929505825043  acc: 0.2758594\n",
      "19 400 loss: 0.1324732005596161  acc: 0.27835938\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20): # iterate db for 10\n",
    "    for step, (x, y) in enumerate(train_db): # for every batch\n",
    "        # x:[128, 28, 28]\n",
    "        # y: [128]\n",
    "\n",
    "        # [b, 28, 28] => [b, 28*28]\n",
    "        x = tf.reshape(x, [-1, 28*28])\n",
    "\n",
    "        with tf.GradientTape() as tape: # tf.Variable\n",
    "            # x: [b, 28*28]\n",
    "            # h1 = x@w1 + b1\n",
    "            # [b, 784]@[784, 256] + [256] => [b, 256] + [256] => [b, 256] + [b, 256]\n",
    "            h1 = x@w1 + tf.broadcast_to(b1, [x.shape[0], 256])\n",
    "            h1 = tf.nn.relu(h1)\n",
    "            # [b, 256] => [b, 128]\n",
    "            h2 = h1@w2 + b2\n",
    "            h2 = tf.nn.relu(h2)\n",
    "            # [b, 128] => [b, 10]\n",
    "            out = h2@w3 + b3\n",
    "\n",
    "            # compute loss\n",
    "            # out: [b, 10]\n",
    "            # y: [b] => [b, 10]\n",
    "            y_onehot = tf.one_hot(y, depth=10)\n",
    "\n",
    "            # mse = mean(sum(y-out)^2)\n",
    "            # [b, 10]\n",
    "            loss = tf.square(y_onehot - out)\n",
    "            # mean: scalar\n",
    "            loss = tf.reduce_mean(loss)\n",
    "\n",
    "        # compute gradients\n",
    "        grads = tape.gradient(loss, [w1, b1, w2, b2, w3, b3])\n",
    "        # print(grads)\n",
    "        # w1 = w1 - lr * w1_grad\n",
    "        w1.assign_sub(lr * grads[0])\n",
    "        b1.assign_sub(lr * grads[1])\n",
    "        w2.assign_sub(lr * grads[2])\n",
    "        b2.assign_sub(lr * grads[3])\n",
    "        w3.assign_sub(lr * grads[4])\n",
    "        b3.assign_sub(lr * grads[5])\n",
    "        \n",
    "        acc_meter.update_state(y,out)\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(epoch, step, 'loss:', float(loss),' acc:',acc_meter.result().numpy())\n",
    "            acc_meter.reset_states()\n",
    "    losses.append(float(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses, color='C0', marker='s', label='训练')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.ylabel('MSE')\n",
    "# plt.savefig('forward.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 手写训练参数，损失函数为交叉熵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "# Default parameters for plots\n",
    "matplotlib.rcParams['font.size'] = 20\n",
    "matplotlib.rcParams['figure.titlesize'] = 20\n",
    "matplotlib.rcParams['figure.figsize'] = [9, 7]\n",
    "matplotlib.rcParams['font.family'] = ['STKaiTi']\n",
    "matplotlib.rcParams['axes.unicode_minus']=False \n",
    "\n",
    "import  tensorflow as tf\n",
    "from    tensorflow import keras\n",
    "from    tensorflow.keras import datasets\n",
    "import  os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,) <dtype: 'float32'> <dtype: 'int32'>\n",
      "tf.Tensor(0.0, shape=(), dtype=float32) tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "tf.Tensor(0, shape=(), dtype=int32) tf.Tensor(9, shape=(), dtype=int32)\n",
      "batch: (128, 28, 28) (128,)\n"
     ]
    }
   ],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# x: [60k, 28, 28],\n",
    "# y: [60k]\n",
    "(x, y), _ = datasets.mnist.load_data()\n",
    "# x: [0~255] => [0~1.]\n",
    "x = tf.convert_to_tensor(x, dtype=tf.float32) / 255.\n",
    "y = tf.convert_to_tensor(y, dtype=tf.int32)\n",
    "\n",
    "print(x.shape, y.shape, x.dtype, y.dtype)\n",
    "print(tf.reduce_min(x), tf.reduce_max(x))\n",
    "print(tf.reduce_min(y), tf.reduce_max(y))\n",
    "\n",
    "\n",
    "train_db = tf.data.Dataset.from_tensor_slices((x,y)).batch(128)\n",
    "train_iter = iter(train_db)\n",
    "sample = next(train_iter)\n",
    "print('batch:', sample[0].shape, sample[1].shape)\n",
    "\n",
    "\n",
    "# [b, 784] => [b, 256] => [b, 128] => [b, 10]\n",
    "# [dim_in, dim_out], [dim_out]\n",
    "w1 = tf.Variable(tf.random.truncated_normal([784, 256], stddev=0.1))\n",
    "b1 = tf.Variable(tf.zeros([256]))\n",
    "w2 = tf.Variable(tf.random.truncated_normal([256, 128], stddev=0.1))\n",
    "b2 = tf.Variable(tf.zeros([128]))\n",
    "w3 = tf.Variable(tf.random.truncated_normal([128, 10], stddev=0.1))\n",
    "b3 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "lr = 1e-3\n",
    "\n",
    "losses = []\n",
    "acc_meter = metrics.SparseCategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 loss: 2.311211585998535  acc: 0.6740909\n",
      "0 100 loss: 2.273275852203369  acc: 0.15359375\n",
      "0 200 loss: 2.116412878036499  acc: 0.22101563\n",
      "0 300 loss: 2.0204038619995117  acc: 0.29179686\n",
      "0 400 loss: 1.9008249044418335  acc: 0.37460938\n",
      "1 0 loss: 1.830556869506836  acc: 0.45943183\n",
      "1 100 loss: 1.8329212665557861  acc: 0.5071875\n",
      "1 200 loss: 1.6500616073608398  acc: 0.53375\n",
      "1 300 loss: 1.6206507682800293  acc: 0.5692969\n",
      "1 400 loss: 1.528738260269165  acc: 0.5975781\n",
      "2 0 loss: 1.4480079412460327  acc: 0.6514773\n",
      "2 100 loss: 1.4876084327697754  acc: 0.65335935\n",
      "2 200 loss: 1.2749314308166504  acc: 0.65398437\n",
      "2 300 loss: 1.2831480503082275  acc: 0.6766406\n",
      "2 400 loss: 1.2421772480010986  acc: 0.6892969\n",
      "3 0 loss: 1.1509872674942017  acc: 0.73136365\n",
      "3 100 loss: 1.2081576585769653  acc: 0.72632813\n",
      "3 200 loss: 0.9985957145690918  acc: 0.71828127\n",
      "3 300 loss: 1.0393104553222656  acc: 0.73695314\n",
      "3 400 loss: 1.0432581901550293  acc: 0.74507815\n",
      "4 0 loss: 0.9462477564811707  acc: 0.7846591\n",
      "4 100 loss: 1.0039777755737305  acc: 0.774375\n",
      "4 200 loss: 0.8117508292198181  acc: 0.760625\n",
      "4 300 loss: 0.8741839528083801  acc: 0.77375\n",
      "4 400 loss: 0.906623125076294  acc: 0.7796094\n",
      "5 0 loss: 0.8096656799316406  acc: 0.8148864\n",
      "5 100 loss: 0.8588958382606506  acc: 0.8016406\n",
      "5 200 loss: 0.686604380607605  acc: 0.78867185\n",
      "5 300 loss: 0.762417197227478  acc: 0.7974219\n",
      "5 400 loss: 0.810148298740387  acc: 0.799375\n",
      "6 0 loss: 0.7161828875541687  acc: 0.83522725\n",
      "6 100 loss: 0.7553275227546692  acc: 0.81992185\n",
      "6 200 loss: 0.6003581881523132  acc: 0.80789065\n",
      "6 300 loss: 0.6833979487419128  acc: 0.8130469\n",
      "6 400 loss: 0.7396091222763062  acc: 0.8146875\n",
      "7 0 loss: 0.6495084166526794  acc: 0.8475\n",
      "7 100 loss: 0.6789604425430298  acc: 0.8317969\n",
      "7 200 loss: 0.5382375121116638  acc: 0.8222656\n",
      "7 300 loss: 0.6248852610588074  acc: 0.82585937\n",
      "7 400 loss: 0.6864309906959534  acc: 0.8265625\n",
      "8 0 loss: 0.5997041463851929  acc: 0.8570455\n",
      "8 100 loss: 0.6209056377410889  acc: 0.8414062\n",
      "8 200 loss: 0.4913550019264221  acc: 0.8335937\n",
      "8 300 loss: 0.5801041126251221  acc: 0.8359375\n",
      "8 400 loss: 0.6453357338905334  acc: 0.8353125\n",
      "9 0 loss: 0.5611670017242432  acc: 0.8643182\n",
      "9 100 loss: 0.5757982730865479  acc: 0.8496875\n",
      "9 200 loss: 0.454805850982666  acc: 0.8414062\n",
      "9 300 loss: 0.5448601245880127  acc: 0.8444531\n",
      "9 400 loss: 0.6129509210586548  acc: 0.8430469\n",
      "10 0 loss: 0.5303987264633179  acc: 0.8710227\n",
      "10 100 loss: 0.5396832227706909  acc: 0.8570312\n",
      "10 200 loss: 0.4256376028060913  acc: 0.84765625\n",
      "10 300 loss: 0.5163934826850891  acc: 0.85117185\n",
      "10 400 loss: 0.5862133502960205  acc: 0.8500781\n",
      "11 0 loss: 0.5048658847808838  acc: 0.87704545\n",
      "11 100 loss: 0.5099880695343018  acc: 0.86265624\n",
      "11 200 loss: 0.40159010887145996  acc: 0.8534375\n",
      "11 300 loss: 0.4929155111312866  acc: 0.8567188\n",
      "11 400 loss: 0.5638967752456665  acc: 0.85484374\n",
      "12 0 loss: 0.4832974076271057  acc: 0.88170457\n",
      "12 100 loss: 0.4850035607814789  acc: 0.8675\n",
      "12 200 loss: 0.38138067722320557  acc: 0.85875\n",
      "12 300 loss: 0.4732270836830139  acc: 0.8608594\n",
      "12 400 loss: 0.545009434223175  acc: 0.85914063\n",
      "13 0 loss: 0.46481096744537354  acc: 0.885\n",
      "13 100 loss: 0.4637051820755005  acc: 0.87289065\n",
      "13 200 loss: 0.36410167813301086  acc: 0.8627344\n",
      "13 300 loss: 0.45619040727615356  acc: 0.8640625\n",
      "13 400 loss: 0.5286539793014526  acc: 0.86390626\n",
      "14 0 loss: 0.44870853424072266  acc: 0.8875\n",
      "14 100 loss: 0.44529110193252563  acc: 0.8764844\n",
      "14 200 loss: 0.34904924035072327  acc: 0.8664844\n",
      "14 300 loss: 0.4414622187614441  acc: 0.86640626\n",
      "14 400 loss: 0.5144237279891968  acc: 0.8670313\n",
      "15 0 loss: 0.4345695972442627  acc: 0.89022726\n",
      "15 100 loss: 0.4291062355041504  acc: 0.8794531\n",
      "15 200 loss: 0.3359261155128479  acc: 0.87109375\n",
      "15 300 loss: 0.42861226201057434  acc: 0.8702344\n",
      "15 400 loss: 0.5018196702003479  acc: 0.86929685\n",
      "16 0 loss: 0.4220428764820099  acc: 0.89261365\n",
      "16 100 loss: 0.41465839743614197  acc: 0.8819531\n",
      "16 200 loss: 0.32444125413894653  acc: 0.8739844\n",
      "16 300 loss: 0.4172167479991913  acc: 0.87296873\n",
      "16 400 loss: 0.4904235601425171  acc: 0.8713281\n",
      "17 0 loss: 0.4107005000114441  acc: 0.89511365\n",
      "17 100 loss: 0.4016887843608856  acc: 0.88484377\n",
      "17 200 loss: 0.31424590945243835  acc: 0.8771875\n",
      "17 300 loss: 0.4070162773132324  acc: 0.87578124\n",
      "17 400 loss: 0.48007553815841675  acc: 0.87390625\n",
      "18 0 loss: 0.4004143178462982  acc: 0.8969318\n",
      "18 100 loss: 0.38997694849967957  acc: 0.8866406\n",
      "18 200 loss: 0.30510061979293823  acc: 0.8801563\n",
      "18 300 loss: 0.3979172110557556  acc: 0.87875\n",
      "18 400 loss: 0.4705449938774109  acc: 0.87640625\n",
      "19 0 loss: 0.39096662402153015  acc: 0.89931816\n",
      "19 100 loss: 0.3793354034423828  acc: 0.88875\n",
      "19 200 loss: 0.29690244793891907  acc: 0.88234377\n",
      "19 300 loss: 0.38960230350494385  acc: 0.88078123\n",
      "19 400 loss: 0.461758553981781  acc: 0.8791406\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20): # iterate db for 10\n",
    "    for step, (x, y) in enumerate(train_db): # for every batch\n",
    "        # x:[128, 28, 28]\n",
    "        # y: [128]\n",
    "\n",
    "        # [b, 28, 28] => [b, 28*28]\n",
    "        x = tf.reshape(x, [-1, 28*28])\n",
    "\n",
    "        with tf.GradientTape() as tape: # tf.Variable\n",
    "            # x: [b, 28*28]\n",
    "            # h1 = x@w1 + b1\n",
    "            # [b, 784]@[784, 256] + [256] => [b, 256] + [256] => [b, 256] + [b, 256]\n",
    "            h1 = x@w1 + tf.broadcast_to(b1, [x.shape[0], 256])\n",
    "            h1 = tf.nn.relu(h1)\n",
    "            # [b, 256] => [b, 128]\n",
    "            h2 = h1@w2 + b2\n",
    "            h2 = tf.nn.relu(h2)\n",
    "            # [b, 128] => [b, 10]\n",
    "            out = h2@w3 + b3\n",
    "            out = tf.nn.softmax(out)\n",
    "            # compute loss\n",
    "            # out: [b, 10]\n",
    "            # y: [b] => [b, 10]\n",
    "#             y_onehot = tf.one_hot(y, depth=10)\n",
    "\n",
    "            # mse = mean(sum(y-out)^2)\n",
    "            # [b, 10]\n",
    "            loss = loss_object(y,out)\n",
    "            # mean: scalar\n",
    "#             loss = tf.reduce_mean(loss)\n",
    "\n",
    "        # compute gradients\n",
    "        grads = tape.gradient(loss, [w1, b1, w2, b2, w3, b3])\n",
    "        # print(grads)\n",
    "        # w1 = w1 - lr * w1_grad\n",
    "        w1.assign_sub(lr * grads[0])\n",
    "        b1.assign_sub(lr * grads[1])\n",
    "        w2.assign_sub(lr * grads[2])\n",
    "        b2.assign_sub(lr * grads[3])\n",
    "        w3.assign_sub(lr * grads[4])\n",
    "        b3.assign_sub(lr * grads[5])\n",
    "        \n",
    "        acc_meter.update_state(y,out)\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(epoch, step, 'loss:', float(loss),' acc:',acc_meter.result().numpy())\n",
    "            acc_meter.reset_states()\n",
    "    losses.append(float(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses, color='C0', marker='s', label='训练')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.ylabel('MSE')\n",
    "# plt.savefig('forward.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 调用库函数，损失函数为MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "\n",
    "import  tensorflow as tf\n",
    "from    tensorflow import keras\n",
    "from    tensorflow.keras import layers, optimizers, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n"
     ]
    }
   ],
   "source": [
    "(x, y), (x_val, y_val) = datasets.mnist.load_data() \n",
    "x = tf.convert_to_tensor(x, dtype=tf.float32) / 255.\n",
    "y = tf.convert_to_tensor(y, dtype=tf.int32)\n",
    "# y = tf.one_hot(y, depth=10)\n",
    "print(x.shape, y.shape)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "train_dataset = train_dataset.batch(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([ \n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(10)])\n",
    "optimizer = optimizers.SGD(learning_rate=0.001)\n",
    "acc_meter = metrics.SparseCategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch):\n",
    "\n",
    "    # Step4.loop\n",
    "    for step, (x, y) in enumerate(train_dataset):\n",
    "\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # [b, 28, 28] => [b, 784]\n",
    "            x = tf.reshape(x, (-1, 28*28))\n",
    "            # Step1. compute output\n",
    "            # [b, 784] => [b, 10]\n",
    "            out = model(x)\n",
    "            \n",
    "            y_onehot = tf.one_hot(y, depth=10)\n",
    "            \n",
    "            # Step2. compute loss\n",
    "            loss = tf.reduce_sum(tf.square(out - y_onehot)) / x.shape[0]\n",
    "\n",
    "        # Step3. optimize and update w1, w2, w3, b1, b2, b3\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        # w' = w - lr * grad\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        \n",
    "        acc_meter.update_state(y,out)\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(epoch, step, 'loss:', loss.numpy(),' acc:',acc_meter.result().numpy())\n",
    "            acc_meter.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 loss: 1.8774716  acc: 0.075\n",
      "0 100 loss: 0.9665799  acc: 0.1758\n",
      "0 200 loss: 0.7579569  acc: 0.41275\n",
      "1 0 loss: 0.6786789  acc: 0.5691\n",
      "1 100 loss: 0.6868919  acc: 0.6414\n",
      "1 200 loss: 0.5671677  acc: 0.68565\n",
      "2 0 loss: 0.548317  acc: 0.73515\n",
      "2 100 loss: 0.5926056  acc: 0.74545\n",
      "2 200 loss: 0.4923066  acc: 0.76195\n",
      "3 0 loss: 0.48660746  acc: 0.7928\n",
      "3 100 loss: 0.54066175  acc: 0.7914\n",
      "3 200 loss: 0.4489276  acc: 0.8011\n",
      "4 0 loss: 0.4482039  acc: 0.82135\n",
      "4 100 loss: 0.5052299  acc: 0.816\n",
      "4 200 loss: 0.41827935  acc: 0.82205\n",
      "5 0 loss: 0.42110932  acc: 0.8394\n",
      "5 100 loss: 0.47841254  acc: 0.8339\n",
      "5 200 loss: 0.39507523  acc: 0.83835\n",
      "6 0 loss: 0.4000894  acc: 0.8539\n",
      "6 100 loss: 0.4570197  acc: 0.84625\n",
      "6 200 loss: 0.37625104  acc: 0.8495\n",
      "7 0 loss: 0.38318336  acc: 0.8647\n",
      "7 100 loss: 0.43926594  acc: 0.8556\n",
      "7 200 loss: 0.36047664  acc: 0.859\n",
      "8 0 loss: 0.3691645  acc: 0.87185\n",
      "8 100 loss: 0.4242443  acc: 0.86345\n",
      "8 200 loss: 0.3470456  acc: 0.8675\n",
      "9 0 loss: 0.35727313  acc: 0.87875\n",
      "9 100 loss: 0.4111853  acc: 0.8703\n",
      "9 200 loss: 0.33526275  acc: 0.87345\n",
      "10 0 loss: 0.34696525  acc: 0.8831\n",
      "10 100 loss: 0.3996301  acc: 0.87575\n",
      "10 200 loss: 0.32471755  acc: 0.87815\n",
      "11 0 loss: 0.3377511  acc: 0.8874\n",
      "11 100 loss: 0.38927203  acc: 0.88055\n",
      "11 200 loss: 0.31532  acc: 0.88185\n",
      "12 0 loss: 0.32942837  acc: 0.89075\n",
      "12 100 loss: 0.37988257  acc: 0.88495\n",
      "12 200 loss: 0.30693957  acc: 0.8861\n",
      "13 0 loss: 0.3218844  acc: 0.8939\n",
      "13 100 loss: 0.37121144  acc: 0.8892\n",
      "13 200 loss: 0.2993647  acc: 0.8895\n",
      "14 0 loss: 0.31502372  acc: 0.89775\n",
      "14 100 loss: 0.36314178  acc: 0.8926\n",
      "14 200 loss: 0.29249576  acc: 0.8929\n",
      "15 0 loss: 0.30880356  acc: 0.9002\n",
      "15 100 loss: 0.35571817  acc: 0.8957\n",
      "15 200 loss: 0.2862424  acc: 0.89535\n",
      "16 0 loss: 0.30294245  acc: 0.9026\n",
      "16 100 loss: 0.3487503  acc: 0.89845\n",
      "16 200 loss: 0.28040978  acc: 0.8982\n",
      "17 0 loss: 0.2974241  acc: 0.90505\n",
      "17 100 loss: 0.34221974  acc: 0.90175\n",
      "17 200 loss: 0.2750139  acc: 0.90035\n",
      "18 0 loss: 0.29223698  acc: 0.907\n",
      "18 100 loss: 0.33614963  acc: 0.9034\n",
      "18 200 loss: 0.26990882  acc: 0.9026\n",
      "19 0 loss: 0.28732908  acc: 0.90855\n",
      "19 100 loss: 0.3305195  acc: 0.90495\n",
      "19 200 loss: 0.26511964  acc: 0.90435\n",
      "20 0 loss: 0.2826718  acc: 0.91055\n",
      "20 100 loss: 0.32525906  acc: 0.90765\n",
      "20 200 loss: 0.26063296  acc: 0.90615\n",
      "21 0 loss: 0.27825633  acc: 0.912\n",
      "21 100 loss: 0.3203447  acc: 0.9096\n",
      "21 200 loss: 0.2564199  acc: 0.9084\n",
      "22 0 loss: 0.27404463  acc: 0.91315\n",
      "22 100 loss: 0.3157249  acc: 0.9115\n",
      "22 200 loss: 0.25244233  acc: 0.9101\n",
      "23 0 loss: 0.27008662  acc: 0.91475\n",
      "23 100 loss: 0.31137523  acc: 0.9124\n",
      "23 200 loss: 0.24865723  acc: 0.9114\n",
      "24 0 loss: 0.2662836  acc: 0.91615\n",
      "24 100 loss: 0.30726668  acc: 0.9142\n",
      "24 200 loss: 0.24504387  acc: 0.9127\n",
      "25 0 loss: 0.2626779  acc: 0.91725\n",
      "25 100 loss: 0.3033664  acc: 0.9156\n",
      "25 200 loss: 0.24162273  acc: 0.91395\n",
      "26 0 loss: 0.25923342  acc: 0.91855\n",
      "26 100 loss: 0.29962528  acc: 0.9165\n",
      "26 200 loss: 0.23836817  acc: 0.9155\n",
      "27 0 loss: 0.25592563  acc: 0.9198\n",
      "27 100 loss: 0.29609862  acc: 0.91755\n",
      "27 200 loss: 0.23528267  acc: 0.9164\n",
      "28 0 loss: 0.25274473  acc: 0.92095\n",
      "28 100 loss: 0.29272944  acc: 0.91915\n",
      "28 200 loss: 0.23233543  acc: 0.91765\n",
      "29 0 loss: 0.24966398  acc: 0.922\n",
      "29 100 loss: 0.28950268  acc: 0.9205\n",
      "29 200 loss: 0.22952732  acc: 0.91895\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(30):\n",
    "    train_epoch(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 调用库函数，损失函数为交叉熵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "\n",
    "import  tensorflow as tf\n",
    "from    tensorflow import keras\n",
    "from    tensorflow.keras import layers, optimizers, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n"
     ]
    }
   ],
   "source": [
    "(x, y), (x_val, y_val) = datasets.mnist.load_data() \n",
    "x = tf.convert_to_tensor(x, dtype=tf.float32) / 255.\n",
    "y = tf.convert_to_tensor(y, dtype=tf.int32)\n",
    "# y = tf.one_hot(y, depth=10)\n",
    "print(x.shape, y.shape)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "train_dataset = train_dataset.batch(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([ \n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(10,activation='softmax')])\n",
    "optimizer = optimizers.SGD(learning_rate=0.001)\n",
    "acc_meter = metrics.SparseCategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch):\n",
    "\n",
    "    # Step4.loop\n",
    "    for step, (x, y) in enumerate(train_dataset):\n",
    "\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # [b, 28, 28] => [b, 784]\n",
    "            x = tf.reshape(x, (-1, 28*28))\n",
    "            # Step1. compute output\n",
    "            # [b, 784] => [b, 10]\n",
    "            out = model(x)\n",
    "            \n",
    "            # Step2. compute loss\n",
    "            loss = loss_object(y,out)\n",
    "\n",
    "        # Step3. optimize and update w1, w2, w3, b1, b2, b3\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        # w' = w - lr * grad\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        \n",
    "        acc_meter.update_state(y,out)\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(epoch, step, 'loss:', loss.numpy(),' acc:',acc_meter.result().numpy())\n",
    "            acc_meter.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 loss: 2.3235595  acc: 0.105\n",
      "0 100 loss: 2.2804399  acc: 0.1387\n",
      "0 200 loss: 2.1543093  acc: 0.22895\n",
      "1 0 loss: 2.0839617  acc: 0.33835\n",
      "1 100 loss: 2.0739968  acc: 0.4592\n",
      "1 200 loss: 1.9301246  acc: 0.5425\n",
      "2 0 loss: 1.8636914  acc: 0.60345\n",
      "2 100 loss: 1.8756468  acc: 0.6399\n",
      "2 200 loss: 1.7066371  acc: 0.66845\n",
      "3 0 loss: 1.6440017  acc: 0.7026\n",
      "3 100 loss: 1.6761326  acc: 0.7106\n",
      "3 200 loss: 1.4868671  acc: 0.72385\n",
      "4 0 loss: 1.436232  acc: 0.75025\n",
      "4 100 loss: 1.4843764  acc: 0.7499\n",
      "4 200 loss: 1.2864811  acc: 0.75865\n",
      "5 0 loss: 1.2541988  acc: 0.78105\n",
      "5 100 loss: 1.314904  acc: 0.77745\n",
      "5 200 loss: 1.1158532  acc: 0.78435\n",
      "6 0 loss: 1.103311  acc: 0.80475\n",
      "6 100 loss: 1.1727614  acc: 0.79925\n",
      "6 200 loss: 0.9767914  acc: 0.80435\n",
      "7 0 loss: 0.9817762  acc: 0.8225\n",
      "7 100 loss: 1.0569634  acc: 0.8147\n",
      "7 200 loss: 0.8657117  acc: 0.81895\n",
      "8 0 loss: 0.88548654  acc: 0.83425\n",
      "8 100 loss: 0.9640691  acc: 0.82745\n",
      "8 200 loss: 0.7780563  acc: 0.83005\n",
      "9 0 loss: 0.8088976  acc: 0.8438\n",
      "9 100 loss: 0.8893556  acc: 0.836\n",
      "9 200 loss: 0.708362  acc: 0.8374\n",
      "10 0 loss: 0.74727005  acc: 0.85035\n",
      "10 100 loss: 0.8285467  acc: 0.8428\n",
      "10 200 loss: 0.6521965  acc: 0.844\n",
      "11 0 loss: 0.6972875  acc: 0.8553\n",
      "11 100 loss: 0.7783569  acc: 0.8498\n",
      "11 200 loss: 0.60620886  acc: 0.84895\n",
      "12 0 loss: 0.6561524  acc: 0.85965\n",
      "12 100 loss: 0.7365313  acc: 0.85465\n",
      "12 200 loss: 0.56815916  acc: 0.8536\n",
      "13 0 loss: 0.6219073  acc: 0.8635\n",
      "13 100 loss: 0.7011485  acc: 0.8579\n",
      "13 200 loss: 0.53625834  acc: 0.8573\n",
      "14 0 loss: 0.592932  acc: 0.8672\n",
      "14 100 loss: 0.6710743  acc: 0.86125\n",
      "14 200 loss: 0.5091687  acc: 0.86075\n",
      "15 0 loss: 0.5679929  acc: 0.87025\n",
      "15 100 loss: 0.64506346  acc: 0.8634\n",
      "15 200 loss: 0.48588327  acc: 0.8639\n",
      "16 0 loss: 0.5463778  acc: 0.87295\n",
      "16 100 loss: 0.62240547  acc: 0.86635\n",
      "16 200 loss: 0.4657096  acc: 0.86625\n",
      "17 0 loss: 0.52750313  acc: 0.8762\n",
      "17 100 loss: 0.60238886  acc: 0.86935\n",
      "17 200 loss: 0.44803268  acc: 0.86975\n",
      "18 0 loss: 0.5108009  acc: 0.879\n",
      "18 100 loss: 0.58459955  acc: 0.8715\n",
      "18 200 loss: 0.43244308  acc: 0.87225\n",
      "19 0 loss: 0.49587315  acc: 0.88025\n",
      "19 100 loss: 0.56864107  acc: 0.87345\n",
      "19 200 loss: 0.41860536  acc: 0.8743\n",
      "20 0 loss: 0.48244086  acc: 0.8825\n",
      "20 100 loss: 0.55426437  acc: 0.87545\n",
      "20 200 loss: 0.40618977  acc: 0.87575\n",
      "21 0 loss: 0.47031215  acc: 0.88445\n",
      "21 100 loss: 0.5413131  acc: 0.8777\n",
      "21 200 loss: 0.394986  acc: 0.878\n",
      "22 0 loss: 0.4593327  acc: 0.88655\n",
      "22 100 loss: 0.5295818  acc: 0.8806\n",
      "22 200 loss: 0.3848771  acc: 0.87985\n",
      "23 0 loss: 0.44930422  acc: 0.88795\n",
      "23 100 loss: 0.5188725  acc: 0.8827\n",
      "23 200 loss: 0.37567768  acc: 0.8812\n",
      "24 0 loss: 0.4400975  acc: 0.8885\n",
      "24 100 loss: 0.5090007  acc: 0.88495\n",
      "24 200 loss: 0.36727098  acc: 0.8829\n",
      "25 0 loss: 0.4315921  acc: 0.88955\n",
      "25 100 loss: 0.49992588  acc: 0.88645\n",
      "25 200 loss: 0.35952938  acc: 0.88435\n",
      "26 0 loss: 0.4237037  acc: 0.8907\n",
      "26 100 loss: 0.49150154  acc: 0.8884\n",
      "26 200 loss: 0.3523823  acc: 0.88555\n",
      "27 0 loss: 0.41634247  acc: 0.8922\n",
      "27 100 loss: 0.48363417  acc: 0.88985\n",
      "27 200 loss: 0.34575835  acc: 0.8865\n",
      "28 0 loss: 0.40945643  acc: 0.89295\n",
      "28 100 loss: 0.4762718  acc: 0.89085\n",
      "28 200 loss: 0.33959308  acc: 0.88775\n",
      "29 0 loss: 0.40299365  acc: 0.894\n",
      "29 100 loss: 0.469392  acc: 0.8916\n",
      "29 200 loss: 0.33385354  acc: 0.88905\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(30):\n",
    "    train_epoch(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 彩蛋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# Install TensorFlow\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 - 5s - loss: 0.2045 - accuracy: 0.9387\n",
      "Epoch 2/5\n",
      "60000/60000 - 5s - loss: 0.0858 - accuracy: 0.9729\n",
      "Epoch 3/5\n",
      "60000/60000 - 5s - loss: 0.0590 - accuracy: 0.9813\n",
      "Epoch 4/5\n",
      "60000/60000 - 4s - loss: 0.0448 - accuracy: 0.9854\n",
      "Epoch 5/5\n",
      "60000/60000 - 4s - loss: 0.0356 - accuracy: 0.9884\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x249aca962e8>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=5,verbose=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
