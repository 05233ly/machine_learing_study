{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResnetNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  tensorflow as tf\n",
    "from    tensorflow import keras\n",
    "from    tensorflow.keras import layers, Sequential\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  tensorflow as tf\n",
    "from    tensorflow.keras import layers, optimizers, datasets, Sequential\n",
    "import  os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "tf.random.set_seed(2345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "    # 将数据映射到-1~1\n",
    "    x = 2*tf.cast(x, dtype=tf.float32) / 255. - 1\n",
    "    y = tf.cast(y, dtype=tf.int32) # 类型转换\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(layers.Layer):\n",
    "    # 残差模块\n",
    "    def __init__(self, filter_num, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        # 第一个卷积单元\n",
    "        self.conv1 = layers.Conv2D(filter_num, (3, 3), strides=stride, padding='same')\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.relu = layers.Activation('relu')\n",
    "        # 第二个卷积单元\n",
    "        self.conv2 = layers.Conv2D(filter_num, (3, 3), strides=1, padding='same')\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "\n",
    "        if stride != 1:# 通过1x1卷积完成shape匹配\n",
    "            self.downsample = Sequential()\n",
    "            self.downsample.add(layers.Conv2D(filter_num, (1, 1), strides=stride))\n",
    "        else:# shape匹配，直接短接\n",
    "            self.downsample = lambda x:x\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "\n",
    "        # [b, h, w, c]，通过第一个卷积单元\n",
    "        out = self.conv1(inputs)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        # 通过第二个卷积单元\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        # 通过identity模块\n",
    "        identity = self.downsample(inputs)\n",
    "        # 2条路径输出直接相加\n",
    "        output = layers.add([out, identity])\n",
    "        output = tf.nn.relu(output) # 激活函数\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(keras.Model):\n",
    "    # 通用的ResNet实现类\n",
    "    def __init__(self, layer_dims, num_classes=10): # [2, 2, 2, 2]\n",
    "        super(ResNet, self).__init__()\n",
    "        # 根网络，预处理\n",
    "        self.stem = Sequential([layers.Conv2D(64, (3, 3), strides=(1, 1)),\n",
    "                                layers.BatchNormalization(),\n",
    "                                layers.Activation('relu'),\n",
    "                                layers.MaxPool2D(pool_size=(2, 2), strides=(1, 1), padding='same')\n",
    "                                ])\n",
    "        # 堆叠4个Block，每个block包含了多个BasicBlock,设置步长不一样\n",
    "        self.layer1 = self.build_resblock(64,  layer_dims[0])\n",
    "        self.layer2 = self.build_resblock(128, layer_dims[1], stride=2)\n",
    "        self.layer3 = self.build_resblock(256, layer_dims[2], stride=2)\n",
    "        self.layer4 = self.build_resblock(512, layer_dims[3], stride=2)\n",
    "\n",
    "        # 通过Pooling层将高宽降低为1x1\n",
    "        self.avgpool = layers.GlobalAveragePooling2D()\n",
    "        # 最后连接一个全连接层分类\n",
    "        self.fc = layers.Dense(num_classes)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # 通过根网络\n",
    "        x = self.stem(inputs)\n",
    "        # 一次通过4个模块\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        # 通过池化层\n",
    "        x = self.avgpool(x)\n",
    "        # 通过全连接层\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "    def build_resblock(self, filter_num, blocks, stride=1):\n",
    "        # 辅助函数，堆叠filter_num个BasicBlock\n",
    "        res_blocks = Sequential()\n",
    "        # 只有第一个BasicBlock的步长可能不为1，实现下采样\n",
    "        res_blocks.add(BasicBlock(filter_num, stride))\n",
    "\n",
    "        for _ in range(1, blocks):#其他BasicBlock步长都为1\n",
    "            res_blocks.add(BasicBlock(filter_num, stride=1))\n",
    "\n",
    "        return res_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) (50000,) (10000, 32, 32, 3) (10000,)\n",
      "sample: (512, 32, 32, 3) (512,) tf.Tensor(-1.0, shape=(), dtype=float32) tf.Tensor(1.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "(x,y), (x_test, y_test) = datasets.cifar10.load_data() # 加载数据集\n",
    "y = tf.squeeze(y, axis=1) # 删除不必要的维度\n",
    "y_test = tf.squeeze(y_test, axis=1) # 删除不必要的维度\n",
    "print(x.shape, y.shape, x_test.shape, y_test.shape)\n",
    "\n",
    "\n",
    "train_db = tf.data.Dataset.from_tensor_slices((x,y)) # 构建训练集\n",
    "# 随机打散，预处理，批量化\n",
    "train_db = train_db.shuffle(1000).map(preprocess).batch(512)\n",
    "\n",
    "test_db = tf.data.Dataset.from_tensor_slices((x_test,y_test)) #构建测试集\n",
    "# 随机打散，预处理，批量化\n",
    "test_db = test_db.map(preprocess).batch(512)\n",
    "# 采样一个样本\n",
    "sample = next(iter(train_db))\n",
    "print('sample:', sample[0].shape, sample[1].shape,\n",
    "      tf.reduce_min(sample[0]), tf.reduce_max(sample[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"res_net\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential (Sequential)      multiple                  2048      \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    multiple                  148736    \n",
      "_________________________________________________________________\n",
      "sequential_2 (Sequential)    multiple                  526976    \n",
      "_________________________________________________________________\n",
      "sequential_4 (Sequential)    multiple                  2102528   \n",
      "_________________________________________________________________\n",
      "sequential_6 (Sequential)    multiple                  8399360   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  5130      \n",
      "=================================================================\n",
      "Total params: 11,184,778\n",
      "Trainable params: 11,176,970\n",
      "Non-trainable params: 7,808\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8f5b6ebc185455b8ceb1b02167c8e33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 train loss: 2.304671287536621 train acc: 12.1094\n",
      "0 50 train loss: 1.7226312160491943 train acc: 27.0078\n",
      "0 test loss:1.5819 test acc: 0.4099\n",
      "1 0 train loss: 1.5232518911361694 train acc: 37.5574\n",
      "1 50 train loss: 1.4123958349227905 train acc: 42.7773\n",
      "1 test loss:1.4209 test acc: 0.4754\n",
      "2 0 train loss: 1.3960447311401367 train acc: 46.8893\n",
      "2 50 train loss: 1.3213202953338623 train acc: 49.4805\n",
      "2 test loss:1.2638 test acc: 0.5383\n",
      "3 0 train loss: 1.1700407266616821 train acc: 52.8279\n",
      "3 50 train loss: 1.1308753490447998 train acc: 54.2969\n",
      "3 test loss:1.1603 test acc: 0.5828\n",
      "4 0 train loss: 1.1618928909301758 train acc: 56.7746\n",
      "4 50 train loss: 1.1016786098480225 train acc: 58.8711\n",
      "4 test loss:1.0728 test acc: 0.6129\n",
      "5 0 train loss: 1.0279364585876465 train acc: 61.4098\n",
      "5 50 train loss: 1.0679045915603638 train acc: 61.9648\n",
      "5 test loss:1.0353 test acc: 0.6343\n",
      "6 0 train loss: 1.026222825050354 train acc: 63.3238\n",
      "6 50 train loss: 0.9614042043685913 train acc: 64.4102\n",
      "6 test loss:1.0449 test acc: 0.6301\n",
      "7 0 train loss: 0.9170571565628052 train acc: 66.5738\n",
      "7 50 train loss: 0.8419209718704224 train acc: 67.5430\n",
      "7 test loss:0.9495 test acc: 0.6605\n",
      "8 0 train loss: 0.8894334435462952 train acc: 68.6148\n",
      "8 50 train loss: 0.8202975392341614 train acc: 68.8750\n",
      "8 test loss:0.8759 test acc: 0.6866\n",
      "9 0 train loss: 0.707476019859314 train acc: 70.9098\n",
      "9 50 train loss: 0.7222920656204224 train acc: 71.3203\n",
      "9 test loss:0.8848 test acc: 0.6909\n",
      "10 0 train loss: 0.7870585918426514 train acc: 71.5246\n",
      "10 50 train loss: 0.7857496738433838 train acc: 72.6133\n",
      "10 test loss:0.8524 test acc: 0.6928\n",
      "11 0 train loss: 0.671729564666748 train acc: 75.0246\n",
      "11 50 train loss: 0.6514235734939575 train acc: 74.5430\n",
      "11 test loss:0.7782 test acc: 0.7242\n",
      "12 0 train loss: 0.6322186589241028 train acc: 75.4713\n",
      "12 50 train loss: 0.7342002391815186 train acc: 76.9062\n",
      "12 test loss:0.7626 test acc: 0.7346\n",
      "13 0 train loss: 0.5867350697517395 train acc: 77.8975\n",
      "13 50 train loss: 0.4873456358909607 train acc: 78.3203\n",
      "13 test loss:0.8190 test acc: 0.717\n",
      "14 0 train loss: 0.5142907500267029 train acc: 79.7541\n",
      "14 50 train loss: 0.5383484363555908 train acc: 79.6953\n",
      "14 test loss:0.7462 test acc: 0.7451\n",
      "15 0 train loss: 0.46083587408065796 train acc: 81.6516\n",
      "15 50 train loss: 0.4476620852947235 train acc: 82.0352\n",
      "15 test loss:0.8040 test acc: 0.7295\n",
      "16 0 train loss: 0.48359745740890503 train acc: 82.5820\n",
      "16 50 train loss: 0.41002798080444336 train acc: 83.0586\n",
      "16 test loss:0.7506 test acc: 0.7489\n",
      "17 0 train loss: 0.4889434576034546 train acc: 84.3361\n",
      "17 50 train loss: 0.4220854341983795 train acc: 84.4219\n",
      "17 test loss:0.7741 test acc: 0.7505\n",
      "18 0 train loss: 0.3838649392127991 train acc: 85.8402\n",
      "18 50 train loss: 0.3390352129936218 train acc: 85.5625\n",
      "18 test loss:0.7890 test acc: 0.7473\n",
      "19 0 train loss: 0.34585171937942505 train acc: 87.5615\n",
      "19 50 train loss: 0.3264527916908264 train acc: 87.3320\n",
      "19 test loss:0.8176 test acc: 0.7515\n",
      "20 0 train loss: 0.3234642744064331 train acc: 89.3811\n",
      "20 50 train loss: 0.21660886704921722 train acc: 89.1719\n",
      "20 test loss:0.8788 test acc: 0.7482\n",
      "21 0 train loss: 0.35921674966812134 train acc: 90.9836\n",
      "21 50 train loss: 0.2518094778060913 train acc: 90.1602\n",
      "21 test loss:0.8516 test acc: 0.7408\n",
      "22 0 train loss: 0.3356007933616638 train acc: 90.1721\n",
      "22 50 train loss: 0.22132237255573273 train acc: 91.2852\n",
      "22 test loss:0.9394 test acc: 0.7388\n",
      "23 0 train loss: 0.2419871687889099 train acc: 92.4918\n",
      "23 50 train loss: 0.1924762725830078 train acc: 93.1328\n",
      "23 test loss:0.8853 test acc: 0.7551\n",
      "24 0 train loss: 0.18240411579608917 train acc: 92.2295\n",
      "24 50 train loss: 0.235911026597023 train acc: 94.4336\n",
      "24 test loss:0.8920 test acc: 0.761\n",
      "25 0 train loss: 0.11000505089759827 train acc: 93.9918\n",
      "25 50 train loss: 0.08686985820531845 train acc: 94.8633\n",
      "25 test loss:0.9730 test acc: 0.7499\n",
      "26 0 train loss: 0.1325475573539734 train acc: 95.0123\n",
      "26 50 train loss: 0.11842228472232819 train acc: 95.3906\n",
      "26 test loss:1.0055 test acc: 0.7667\n",
      "27 0 train loss: 0.06886060535907745 train acc: 96.8811\n",
      "27 50 train loss: 0.0631059929728508 train acc: 96.6016\n",
      "27 test loss:1.2086 test acc: 0.7247\n",
      "28 0 train loss: 0.2509407103061676 train acc: 95.8975\n",
      "28 50 train loss: 0.08448587357997894 train acc: 96.6562\n",
      "28 test loss:1.1381 test acc: 0.7646\n",
      "29 0 train loss: 0.059617914259433746 train acc: 97.1025\n",
      "29 50 train loss: 0.056769877672195435 train acc: 98.0000\n",
      "29 test loss:1.1792 test acc: 0.761\n",
      "30 0 train loss: 0.059597235172986984 train acc: 97.6803\n",
      "30 50 train loss: 0.03375014290213585 train acc: 97.9023\n",
      "30 test loss:1.3116 test acc: 0.7482\n",
      "31 0 train loss: 0.07625383138656616 train acc: 98.1516\n",
      "31 50 train loss: 0.050344888120889664 train acc: 97.3711\n",
      "31 test loss:1.2732 test acc: 0.7494\n",
      "32 0 train loss: 0.06626096367835999 train acc: 98.2336\n",
      "32 50 train loss: 0.029921602457761765 train acc: 97.0273\n",
      "32 test loss:1.1234 test acc: 0.7608\n",
      "33 0 train loss: 0.05955334007740021 train acc: 97.4795\n",
      "33 50 train loss: 0.03628363832831383 train acc: 98.6719\n",
      "33 test loss:1.4153 test acc: 0.752\n",
      "34 0 train loss: 0.09531755745410919 train acc: 98.6967\n",
      "34 50 train loss: 0.024861719459295273 train acc: 98.8398\n",
      "34 test loss:1.2412 test acc: 0.7652\n",
      "35 0 train loss: 0.027793962508440018 train acc: 99.1230\n",
      "35 50 train loss: 0.024234065786004066 train acc: 98.4961\n",
      "35 test loss:1.0450 test acc: 0.7548\n",
      "36 0 train loss: 0.07829529792070389 train acc: 94.4098\n",
      "36 50 train loss: 0.02446972206234932 train acc: 98.8203\n",
      "36 test loss:1.2429 test acc: 0.7756\n",
      "37 0 train loss: 0.014352090656757355 train acc: 99.6639\n",
      "37 50 train loss: 0.04494554549455643 train acc: 99.4844\n",
      "37 test loss:1.3933 test acc: 0.7689\n",
      "38 0 train loss: 0.03054567240178585 train acc: 99.4672\n",
      "38 50 train loss: 0.0395163856446743 train acc: 98.4883\n",
      "38 test loss:1.4015 test acc: 0.7561\n",
      "39 0 train loss: 0.02468792349100113 train acc: 98.7008\n",
      "39 50 train loss: 0.0430084690451622 train acc: 99.3008\n",
      "39 test loss:1.3835 test acc: 0.7715\n",
      "40 0 train loss: 0.00942354928702116 train acc: 99.6639\n",
      "40 50 train loss: 0.004258289467543364 train acc: 99.7812\n",
      "40 test loss:1.5361 test acc: 0.77\n",
      "41 0 train loss: 0.004609074909240007 train acc: 99.8156\n",
      "41 50 train loss: 0.019404212012887 train acc: 99.4883\n",
      "41 test loss:1.4789 test acc: 0.769\n",
      "42 0 train loss: 0.03604242205619812 train acc: 99.4467\n",
      "42 50 train loss: 0.014899828471243382 train acc: 99.3320\n",
      "42 test loss:1.7407 test acc: 0.7475\n",
      "43 0 train loss: 0.030600855126976967 train acc: 99.7172\n",
      "43 50 train loss: 0.09793637692928314 train acc: 94.3359\n",
      "43 test loss:1.1986 test acc: 0.7725\n",
      "44 0 train loss: 0.01937127485871315 train acc: 98.6311\n",
      "44 50 train loss: 0.016874101012945175 train acc: 99.6406\n",
      "44 test loss:1.3484 test acc: 0.7661\n",
      "45 0 train loss: 0.01637575402855873 train acc: 99.4672\n",
      "45 50 train loss: 0.008275973610579967 train acc: 99.5508\n",
      "45 test loss:1.4146 test acc: 0.7744\n",
      "46 0 train loss: 0.022187896072864532 train acc: 99.5861\n",
      "46 50 train loss: 0.014369336888194084 train acc: 99.2422\n",
      "46 test loss:1.5466 test acc: 0.7692\n",
      "47 0 train loss: 0.018973752856254578 train acc: 99.7295\n",
      "47 50 train loss: 0.009164567105472088 train acc: 99.6758\n",
      "47 test loss:1.6243 test acc: 0.7616\n",
      "48 0 train loss: 0.04889781028032303 train acc: 99.6516\n",
      "48 50 train loss: 0.01733783632516861 train acc: 99.5781\n",
      "48 test loss:1.6858 test acc: 0.7414\n",
      "49 0 train loss: 0.12467119097709656 train acc: 99.2295\n",
      "49 50 train loss: 0.02681640163064003 train acc: 96.8281\n",
      "49 test loss:1.3520 test acc: 0.7774\n",
      "50 0 train loss: 0.0038984741549938917 train acc: 99.7992\n",
      "50 50 train loss: 0.002369117457419634 train acc: 99.8516\n",
      "50 test loss:1.5660 test acc: 0.7736\n",
      "51 0 train loss: 0.01714855059981346 train acc: 99.8770\n",
      "51 50 train loss: 0.031581513583660126 train acc: 99.5469\n",
      "51 test loss:1.4727 test acc: 0.7742\n",
      "52 0 train loss: 0.005178428720682859 train acc: 99.7377\n",
      "52 50 train loss: 0.007488427683711052 train acc: 99.8945\n",
      "52 test loss:1.4135 test acc: 0.7651\n",
      "53 0 train loss: 0.01532771997153759 train acc: 99.1926\n",
      "53 50 train loss: 0.02446133829653263 train acc: 99.2773\n",
      "53 test loss:1.5904 test acc: 0.7546\n",
      "54 0 train loss: 0.019911514595150948 train acc: 99.4139\n",
      "54 50 train loss: 0.013112819753587246 train acc: 99.5117\n",
      "54 test loss:1.5169 test acc: 0.7681\n",
      "55 0 train loss: 0.007412286940962076 train acc: 99.4672\n",
      "55 50 train loss: 0.01174076460301876 train acc: 99.5430\n",
      "55 test loss:1.3759 test acc: 0.7378\n",
      "56 0 train loss: 0.12891674041748047 train acc: 97.7295\n",
      "56 50 train loss: 0.022777004167437553 train acc: 99.1289\n",
      "56 test loss:1.4196 test acc: 0.7701\n",
      "57 0 train loss: 0.005149907898157835 train acc: 99.7500\n",
      "57 50 train loss: 0.003193717449903488 train acc: 99.8945\n",
      "57 test loss:1.5382 test acc: 0.7736\n",
      "58 0 train loss: 0.007693514693528414 train acc: 99.8648\n",
      "58 50 train loss: 0.0025350279174745083 train acc: 99.8164\n",
      "58 test loss:1.1307 test acc: 0.7687\n",
      "59 0 train loss: 0.016967155039310455 train acc: 96.8607\n",
      "59 50 train loss: 0.01637360453605652 train acc: 99.6875\n",
      "59 test loss:1.3961 test acc: 0.776\n",
      "60 0 train loss: 0.00371805508621037 train acc: 99.8033\n",
      "60 50 train loss: 0.004087585490196943 train acc: 99.8398\n",
      "60 test loss:1.6555 test acc: 0.7655\n",
      "61 0 train loss: 0.008385075256228447 train acc: 99.7254\n",
      "61 50 train loss: 0.018826205283403397 train acc: 99.4180\n",
      "61 test loss:1.4937 test acc: 0.7783\n",
      "62 0 train loss: 0.00965083111077547 train acc: 99.7049\n",
      "62 50 train loss: 0.030852675437927246 train acc: 99.6914\n",
      "62 test loss:1.5106 test acc: 0.7694\n",
      "63 0 train loss: 0.01841122657060623 train acc: 99.3770\n",
      "63 50 train loss: 0.013796675018966198 train acc: 99.4492\n",
      "63 test loss:1.6389 test acc: 0.7655\n",
      "64 0 train loss: 0.020683052018284798 train acc: 99.5615\n",
      "64 50 train loss: 0.015858260914683342 train acc: 99.0898\n",
      "64 test loss:1.5326 test acc: 0.7625\n",
      "65 0 train loss: 0.011423571966588497 train acc: 99.5656\n",
      "65 50 train loss: 0.03243103623390198 train acc: 99.6914\n",
      "65 test loss:0.9290 test acc: 0.6749\n",
      "66 0 train loss: 0.6996860504150391 train acc: 83.7787\n",
      "66 50 train loss: 0.12541763484477997 train acc: 88.5312\n",
      "66 test loss:1.1435 test acc: 0.7587\n",
      "67 0 train loss: 0.05993851274251938 train acc: 97.7746\n",
      "67 50 train loss: 0.02373197302222252 train acc: 99.3125\n",
      "67 test loss:1.3377 test acc: 0.7727\n",
      "68 0 train loss: 0.01399090513586998 train acc: 99.5287\n",
      "68 50 train loss: 0.020002033561468124 train acc: 99.7148\n",
      "68 test loss:1.4105 test acc: 0.7813\n",
      "69 0 train loss: 0.0017745713703334332 train acc: 99.8443\n",
      "69 50 train loss: 0.002726999344304204 train acc: 99.9219\n",
      "69 test loss:1.5359 test acc: 0.7868\n",
      "70 0 train loss: 0.0034018782898783684 train acc: 99.9836\n",
      "70 50 train loss: 0.0023657395504415035 train acc: 99.9375\n",
      "70 test loss:1.5437 test acc: 0.7834\n",
      "71 0 train loss: 0.0016672261990606785 train acc: 99.9262\n",
      "71 50 train loss: 0.0011395672336220741 train acc: 99.9609\n",
      "71 test loss:1.6338 test acc: 0.7792\n",
      "72 0 train loss: 0.004334990866482258 train acc: 99.9713\n",
      "72 50 train loss: 0.0009789379546418786 train acc: 99.9453\n",
      "72 test loss:1.8189 test acc: 0.7583\n",
      "73 0 train loss: 0.0764317736029625 train acc: 99.6721\n",
      "73 50 train loss: 0.11642101407051086 train acc: 97.7500\n",
      "73 test loss:1.3435 test acc: 0.7701\n",
      "74 0 train loss: 0.010112280026078224 train acc: 98.7869\n",
      "74 50 train loss: 0.006137903314083815 train acc: 99.7188\n",
      "74 test loss:1.5419 test acc: 0.7772\n",
      "75 0 train loss: 0.004193048924207687 train acc: 99.8893\n",
      "75 50 train loss: 0.0010248062899336219 train acc: 99.8984\n",
      "75 test loss:1.5663 test acc: 0.7831\n",
      "76 0 train loss: 0.002593471435829997 train acc: 99.8730\n",
      "76 50 train loss: 0.00817301869392395 train acc: 99.8594\n",
      "76 test loss:1.5296 test acc: 0.7625\n",
      "77 0 train loss: 0.022492412477731705 train acc: 99.2787\n",
      "77 50 train loss: 0.005902417935431004 train acc: 99.3906\n",
      "77 test loss:1.4781 test acc: 0.785\n",
      "78 0 train loss: 0.002622275147587061 train acc: 99.7951\n",
      "78 50 train loss: 0.061100445687770844 train acc: 97.6211\n",
      "78 test loss:1.2455 test acc: 0.7842\n",
      "79 0 train loss: 0.006925301160663366 train acc: 99.2869\n",
      "79 50 train loss: 0.0015349233290180564 train acc: 99.8867\n",
      "79 test loss:1.5278 test acc: 0.7831\n",
      "80 0 train loss: 0.0021400833502411842 train acc: 99.9713\n",
      "80 50 train loss: 0.0001992569596040994 train acc: 99.9883\n",
      "80 test loss:1.5289 test acc: 0.7871\n",
      "81 0 train loss: 0.0008617020212113857 train acc: 99.9836\n",
      "81 50 train loss: 0.0008310244884341955 train acc: 99.9961\n",
      "81 test loss:1.5889 test acc: 0.7927\n",
      "82 0 train loss: 0.00018037622794508934 train acc: 100.0000\n",
      "82 50 train loss: 5.7082164858002216e-05 train acc: 100.0000\n",
      "82 test loss:1.6367 test acc: 0.7943\n",
      "83 0 train loss: 2.5341150831081904e-05 train acc: 100.0000\n",
      "83 50 train loss: 3.591535278246738e-05 train acc: 100.0000\n",
      "83 test loss:1.6697 test acc: 0.7942\n",
      "84 0 train loss: 3.034148903680034e-05 train acc: 100.0000\n",
      "84 50 train loss: 1.848266037995927e-05 train acc: 100.0000\n",
      "84 test loss:1.6973 test acc: 0.7936\n",
      "85 0 train loss: 2.1853807993466035e-05 train acc: 100.0000\n",
      "85 50 train loss: 2.5672798074083403e-05 train acc: 100.0000\n",
      "85 test loss:1.7213 test acc: 0.7937\n",
      "86 0 train loss: 2.102739381371066e-05 train acc: 100.0000\n",
      "86 50 train loss: 1.4003832802700344e-05 train acc: 100.0000\n",
      "86 test loss:1.7428 test acc: 0.7939\n",
      "87 0 train loss: 1.758871803758666e-05 train acc: 100.0000\n",
      "87 50 train loss: 1.8802618797053583e-05 train acc: 100.0000\n",
      "87 test loss:1.7625 test acc: 0.7942\n",
      "88 0 train loss: 1.519357738288818e-05 train acc: 100.0000\n",
      "88 50 train loss: 1.5786135918460786e-05 train acc: 100.0000\n",
      "88 test loss:1.7807 test acc: 0.794\n",
      "89 0 train loss: 1.4270855899667367e-05 train acc: 100.0000\n",
      "89 50 train loss: 1.5174377949733753e-05 train acc: 100.0000\n",
      "89 test loss:1.7978 test acc: 0.794\n",
      "90 0 train loss: 1.4404621651920024e-05 train acc: 100.0000\n",
      "90 50 train loss: 9.118212801695336e-06 train acc: 100.0000\n",
      "90 test loss:1.8140 test acc: 0.7941\n",
      "91 0 train loss: 1.1312211427139118e-05 train acc: 100.0000\n",
      "91 50 train loss: 1.131670614995528e-05 train acc: 100.0000\n",
      "91 test loss:1.8292 test acc: 0.7942\n",
      "92 0 train loss: 1.2893011444248259e-05 train acc: 100.0000\n",
      "92 50 train loss: 7.479142823285656e-06 train acc: 100.0000\n",
      "92 test loss:1.8439 test acc: 0.7942\n",
      "93 0 train loss: 1.0783202014863491e-05 train acc: 100.0000\n",
      "93 50 train loss: 9.358877832710277e-06 train acc: 100.0000\n",
      "93 test loss:1.8581 test acc: 0.7941\n",
      "94 0 train loss: 7.900993296061642e-06 train acc: 100.0000\n",
      "94 50 train loss: 8.612900273874402e-06 train acc: 100.0000\n",
      "94 test loss:1.8717 test acc: 0.7947\n",
      "95 0 train loss: 6.572334768861765e-06 train acc: 100.0000\n",
      "95 50 train loss: 6.78227343087201e-06 train acc: 100.0000\n",
      "95 test loss:1.8848 test acc: 0.7948\n",
      "96 0 train loss: 8.646813512314111e-06 train acc: 100.0000\n",
      "96 50 train loss: 7.289854693226516e-06 train acc: 100.0000\n",
      "96 test loss:1.8975 test acc: 0.7952\n",
      "97 0 train loss: 5.031886303186184e-06 train acc: 100.0000\n",
      "97 50 train loss: 5.7927818488678895e-06 train acc: 100.0000\n",
      "97 test loss:1.9100 test acc: 0.7952\n",
      "98 0 train loss: 5.811036317027174e-06 train acc: 100.0000\n",
      "98 50 train loss: 7.686612661927938e-06 train acc: 100.0000\n",
      "98 test loss:1.9221 test acc: 0.7952\n",
      "99 0 train loss: 8.578654160373844e-06 train acc: 100.0000\n",
      "99 50 train loss: 5.653944754158147e-06 train acc: 100.0000\n",
      "99 test loss:1.9340 test acc: 0.7948\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# [b, 32, 32, 3] => [b, 1, 1, 512]\n",
    "model = ResNet([2, 2, 2, 2]) # ResNet18网络\n",
    "# model = ResNet([3, 4, 6, 3]) # ResNet34网络\n",
    "model.build(input_shape=(None, 32, 32, 3))\n",
    "model.summary() # 统计网络参数\n",
    "optimizer = optimizers.Adam(lr=1e-4) # 构建优化器\n",
    "\n",
    "for epoch in tqdm_notebook(range(100)): # 训练epoch\n",
    "\n",
    "    for step, (x,y) in enumerate(train_db):\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # [b, 32, 32, 3] => [b, 10],前向传播\n",
    "            logits = model(x)\n",
    "            train_accuracy(y,logits)\n",
    "            # [b] => [b, 10],one-hot编码\n",
    "            y_onehot = tf.one_hot(y, depth=10)\n",
    "            # 计算交叉熵\n",
    "            loss = tf.losses.categorical_crossentropy(y_onehot, logits, from_logits=True)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "        # 计算梯度信息\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        # 更新网络参数\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        if step %50 == 0:\n",
    "            print(epoch, step, 'train loss:', float(loss),'train acc: {:.4f}'.format(train_accuracy.result()*100))\n",
    "            train_accuracy.reset_states()\n",
    "\n",
    "\n",
    "    total_num = 0\n",
    "    total_correct = 0\n",
    "    for x,y in test_db:\n",
    "\n",
    "        logits = model(x)\n",
    "        loss = tf.losses.sparse_categorical_crossentropy(y, logits, from_logits=True)\n",
    "        test_loss(loss)\n",
    "        \n",
    "        prob = tf.nn.softmax(logits, axis=1)\n",
    "        pred = tf.argmax(prob, axis=1)\n",
    "        pred = tf.cast(pred, dtype=tf.int32)\n",
    "        \n",
    "        correct = tf.cast(tf.equal(pred, y), dtype=tf.int32)\n",
    "        correct = tf.reduce_sum(correct)\n",
    "\n",
    "        total_num += x.shape[0]\n",
    "        total_correct += int(correct)\n",
    "\n",
    "    acc = total_correct / total_num\n",
    "    print(epoch,'test loss:{:.4f}'.format(test_loss.result()), 'test acc:', acc)\n",
    "    test_loss.reset_states()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
