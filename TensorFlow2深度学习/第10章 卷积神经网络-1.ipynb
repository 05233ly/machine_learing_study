{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers,Sequential,losses,optimizers,datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "# 获取所有GPU 设备列表\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # 设置GPU 显存占用为按需分配\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # 异常处理\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 全连接网络的问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=keras.Sequential([\n",
    "    layers.Flatten(input_shape=(28, 28)),\n",
    "    layers.Dense(256,activation='relu'),\n",
    "    layers.Dense(256,activation='relu'),\n",
    "    layers.Dense(256,activation='relu'),\n",
    "    layers.Dense(10),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 335,114\n",
      "Trainable params: 335,114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.build(input_shape=(4,784))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.2048 - accuracy: 0.9378\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.0919 - accuracy: 0.9724\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 0.0673 - accuracy: 0.9788\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 5s 75us/sample - loss: 0.0518 - accuracy: 0.9836\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.0428 - accuracy: 0.9863\n",
      "10000/1 - 1s - loss: 0.0489 - accuracy: 0.9741\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09734694455809659, 0.9741]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 卷积层实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自定义 张量形式实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "x = tf.random.normal([2,5,5,4]) # 模拟输入，3通道，高宽为5\n",
    "# 需要根据[k,k,cin,cout]格式创建，4个卷积核\n",
    "w = tf.random.normal([3,3,4,4]) \n",
    "# 步长为1, padding为0,\n",
    "out = tf.nn.conv2d(x,w,strides=1,padding=[[0,0],[0,0],[0,0],[0,0]])\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 5, 5, 4)\n"
     ]
    }
   ],
   "source": [
    "x = tf.random.normal([2,5,5,3]) # 模拟输入，3通道，高宽为5\n",
    "# 需要根据[k,k,cin,cout]格式创建，4个卷积核\n",
    "w = tf.random.normal([3,3,3,4])\n",
    "# 步长为1, padding为1,\n",
    "out = tf.nn.conv2d(x,w,strides=1,padding=[[0,0],[1,1],[1,1],[0,0]])\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 5, 5, 4)\n"
     ]
    }
   ],
   "source": [
    "x = tf.random.normal([2,5,5,3]) # 模拟输入，3通道，高宽为5\n",
    "w = tf.random.normal([3,3,3,4]) # 4个3x3大小的卷积核\n",
    "# 步长为,padding设置为输出、输入同大小\n",
    "# 需要注意的是, padding=same只有在strides=1时才是同大小\n",
    "out = tf.nn.conv2d(x,w,strides=1,padding='SAME')\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2, 2, 4)\n"
     ]
    }
   ],
   "source": [
    "x = tf.random.normal([2,5,5,3])\n",
    "w = tf.random.normal([3,3,3,4])\n",
    "# 高宽按3倍减少\n",
    "out = tf.nn.conv2d(x,w,strides=3,padding='SAME')\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2, 2, 4)\n"
     ]
    }
   ],
   "source": [
    "# 根据[cout]格式创建偏置向量\n",
    "b = tf.zeros([4])\n",
    "# 在卷积输出上叠加偏置向量，它会自动broadcasting为[b,h',w',cout]\n",
    "out = out + b\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 卷积层类实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 5, 4)\n"
     ]
    }
   ],
   "source": [
    "# 创建卷积层类\n",
    "layer = layers.Conv2D(4,kernel_size=(3,4),strides=(2,1),padding='SAME')\n",
    "out = layer(x) # 前向计算\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'conv2d_1/kernel:0' shape=(3, 4, 3, 4) dtype=float32, numpy=\n",
       " array([[[[ 0.02870092,  0.05202577, -0.23144433, -0.11545415],\n",
       "          [-0.05260587, -0.25014585,  0.00097996,  0.20519614],\n",
       "          [-0.08497144,  0.09365889, -0.14309603, -0.25663304]],\n",
       " \n",
       "         [[ 0.23720178,  0.04914492,  0.2582697 , -0.11836813],\n",
       "          [ 0.16496718,  0.11108315,  0.10814771, -0.08780807],\n",
       "          [ 0.21826416, -0.05005611,  0.04483968,  0.11844498]],\n",
       " \n",
       "         [[-0.10930988, -0.11488794, -0.2559034 , -0.23735589],\n",
       "          [ 0.11511153,  0.11254972, -0.24527349,  0.06609675],\n",
       "          [ 0.02759695,  0.16435611, -0.20593114, -0.06365964]],\n",
       " \n",
       "         [[ 0.15198109, -0.02942209, -0.15500818, -0.09969759],\n",
       "          [-0.0033367 , -0.03676398,  0.21261054, -0.10208614],\n",
       "          [ 0.10273162,  0.256018  , -0.06240423,  0.08979797]]],\n",
       " \n",
       " \n",
       "        [[[-0.16935061, -0.1285987 ,  0.01809508,  0.12173331],\n",
       "          [ 0.20492691, -0.14370725,  0.2656764 , -0.09339543],\n",
       "          [-0.15851699, -0.20457938,  0.08113462, -0.17709234]],\n",
       " \n",
       "         [[-0.20194629, -0.1389536 , -0.23869228,  0.07571706],\n",
       "          [-0.02907412,  0.18712035, -0.2244961 ,  0.13480765],\n",
       "          [-0.09106091,  0.22794259,  0.09535098, -0.20964742]],\n",
       " \n",
       "         [[-0.10693122, -0.10909796, -0.0745348 ,  0.21635479],\n",
       "          [ 0.21400937,  0.1417762 ,  0.06405267,  0.07096386],\n",
       "          [ 0.01596588, -0.08818848, -0.16997278, -0.06806907]],\n",
       " \n",
       "         [[-0.14018773,  0.02582932,  0.10962129, -0.08143887],\n",
       "          [-0.04236835,  0.25995907,  0.06683794,  0.04672763],\n",
       "          [-0.13011798, -0.04615313,  0.13889453, -0.1396716 ]]],\n",
       " \n",
       " \n",
       "        [[[ 0.05933344, -0.23174076,  0.21015215, -0.12344095],\n",
       "          [-0.10767955, -0.2378531 , -0.10651959,  0.12652144],\n",
       "          [-0.04072285, -0.07028493,  0.0124349 ,  0.12000197]],\n",
       " \n",
       "         [[-0.05203487, -0.16911669,  0.04398283,  0.235383  ],\n",
       "          [ 0.18413085,  0.13531366, -0.1639202 ,  0.04717603],\n",
       "          [-0.1774233 ,  0.01945996, -0.09801736,  0.18879166]],\n",
       " \n",
       "         [[-0.1323812 ,  0.1287517 , -0.04619308, -0.06368251],\n",
       "          [ 0.13168919, -0.05808331, -0.2610781 , -0.13696401],\n",
       "          [ 0.08337429, -0.0307615 ,  0.16533765,  0.01946992]],\n",
       " \n",
       "         [[ 0.15828824,  0.07792845, -0.08696295, -0.10172135],\n",
       "          [-0.06054877,  0.22025585,  0.1006555 ,  0.07116663],\n",
       "          [ 0.17175278, -0.04732393, -0.18636775, -0.00110039]]]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'conv2d_1/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.kernel,layer.bias\n",
    "# 返回所有待优化张量列表\n",
    "layer.trainable_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNet-5 实战"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keras高级API实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            multiple                  60        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 multiple                  0         \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            multiple                  880       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 multiple                  0         \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               multiple                  0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             multiple                  48120     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             multiple                  10164     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             multiple                  850       \n",
      "=================================================================\n",
      "Total params: 60,074\n",
      "Trainable params: 60,074\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "network = Sequential([ # 网络容器\n",
    "    layers.Conv2D(6,kernel_size=3,strides=1), # 第一个卷积层, 6个3x3卷积核\n",
    "    layers.MaxPooling2D(pool_size=2,strides=2), # 高宽各减半的池化层\n",
    "    layers.ReLU(), # 激活函数\n",
    "    layers.Conv2D(16,kernel_size=3,strides=1), # 第二个卷积层, 16个3x3卷积核\n",
    "    layers.MaxPooling2D(pool_size=2,strides=2), # 高宽各减半的池化层\n",
    "    layers.ReLU(), # 激活函数\n",
    "    layers.Flatten(), # 打平层，方便全连接层处理\n",
    "\n",
    "    layers.Dense(120, activation='relu'), # 全连接层，120个节点\n",
    "    layers.Dense(84, activation='relu'), # 全连接层，84节点\n",
    "    layers.Dense(10) # 全连接层，10个节点\n",
    "                    ])\n",
    "# build一次网络模型，给输入X的形状，其中4为随意给的batchsz\n",
    "network.build(input_shape=(4, 28, 28, 1))\n",
    "# 统计网络信息\n",
    "network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.compile(optimizer='adam',\n",
    "              loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.0382 - accuracy: 0.9884\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.0314 - accuracy: 0.9898\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.0279 - accuracy: 0.9919\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.0261 - accuracy: 0.9919\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.0208 - accuracy: 0.9937\n",
      "10000/1 - 1s - loss: 0.0465 - accuracy: 0.9783\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09296019066644755, 0.9783]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "network.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 手动实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train, y_train)).shuffle(10000).batch(32)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            multiple                  60        \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) multiple                  0         \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            multiple                  880       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 multiple                  0         \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               multiple                  0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              multiple                  48120     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              multiple                  10164     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              multiple                  850       \n",
      "=================================================================\n",
      "Total params: 60,074\n",
      "Trainable params: 60,074\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "network = Sequential([ # 网络容器\n",
    "    layers.Conv2D(6,kernel_size=3,strides=1), # 第一个卷积层, 6个3x3卷积核\n",
    "    layers.MaxPooling2D(pool_size=2,strides=2), # 高宽各减半的池化层\n",
    "    layers.ReLU(), # 激活函数\n",
    "    layers.Conv2D(16,kernel_size=3,strides=1), # 第二个卷积层, 16个3x3卷积核\n",
    "    layers.MaxPooling2D(pool_size=2,strides=2), # 高宽各减半的池化层\n",
    "    layers.ReLU(), # 激活函数\n",
    "    layers.Flatten(), # 打平层，方便全连接层处理\n",
    "\n",
    "    layers.Dense(120, activation='relu'), # 全连接层，120个节点\n",
    "    layers.Dense(84, activation='relu'), # 全连接层，84节点\n",
    "    layers.Dense(10) # 全连接层，10个节点\n",
    "                    ])\n",
    "# build一次网络模型，给输入X的形状，其中4为随意给的batchsz\n",
    "network.build(input_shape=(4, 28, 28, 1))\n",
    "# 统计网络信息\n",
    "network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = network(images)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, network.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, network.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    predictions = network(images)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer sequential_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Epoch 1, Loss: 0.1978, Accuracy: 93.9900, Test Loss: 0.0790, Test Accuracy: 97.4700\n",
      "Epoch 2, Loss: 0.1331, Accuracy: 95.9350, Test Loss: 0.0686, Test Accuracy: 97.7400\n",
      "Epoch 3, Loss: 0.1056, Accuracy: 96.7700, Test Loss: 0.0581, Test Accuracy: 98.0833\n",
      "Epoch 4, Loss: 0.0887, Accuracy: 97.2950, Test Loss: 0.0543, Test Accuracy: 98.2025\n",
      "Epoch 5, Loss: 0.0771, Accuracy: 97.6383, Test Loss: 0.0516, Test Accuracy: 98.2900\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for images, labels in train_ds:\n",
    "        images=tf.expand_dims(images,axis=3)\n",
    "        train_step(images, labels)\n",
    "\n",
    "    for test_images, test_labels in test_ds:\n",
    "        test_images=tf.expand_dims(test_images,axis=3)\n",
    "        test_step(test_images, test_labels)\n",
    "\n",
    "    template = 'Epoch {}, Loss: {:.4f}, Accuracy: {:.4f}, Test Loss: {:.4f}, Test Accuracy: {:.4f}'\n",
    "    print (template.format(epoch+1,\n",
    "                         train_loss.result(),\n",
    "                         train_accuracy.result()*100,\n",
    "                         test_loss.result(),\n",
    "                         test_accuracy.result()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加入BN层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_12 (Conv2D)           multiple                  60        \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo multiple                  24        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling multiple                  0         \n",
      "_________________________________________________________________\n",
      "re_lu_10 (ReLU)              multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           multiple                  880       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch multiple                  64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling multiple                  0         \n",
      "_________________________________________________________________\n",
      "re_lu_11 (ReLU)              multiple                  0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             multiple                  48120     \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             multiple                  10164     \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             multiple                  850       \n",
      "=================================================================\n",
      "Total params: 60,162\n",
      "Trainable params: 60,118\n",
      "Non-trainable params: 44\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "network = Sequential([ # 网络容器\n",
    "    layers.Conv2D(6,kernel_size=3,strides=1),\n",
    "    # 插入BN层\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(pool_size=2,strides=2),\n",
    "    layers.ReLU(),\n",
    "    layers.Conv2D(16,kernel_size=3,strides=1),\n",
    "    # 插入BN层\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(pool_size=2,strides=2),\n",
    "    layers.ReLU(),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(120, activation='relu'),\n",
    "    # 此处也可以插入BN层\n",
    "    layers.Dense(84, activation='relu'), \n",
    "    # 此处也可以插入BN层\n",
    "    layers.Dense(10)\n",
    "])\n",
    "# build一次网络模型，给输入X的形状，其中4为随意给的batchsz\n",
    "network.build(input_shape=(4, 28, 28, 1))\n",
    "# 统计网络信息\n",
    "network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 0.0177 - accuracy: 0.9956\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.0151 - accuracy: 0.9968\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 0.0138 - accuracy: 0.9964\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.0122 - accuracy: 0.9967\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 0.0149 - accuracy: 0.9962\n",
      "10000/1 - 1s - loss: 0.0713 - accuracy: 0.9811\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1426826808776987, 0.9811]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cifar10+VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  tensorflow as tf\n",
    "from    tensorflow.keras import layers, optimizers, datasets, Sequential\n",
    "import  os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "tf.random.set_seed(2345)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "    # [0~1]\n",
    "    x = 2*tf.cast(x, dtype=tf.float32) / 255.-1\n",
    "    y = tf.cast(y, dtype=tf.int32)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) (50000,) (10000, 32, 32, 3) (10000,)\n"
     ]
    }
   ],
   "source": [
    "(x,y), (x_test, y_test) = datasets.cifar10.load_data()\n",
    "y = tf.squeeze(y, axis=1)\n",
    "y_test = tf.squeeze(y_test, axis=1)\n",
    "print(x.shape, y.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample: (128, 32, 32, 3) (128,) tf.Tensor(-1.0, shape=(), dtype=float32) tf.Tensor(1.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "train_db = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "train_db = train_db.shuffle(1000).map(preprocess).batch(128)\n",
    "\n",
    "test_db = tf.data.Dataset.from_tensor_slices((x_test,y_test))\n",
    "test_db = test_db.map(preprocess).batch(64)\n",
    "\n",
    "sample = next(iter(train_db))\n",
    "print('sample:', sample[0].shape, sample[1].shape,\n",
    "      tf.reduce_min(sample[0]), tf.reduce_max(sample[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "network=Sequential([\n",
    "     # unit 1\n",
    "    layers.Conv2D(64, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.Conv2D(64, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='same'),\n",
    "\n",
    "    # unit 2\n",
    "    layers.Conv2D(128, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.Conv2D(128, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='same'),\n",
    "\n",
    "    # unit 3\n",
    "    layers.Conv2D(256, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.Conv2D(256, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='same'),\n",
    "\n",
    "    # unit 4\n",
    "    layers.Conv2D(512, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.Conv2D(512, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='same'),\n",
    "\n",
    "    # unit 5\n",
    "    layers.Conv2D(512, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.Conv2D(512, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='same'),\n",
    "    layers.Reshape([512]),\n",
    "    \n",
    "    layers.Dense(256, activation=tf.nn.relu),\n",
    "    layers.Dense(128, activation=tf.nn.relu),\n",
    "    layers.Dense(10, activation=None),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.compile(optimizer='adam',\n",
    "              loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 1.8030 - accuracy: 0.2972\n",
      "Epoch 2/50\n",
      "391/391 [==============================] - 26s 67ms/step - loss: 1.2073 - accuracy: 0.55850s - loss: 1.2097 - accu\n",
      "Epoch 3/50\n",
      "391/391 [==============================] - 26s 67ms/step - loss: 0.8970 - accuracy: 0.6815\n",
      "Epoch 4/50\n",
      "391/391 [==============================] - 26s 67ms/step - loss: 0.6992 - accuracy: 0.7569\n",
      "Epoch 5/50\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.5755 - accuracy: 0.8014 - val_loss: 0.7429 - val_accuracy: 0.7605\n",
      "Epoch 6/50\n",
      "391/391 [==============================] - 26s 67ms/step - loss: 0.4730 - accuracy: 0.8382\n",
      "Epoch 7/50\n",
      "391/391 [==============================] - 26s 67ms/step - loss: 0.3856 - accuracy: 0.8692\n",
      "Epoch 8/50\n",
      "391/391 [==============================] - 26s 67ms/step - loss: 0.3244 - accuracy: 0.8892\n",
      "Epoch 9/50\n",
      "391/391 [==============================] - 26s 67ms/step - loss: 0.2854 - accuracy: 0.9033\n",
      "Epoch 10/50\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2286 - accuracy: 0.9222 - val_loss: 0.8693 - val_accuracy: 0.7922\n",
      "Epoch 11/50\n",
      "391/391 [==============================] - 26s 67ms/step - loss: 0.2040 - accuracy: 0.9313\n",
      "Epoch 12/50\n",
      "391/391 [==============================] - 26s 67ms/step - loss: 0.1842 - accuracy: 0.9387\n",
      "Epoch 13/50\n",
      "391/391 [==============================] - 26s 67ms/step - loss: 0.1558 - accuracy: 0.9477\n",
      "Epoch 14/50\n",
      "391/391 [==============================] - 26s 67ms/step - loss: 0.1374 - accuracy: 0.9537\n",
      "Epoch 15/50\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.1253 - accuracy: 0.9590 - val_loss: 1.0513 - val_accuracy: 0.7911\n",
      "Epoch 16/50\n",
      "391/391 [==============================] - 26s 67ms/step - loss: 0.1258 - accuracy: 0.9593\n",
      "Epoch 17/50\n",
      "391/391 [==============================] - 26s 68ms/step - loss: 0.1067 - accuracy: 0.9649\n",
      "Epoch 18/50\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 0.1051 - accuracy: 0.9666\n",
      "Epoch 19/50\n",
      "391/391 [==============================] - 26s 68ms/step - loss: 0.1083 - accuracy: 0.9643\n",
      "Epoch 20/50\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.0936 - accuracy: 0.9690 - val_loss: 1.2138 - val_accuracy: 0.7890\n",
      "Epoch 21/50\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.0985 - accuracy: 0.9694\n",
      "Epoch 22/50\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 0.0917 - accuracy: 0.9715\n",
      "Epoch 23/50\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 0.0845 - accuracy: 0.9737\n",
      "Epoch 24/50\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 0.0747 - accuracy: 0.9773\n",
      "Epoch 25/50\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.0850 - accuracy: 0.9741 - val_loss: 1.1646 - val_accuracy: 0.7990\n",
      "Epoch 26/50\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.0752 - accuracy: 0.9773\n",
      "Epoch 27/50\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.0720 - accuracy: 0.9789\n",
      "Epoch 28/50\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.0779 - accuracy: 0.9769\n",
      "Epoch 29/50\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.0765 - accuracy: 0.9774\n",
      "Epoch 30/50\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.0674 - accuracy: 0.9795 - val_loss: 1.2455 - val_accuracy: 0.7970\n",
      "Epoch 31/50\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.0650 - accuracy: 0.9802\n",
      "Epoch 32/50\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 0.0747 - accuracy: 0.9770\n",
      "Epoch 33/50\n",
      "391/391 [==============================] - 26s 67ms/step - loss: 0.0767 - accuracy: 0.9781\n",
      "Epoch 34/50\n",
      "391/391 [==============================] - 26s 66ms/step - loss: 0.0737 - accuracy: 0.9782\n",
      "Epoch 35/50\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.0794 - accuracy: 0.9763 - val_loss: 1.2791 - val_accuracy: 0.7944\n",
      "Epoch 36/50\n",
      "391/391 [==============================] - 26s 66ms/step - loss: 0.0724 - accuracy: 0.9788\n",
      "Epoch 37/50\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.0580 - accuracy: 0.9824\n",
      "Epoch 38/50\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.0776 - accuracy: 0.9776\n",
      "Epoch 39/50\n",
      "391/391 [==============================] - 26s 66ms/step - loss: 0.0413 - accuracy: 0.9879\n",
      "Epoch 40/50\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.0596 - accuracy: 0.9834 - val_loss: 1.3713 - val_accuracy: 0.7992\n",
      "Epoch 41/50\n",
      "391/391 [==============================] - 26s 67ms/step - loss: 0.0455 - accuracy: 0.9865\n",
      "Epoch 42/50\n",
      "391/391 [==============================] - 26s 67ms/step - loss: 0.0699 - accuracy: 0.9808\n",
      "Epoch 43/50\n",
      "391/391 [==============================] - 26s 67ms/step - loss: 0.0766 - accuracy: 0.9779\n",
      "Epoch 44/50\n",
      "391/391 [==============================] - 26s 67ms/step - loss: 0.0812 - accuracy: 0.9773\n",
      "Epoch 45/50\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.0602 - accuracy: 0.9821 - val_loss: 1.5380 - val_accuracy: 0.7927\n",
      "Epoch 46/50\n",
      "391/391 [==============================] - 26s 67ms/step - loss: 0.0612 - accuracy: 0.9828\n",
      "Epoch 47/50\n",
      "391/391 [==============================] - 26s 67ms/step - loss: 0.0508 - accuracy: 0.9849\n",
      "Epoch 48/50\n",
      "391/391 [==============================] - 26s 68ms/step - loss: 0.0538 - accuracy: 0.9846\n",
      "Epoch 49/50\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 0.0730 - accuracy: 0.9798\n",
      "Epoch 50/50\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.0755 - accuracy: 0.9794 - val_loss: 1.4075 - val_accuracy: 0.8013\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20ec57cff98>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fit(train_db, epochs=50,validation_data=test_db,validation_freq=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
